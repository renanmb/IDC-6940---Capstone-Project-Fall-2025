---
title: "Dataset Exploration - Week 6"
description: "For Week 6 we are exploring the Stroke Dataset"
author:
  - name: Renan Monteiro Barbosa
    url: https://github.com/renanmb
    affiliation: Master of Data Science Program @ The University of West Florida (UWF)
    # affiliation-url: https://ucsb-meds.github.io/
# date: 10-24-2022
categories: [dataset exploration, week 6, renan]
# citation:
#   url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/
image: images/spongebob-imagination.jpg
draft: false
bibliography: references.bib
link-citations: true
---

This post start at Week 6 and extended over several week. From the discoveries made from Week 5 using the dataset [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset) we will be further exploring it by using insights found in @hassan2024predictive. So to develop a better insight we will be reproducing the research work in this post. 

## Introduction

The issue of data imbalance is a big problem for stroke ­prediction @kokkotis2022explainable. Because of many reasons ranging from privacy to the difficulty of doing cohort studies, the fact that pre-stroke datasets are rare, dataset often contain imbalanced classifications, with most instances being non-stroke c­ases @sirsat2020machine. So its unnecessary to say that this imbalance can result in biased models that favour the majority and ignore the minority, resulting in low forecast accuracy. To solve this issue and increase the effectiveness of the predictive models, we plan on exploring several oversampling and undersampling methods and much more are explored and employed, the popular of which is the ­SMOTE @wongvorachan2023comparison, @sowjanya2023effective.


## 1. Setup and Data Loading

First, we need to load the required R packages and the dataset. The dataset is publicly available on Kaggle and was originally created by McKinsey & Company @fedesorianoStrokePredictionDatasetKaggle.

### 1.1 Load Libraries

```{r}
#| code-fold: true
#| label: install-packages

# Run this once to install all the necessary packages
# install.packages(c("corrplot", "ggpubr", "caret", "mice", "ROSE", "ranger", "stacks", "tidymodels"))
# install.packages("themis")
# install.packages("xgboost")
# install.packages("gghighlight")
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages(c("dplyr", "car", "ResourceSelection", "caret", "pROC",  "logistf", "ggplot2"))
```

We can use this to check installed packages:

```{{r}}
renv::activate("website")
"yardstick" %in% rownames(installed.packages())
```

```{r setup}
#| code-fold: true
#| label: load-libraries
#| message: false
#| warning: false

packages <- c("dplyr", "car", "ResourceSelection", "caret", "pROC",  "logistf", "ggplot2")
lapply(packages, library, character.only = TRUE)

# For data manipulation and visualization
library(tidyverse)
library(ggplot2)
library(corrplot)
library(knitr)
library(ggpubr)

# # For data preprocessing and modeling
# library(caret)
# library(mice)
# library(ROSE) # For SMOTE
# library(ranger) # A fast implementation of random forests

# # For stacking/ensemble models
library(stacks)
library(tidymodels)

library(themis)
library(gghighlight)

# Set seed for reproducibility
set.seed(123)
```

Might need to deal with the conflicts later:


### 1.2 Load Data

We will load the dataset and handle the data given the exploration done in Week5. The id column is unnecessary for prediction as well there are only 2 genders significant for prediction.

```{r}
#| code-fold: true
#| output: false
find_git_root <- function(start = getwd()) {
  path <- normalizePath(start, winslash = "/", mustWork = TRUE)
  while (path != dirname(path)) {
    if (dir.exists(file.path(path, ".git"))) return(path)
    path <- dirname(path)
  }
  stop("No .git directory found — are you inside a Git repository?")
}

repo_root <- find_git_root()
datasets_path <- file.path(repo_root, "datasets")
kaggle_dataset_path <- file.path(datasets_path, "kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv")
kaggle_data1 = read_csv(kaggle_dataset_path, show_col_types = FALSE)

# unique(kaggle_data1$bmi)
kaggle_data1 <- kaggle_data1 %>%
  mutate(bmi = na_if(bmi, "N/A")) %>%   # Convert "N/A" string to NA
  mutate(bmi = as.numeric(bmi))         # Convert from character to numeric

# Remove the 'Other' gender row and the 'id' column
kaggle_data1 <- kaggle_data1 %>%
  filter(gender != "Other") %>%
  select(-id) %>%
  mutate_if(is.character, as.factor) # Convert character columns to factors for easier modeling
```

#### 1.2.1 Steve Dataset

Below will be loading the stroke.csv and performing necessary changes to the dataset and loading into the DataFrame: stroke1

```{r}
#| code-fold: true
# Reading the datafile in (the same one you got for us Renan)#
steve_dataset_path <- file.path(datasets_path, "steve/stroke.csv")
stroke1 = read_csv(steve_dataset_path, show_col_types = FALSE)
```

```{r}
# stroke1 <- read.csv("stroke.csv")
stroke1[stroke1 == "N/A" | stroke1 == "Unknown" | stroke1 == "children" | stroke1 == "other"] <- NA
stroke1$bmi <- round(as.numeric(stroke1$bmi), 2)
stroke1$gender[stroke1$gender == "Male"] <- 1
stroke1$gender[stroke1$gender == "Female"] <- 2
stroke1$gender <- as.numeric(stroke1$gender)
```

```{r}
stroke1$stroke <- as.factor(stroke1$stroke)
stroke1_clean <- na.omit(stroke1)
strokeclean <- stroke1_clean
```

The model For Logistic Regression predicting Stroke

```{r}
formula <- stroke ~ gender + age + hypertension + heart_disease + ever_married +
  work_type + Residence_type + avg_glucose_level + bmi + smoking_status
```

Histograms for all the predictors and the Outcome variable

```{r}
ggplot(strokeclean, aes(x = gender)) +
  geom_bar(fill = "blue", 
           color = "white") +
  labs(title = "Histogram of gender", 
       x = "gender", 
       y = "Frequency")
ggplot(strokeclean, aes(x = age)) +
  geom_histogram(binwidth = 5, 
                 fill = "green", 
                 color = "white") +
  labs(title = "Histogram of Age", 
       x = "Age", 
       y = "Frequency")

ggplot(strokeclean, aes(x = hypertension)) +
  geom_bar(fill = "purple", 
           color = "white") +
  labs(title = "Histogram of hypertension", 
       x = "hypertension", 
       y = "Frequency")

ggplot(strokeclean, aes(x = heart_disease)) +
  geom_bar( fill = "orange",
            color = "white") +
  labs(title = "Histogram of heart_diseasd", 
       x = "Age", 
       y = "Frequency")

ggplot(strokeclean, aes(x = ever_married)) +
  geom_bar(fill = "aquamarine", 
           color = "white") +
  labs(title = "Histogram of ever_married", 
       x = "Age", 
       y = "Frequency")

ggplot(strokeclean, aes(x = work_type)) +
  geom_bar(fill = "steelblue", 
           color = "white") +
  labs(title = "Histogram of work_type", 
       x = "Age", 
       y = "Frequency")

ggplot(strokeclean, aes(x = Residence_type)) +
  geom_bar(fill = "magenta", 
           color = "white") +
  labs(title = "Histogram of Residence_type", 
       x = "Residence_type", 
       y = "Frequency")

ggplot(strokeclean, aes(x = avg_glucose_level)) +
  geom_histogram(binwidth = 5, 
                 fill = "chartreuse", 
                 color = "white") +
  labs(title = "Histogram of avg_gloucose_level",
       x = "avg-glucose_level", 
       y = "Frequency")

ggplot(strokeclean, aes(x = bmi)) +
  geom_histogram(binwidth = 5, 
                 fill = "gold", 
                 color = "white") +
  labs(title = "Histogram of bmi", 
       x = "bmi", 
       y = "Frequency")

ggplot(strokeclean, aes(x = smoking_status)) +
  geom_bar(fill = "deepskyblue", 
           color = "white") +
  labs(title = "smoking_status", 
       x = "smoking_status", 
       y = "Frequency")

ggplot(strokeclean, aes(x = stroke)) +
  geom_bar(fill = "tan", 
           color = "white") +
  labs(title = "Histogram of Age", 
       x = "stroke", 
       y = "Frequency")

```

The Three different Models of Logistic Regression: Baseline Firth and Flic Correction

```{r}
# Baseline Logistic Regression
model_base <- glm(formula, data=strokeclean, family=binomial)
prob_base <- predict(model_base, type="response")

# Firth Logistic Regression
model_firth <- logistf(formula, data=strokeclean)
prob_firth <- predict(model_firth, type="response")

# FLIC Correction (this correction changes the intercept)
model_flic <- flic(formula, data=strokeclean)
prob_flic <- predict(model_flic, type="response")

labels <- strokeclean$stroke

```

Creating Youdens J

```{r}
youden_point <- function(roc_obj) {
  coords <- coords(roc_obj, "best", best.method = "youden", ret=c("threshold", "sensitivity", "specificity", "youden"))
  return(coords)
}
```

The Three Types of Prediction with 0.5 for consistency

```{r}
# Predictions with default threshold 0.5 (for output consistency)
pred_base <- factor(ifelse(prob_base > 0.5, 1, 0), levels=c(0,1))
pred_firth <- factor(ifelse(prob_firth > 0.5, 1, 0), levels=c(0,1))
pred_flic <- factor(ifelse(prob_flic > 0.5, 1, 0), levels=c(0,1))
```

The Function to Compute Metrics

```{r}

  pred_base <- factor(ifelse(prob_base > 0.5, 1, 0), levels=c(0,1))
  pred_firth <- factor(ifelse(prob_firth > 0.5, 1, 0), levels=c(0,1))
  pred_flic <- factor(ifelse(prob_flic > 0.5, 1, 0), levels=c(0,1))

  metrics <- function(pred, prob, labels, name) {
    cm <- confusionMatrix(pred, labels, positive = "1")
    roc_obj <- roc(labels, as.numeric(prob))
    auc_val <- auc(roc_obj)
    precision <- cm$byClass["Pos Pred Value"]
    recall <- cm$byClass["Sensitivity"]
    f1 <- 2 * ((precision * recall) / (precision + recall))
    youden <- youden_point(roc_obj)
    # All list arguments separated by commas only, no '+'
    list(
      confusion = cm$table,
      precision = precision,
      recall = recall,
      f1 = f1,
      auc = auc_val,
      roc_obj = roc_obj,
      youden = youden,
      model = name
    )
  }

```

Intialize Results

```{r}
results_base <- metrics(pred_base, prob_base, labels, "Baseline LR")
results_firth <- metrics(pred_firth, prob_firth, labels, "firth LR")
results_flic <- metrics(pred_flic, prob_flic, labels, "flic LR")
```

Print Results

```{r}
cat("\n== Baseline Logistic Regression ==\n")
print(results_base[1:6])
cat("\nYouden's J (optimal threshold):\n")
print(results_base$youden)
cat("\n== Firth Logistic Regression ==\n")
print(results_firth[1:6])
cat("\nYouden's J (optimal threshold):\n")
print(results_firth$youden)
cat("\n== FLIC Logistic Regression ==\n")
print(results_flic[1:6])
cat("\nYouden's J (optimal threshold):\n")
print(results_flic$youden)

```

Plot the ROC curves and Annotate Youden's J on each of the Curves

```{r}
plot(results_base$roc_obj, col="cyan", main="ROC Curves: Baseline (blue) vs Firth (red)")
plot(results_firth$roc_obj, col="magenta", add=TRUE)
plot(results_flic$roc_obj, col ="gold", add=TRUE)
auc(results_base$roc_obj)
auc(results_firth$roc_obj)
auc(results_flic$roc_obj)

points(
  1-results_base$youden["specificity"],
  results_base$youden["sensitivity"],
  col="cyan", pch=19, cex=1.5
)
points(
  1-results_firth$youden["specificity"],
  results_firth$youden["sensitivity"],
  col="magenta", pch=19, cex=1.5
)
points(
  1-results_flic$youden["specificity"],
  results_flic$youden["sensitivity"],
  col="gold", pch=19, cex=1.5
)

legend("bottomright", legend=c("Baseline", "Firth","flic"), col=c("cyan", "magenta", "gold"), lwd=2)

text(
  x=1-results_base$youden["specificity"], y=results_base$youden["sensitivity"],
  labels=paste0("Youden: ", round(results_base$youden["youden"], 3)),
  pos=4, col="cyan"
)
text(
  x=1-results_firth$youden["specificity"], y=results_firth$youden["sensitivity"],
  labels=paste0("Youden: ", round(results_firth$youden["youden"], 3)),
  pos=4, col="magenta"
)
text(
  x=1-results_flic$youden["specificity"], y=results_flic$youden["sensitivity"],
  labels=paste0("Youden: ", round(results_flic$youden["youden"], 3)),
  pos=4, col="gold"
)
```

Plot the Confusion Matrices

```{r}
par(mfrow=c(1,2))
fourfoldplot(results_base$confusion, color = c("lightskyblue", "plum2"),
             conf.level = 0, margin = 1, main = "Baseline Confusion Matrix")
fourfoldplot(results_firth$confusion, color = c("lightskyblue", "plum2"),
             conf.level = 0, margin = 1, main = "Firth Confusion Matrix")
fourfoldplot(results_flic$confusion, color = c("lightskyblue", "plum2"),
             conf.level = 0, margin = 1, main = "Flic Confusion Matrix")

par(mfrow=c(1,1))
```

### References

::: {#refs}
:::