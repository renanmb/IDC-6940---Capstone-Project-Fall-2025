[
  {
    "objectID": "posts/renan-blog-post-week6/index.html",
    "href": "posts/renan-blog-post-week6/index.html",
    "title": "Dataset Exploration - Week 6",
    "section": "",
    "text": "From the discoveries we made from Week 5 using the dataset Stroke Prediction Dataset we will be exploring it using insights found in (1)."
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#setup-and-data-loading",
    "href": "posts/renan-blog-post-week6/index.html#setup-and-data-loading",
    "title": "Dataset Exploration - Week 6",
    "section": "1. Setup and Data Loading",
    "text": "1. Setup and Data Loading\nFirst, we need to load the required R packages and the dataset. The dataset is publicly available on Kaggle and was originally created by McKinsey & Company [Add citation to dataset].\n\n1.1 Load Libraries\n\n# Run this once to install all the necessary packages\ninstall.packages(c(\"corrplot\", \"ggpubr\", \"caret\", \"mice\", \"ROSE\", \"ranger\", \"stacks\", \"tidymodels\"))\n\nThe following package(s) will be installed:\n- caret      [7.0-1]\n- corrplot   [0.95]\n- ggpubr     [0.6.1]\n- mice       [3.18.0]\n- ranger     [0.17.0]\n- ROSE       [0.0-4]\n- stacks     [1.1.1]\n- tidymodels [1.4.1]\nThese packages will be installed into \"~/Documents/GitHub/renanmb/IDC-6940---Capstone-Project-Fall-2025/website/renv/library/linux-ubuntu-jammy/R-4.5/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing corrplot ...                       OK [linked from cache]\n- Installing ggpubr ...                         OK [linked from cache]\n- Installing caret ...                          OK [linked from cache]\n- Installing mice ...                           OK [linked from cache]\n- Installing ROSE ...                           OK [linked from cache]\n- Installing ranger ...                         OK [linked from cache]\n- Installing stacks ...                         OK [linked from cache]\n- Installing tidymodels ...                     OK [linked from cache]\nSuccessfully installed 8 packages in 12 milliseconds.\n\ninstall.packages(\"themis\")\n\nThe following package(s) will be installed:\n- themis [1.0.3]\nThese packages will be installed into \"~/Documents/GitHub/renanmb/IDC-6940---Capstone-Project-Fall-2025/website/renv/library/linux-ubuntu-jammy/R-4.5/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing themis ...                         OK [linked from cache]\nSuccessfully installed 1 package in 2.5 milliseconds.\n\ninstall.packages(\"xgboost\")\n\nThe following package(s) will be installed:\n- xgboost [1.7.11.1]\nThese packages will be installed into \"~/Documents/GitHub/renanmb/IDC-6940---Capstone-Project-Fall-2025/website/renv/library/linux-ubuntu-jammy/R-4.5/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing xgboost ...                        OK [linked from cache]\nSuccessfully installed 1 package in 2.7 milliseconds.\n\n\nWe can use this to check installed packages:\n```{r}\nrenv::activate(\"website\")\n\"yardstick\" %in% rownames(installed.packages())\n```\n\n# For data manipulation and visualization\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(knitr)\nlibrary(ggpubr)\n\n# For data preprocessing and modeling\nlibrary(caret)\nlibrary(mice)\nlibrary(ROSE) # For SMOTE\nlibrary(ranger) # A fast implementation of random forests\n\n# For stacking/ensemble models\nlibrary(stacks)\nlibrary(tidymodels)\n\nlibrary(themis)\n\n# Set seed for reproducibility\nset.seed(123)\n\nMight need to deal with the conflicts later:\n```{bash}\n── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.9     ✔ rsample      1.3.1\n✔ dials        1.4.2     ✔ tailor       0.1.0\n✔ infer        1.0.9     ✔ tune         2.0.0\n✔ modeldata    1.5.1     ✔ workflows    1.3.0\n✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n✔ recipes      1.3.1     ✔ yardstick    1.3.2\n── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidymodels_conflicts() ──\n✖ rsample::calibration()   masks caret::calibration()\n✖ scales::discard()        masks purrr::discard()\n✖ mice::filter()           masks dplyr::filter(), stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ dplyr::lag()             masks stats::lag()\n✖ caret::lift()            masks purrr::lift()\n✖ yardstick::precision()   masks caret::precision()\n✖ yardstick::recall()      masks caret::recall()\n✖ yardstick::sensitivity() masks caret::sensitivity()\n✖ yardstick::spec()        masks readr::spec()\n✖ yardstick::specificity() masks caret::specificity()\n✖ recipes::step()          masks stats::step()\n```\n\n\n1.2 Load Data\nWe will load the dataset and handle the data given the exploration done in Week5. The id column is unnecessary for prediction as well there are only 2 genders significant for prediction.\n\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nkaggle_dataset_path &lt;- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\nkaggle_data1 = read_csv(kaggle_dataset_path, show_col_types = FALSE)\n\n# unique(kaggle_data1$bmi)\nkaggle_data1 &lt;- kaggle_data1 %&gt;%\n  mutate(bmi = na_if(bmi, \"N/A\")) %&gt;%   # Convert \"N/A\" string to NA\n  mutate(bmi = as.numeric(bmi))         # Convert from character to numeric\n\n# Remove the 'Other' gender row and the 'id' column\nkaggle_data1 &lt;- kaggle_data1 %&gt;%\n  filter(gender != \"Other\") %&gt;%\n  select(-id) %&gt;%\n  mutate_if(is.character, as.factor) # Convert character columns to factors for easier modeling"
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#data-imputation-and-balancing",
    "href": "posts/renan-blog-post-week6/index.html#data-imputation-and-balancing",
    "title": "Dataset Exploration - Week 6",
    "section": "2. Data Imputation and Balancing",
    "text": "2. Data Imputation and Balancing\nTo handle the missing BMI values, the study explores three different imputation techniques. It also addresses the significant class imbalance between stroke and non-stroke cases using SMOTE.\n\n2.1 Imputation Techniques\nWe will create three datasets based on the imputation methods described:\n\nMean Imputation: Replacing missing values with the column’s mean.\nMICE (Multivariate Imputation by Chained Equations): An advanced method that estimates missing values based on other variables.\nAge Group-based Imputation: Replacing missing BMI values with the mean BMI of the corresponding age group.\n\n\n# 1. Mean Imputation\ndf_mean &lt;- kaggle_data1\ndf_mean$bmi[is.na(df_mean$bmi)] &lt;- mean(df_mean$bmi, na.rm = TRUE)\n\n# 2. MICE Imputation\nmice_imputation &lt;- mice(kaggle_data1, method='pmm', m=1, maxit=5, seed=500)\n\n\n iter imp variable\n  1   1  bmi\n  2   1  bmi\n  3   1  bmi\n  4   1  bmi\n  5   1  bmi\n\ndf_mice &lt;- complete(mice_imputation, 1)\n\n# 3. Age Group-based Imputation\ndf_age_group &lt;- kaggle_data1 %&gt;%\n  mutate(age_group = cut(age, breaks = c(0, 20, 40, 60, 81), right = FALSE)) %&gt;%\n  group_by(age_group) %&gt;%\n  mutate(bmi = ifelse(is.na(bmi), mean(bmi, na.rm = TRUE), bmi)) %&gt;%\n  ungroup() %&gt;%\n  select(-age_group)\n\n\n\n2.2 Addressing Class Imbalance with SMOTE\nThe dataset is highly imbalanced, with only 4.87% of cases being stroke instances. This can bias machine learning models. We will use SMOTE to create balanced versions of our imputed datasets by generating synthetic minority (stroke) class samples.\n\n# Ensure the stroke column is a factor for SMOTE\ndf_mice$stroke &lt;- as.factor(df_mice$stroke)\ndf_mean$stroke &lt;- as.factor(df_mean$stroke)\ndf_age_group$stroke &lt;- as.factor(df_age_group$stroke)\n\n# Create balanced datasets using SMOTE\n# Using the MICE imputed dataset as the primary example for balancing\n\n# Get the number of non-stroke (majority) cases\nn_majority &lt;- sum(df_mice$stroke == \"0\")\n\n# Calculate the desired total size for a balanced dataset\ndesired_N &lt;- 2 * n_majority\n\n# Create the balanced dataset\ndata_balanced_mice &lt;- ROSE::ovun.sample(\n  stroke ~ ., \n  data = df_mice, \n  method = \"over\", \n  N = desired_N, \n  seed = 123\n)$data\n\n# Check the new class distribution\ncat(\"Original Class Distribution (MICE imputed):\\n\")\n\nOriginal Class Distribution (MICE imputed):\n\nprint(table(df_mice$stroke))\n\n\n   0    1 \n4860  249 \n\ncat(\"\\nBalanced Class Distribution (SMOTE):\\n\")\n\n\nBalanced Class Distribution (SMOTE):\n\nprint(table(data_balanced_mice$stroke))\n\n\n   0    1 \n4860 4860"
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#exploratory-data-analysis-eda-and-feature-importance",
    "href": "posts/renan-blog-post-week6/index.html#exploratory-data-analysis-eda-and-feature-importance",
    "title": "Dataset Exploration - Week 6",
    "section": "3. Exploratory Data Analysis (EDA) and Feature Importance",
    "text": "3. Exploratory Data Analysis (EDA) and Feature Importance\nThe paper identifies several key risk factors for stroke. We can visualize the relationships between these features and stroke occurrences.\n\n3.1 Visualizing Key Features\nLet’s reproduce some of the visualizations from Figure 1 in the paper, which shows the distribution of features concerning stroke occurrence.\nThese plots should confirm the paper’s findings: stroke incidence increases with age, high glucose levels, higher BMI, and the presence of hypertension.\nADD FIGURE here\nFigure 1.  Distribution of features concerning stroke occurrence. (a) through (j) present diverse aspects of stroke occurrences, revealing nuanced patterns. (a) and (b) demonstrate gender and age-related trends. (c) associates strokes with heart disease, while (d) suggests marital status correlations. (e) explores urban–rural disparities. (f) and (g) show links to average glucose levels and hypertension. (h) relates BMI levels to stroke incidence. (i) emphasizes the role of smoking history, and (j) explores potential occupational influences on stroke likelihood.\n# Using the MICE imputed dataset for visualizations\np1 &lt;- ggplot(df_mice, aes(x = age, fill = factor(stroke))) + \n  geom_histogram(binwidth = 5, position = \"identity\", alpha = 0.6) +\n  labs(title = \"Stroke Cases by Age\", x = \"Age\", y = \"Count\")\n\np2 &lt;- ggplot(df_mice, aes(x = avg_glucose_level, fill = factor(stroke))) + \n  geom_histogram(binwidth = 10, position = \"identity\", alpha = 0.6) +\n  labs(title = \"Stroke Cases by Glucose Level\", x = \"Average Glucose Level\", y = \"Count\")\n\np3 &lt;- ggplot(df_mice, aes(x = bmi, fill = factor(stroke))) + \n  geom_histogram(binwidth = 2, position = \"identity\", alpha = 0.6) +\n  labs(title = \"Stroke Cases by BMI\", x = \"BMI\", y = \"Count\")\n\np4 &lt;- ggplot(df_mice, aes(x = factor(hypertension), fill = factor(stroke))) + \n  geom_bar(position = \"dodge\") +\n  labs(title = \"Stroke Cases by Hypertension\", x = \"Hypertension (0=No, 1=Yes)\", y = \"Count\")\n\n# Arrange plots\nggarrange(p1, p2, p3, p4, ncol = 2, nrow = 2, common.legend = TRUE, legend=\"bottom\")\n\n\n\n\n\n\nAge\n\n\n\n\nDistribution of key features by stroke status.\n\n\n\n\n\n3.2 Feature Importance\nThe study identifies age, average glucose level, BMI, heart disease, hypertension, and marital status as the most influential predictors. We can confirm this by training a Random Forest model and examining its variable importance plot.\nThe plot should confirm that age, avg_glucose_level, and bmi are the top three predictors, consistent with the findings in the paper\nFigure 25.  Feature importance comparison for the proposed DSE model. Feature importance graphs for imbalanced and balanced MICE-imputed datasets are displayed in (a) and (b) respectively\n\n# Train a simple Random Forest model to check feature importance\nrf_model_for_importance &lt;- ranger(stroke ~ ., data = df_mice, importance = 'permutation')\n\n# Create importance plot\nimportance_data &lt;- data.frame(\n  Variable = names(rf_model_for_importance$variable.importance),\n  Importance = rf_model_for_importance$variable.importance\n)\n\nggplot(importance_data, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  coord_flip() +\n  labs(title = \"Feature Importance for Stroke Prediction\", x = \"Features\", y = \"Importance\") +\n  theme_minimal()\n\n\n\n\nFeature importance for stroke prediction using a Random Forest model."
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#model-building-and-evaluation",
    "href": "posts/renan-blog-post-week6/index.html#model-building-and-evaluation",
    "title": "Dataset Exploration - Week 6",
    "section": "4. Model Building and Evaluation",
    "text": "4. Model Building and Evaluation\nThe paper evaluates a baseline model, several advanced models, and a final Dense Stacking Ensemble (DSE) model. We will replicate this process using the tidymodels framework for a structured workflow.\n\n4.1 Data Splitting and Preprocessing Recipe\nWe will use the MICE-imputed datasets (both imbalanced and balanced) for modeling. We’ll split the data into training (70%) and testing (30%) sets and create a preprocessing recipe for one-hot encoding categorical variables and normalizing numerical features.\n\n# Use the MICE imputed data\n# data_imb &lt;- df_mice\n# data_bal &lt;- roc_rose(df_mice, \"stroke\")$data # ROSE is similar to SMOTE\ndata_imb &lt;- df_mice\ndata_bal &lt;- ROSE(stroke ~ ., data = df_mice, seed = 123)$data\n\n# --- Imbalanced Data ---\nset.seed(123)\nsplit_imb &lt;- initial_split(data_imb, prop = 0.7, strata = stroke)\ntrain_imb &lt;- training(split_imb)\ntest_imb  &lt;- testing(split_imb)\n\n# --- Balanced Data ---\nset.seed(123)\nsplit_bal &lt;- initial_split(data_bal, prop = 0.7, strata = stroke)\ntrain_bal &lt;- training(split_bal)\ntest_bal  &lt;- testing(split_bal)\n\n\n# Create a preprocessing recipe\nrecipe_spec &lt;- recipe(stroke ~ ., data = train_imb) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n\n\n4.2 Model Definitions\nWe define the models used in the study.\n\n# 1. Baseline: Logistic Regression\nlog_reg_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\")\n\n# 2. Advanced: Random Forest\nrf_spec &lt;- rand_forest(trees = 100) %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  set_mode(\"classification\")\n\n# 3. Advanced: XGBoost\nxgb_spec &lt;- boost_tree(trees = 100) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\n\n\n4.3 Training and Evaluating Models\nWe will create workflows, train the models, and evaluate their performance on the test set.\n\n4.3.1 Baseline Model (Logistic Regression)\n\n# Create a balanced data frame using a tidymodels recipe\ndata_bal &lt;- recipe(stroke ~ ., data = df_mice) %&gt;%\n  step_rose(stroke) %&gt;%\n  prep() %&gt;%\n  juice()\n\n# Split the balanced data into training and testing sets\nset.seed(123)\nsplit_bal &lt;- initial_split(data_bal, prop = 0.7, strata = stroke)\ntrain_bal &lt;- training(split_bal)\ntest_bal  &lt;- testing(split_bal)\n\n# Confirm that train_bal was created\ncat(\"Balanced training data created successfully. Dimensions:\\n\")\n\nBalanced training data created successfully. Dimensions:\n\ndim(train_bal)\n\n[1] 6803   11\n\n# Workflow for logistic regression\nlog_reg_wf &lt;- workflow() %&gt;%\n  add_recipe(recipe_spec) %&gt;%\n  add_model(log_reg_spec)\n\n# Train on imbalanced data\nfit_log_reg_imb &lt;- fit(log_reg_wf, data = train_imb)\npreds_log_reg_imb &lt;- predict(fit_log_reg_imb, test_imb) %&gt;%\n  bind_cols(test_imb %&gt;% select(stroke))\n\n# Train on balanced data\nfit_log_reg_bal &lt;- fit(log_reg_wf, data = train_bal)\npreds_log_reg_bal &lt;- predict(fit_log_reg_bal, test_bal) %&gt;%\n  bind_cols(test_bal %&gt;% select(stroke))\n\n\n# Evaluate performance\nmetrics_log_reg_imb &lt;- metrics(preds_log_reg_imb, truth = stroke, estimate = .pred_class)\nmetrics_log_reg_bal &lt;- metrics(preds_log_reg_bal, truth = stroke, estimate = .pred_class)\n\ncat(\"Baseline (Logistic Regression) - Imbalanced Data:\\n\")\n\nBaseline (Logistic Regression) - Imbalanced Data:\n\nprint(metrics_log_reg_imb)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.952 \n2 kap      binary        0.0251\n\ncat(\"\\nBaseline (Logistic Regression) - Balanced Data:\\n\")\n\n\nBaseline (Logistic Regression) - Balanced Data:\n\nprint(metrics_log_reg_bal)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.772\n2 kap      binary         0.544\n\n\nAs the paper notes, the baseline model’s performance improves significantly on the balanced dataset.\n\n\n4.3.2 Advanced Models (Random Forest and XGBoost)\n\n# --- Random Forest ---\nrf_wf &lt;- workflow() |&gt; add_recipe(recipe_spec) |&gt; add_model(rf_spec)\nfit_rf_bal &lt;- fit(rf_wf, data = train_bal)\npreds_rf_bal &lt;- predict(fit_rf_bal, test_bal) |&gt; bind_cols(test_bal |&gt; select(stroke))\nmetrics_rf_bal &lt;- metrics(preds_rf_bal, truth = stroke, estimate = .pred_class)\n\n# --- XGBoost ---\nxgb_wf &lt;- workflow() |&gt; add_recipe(recipe_spec) |&gt; add_model(xgb_spec)\nfit_xgb_bal &lt;- fit(xgb_wf, data = train_bal)\npreds_xgb_bal &lt;- predict(fit_xgb_bal, test_bal) |&gt; bind_cols(test_bal |&gt; select(stroke))\nmetrics_xgb_bal &lt;- metrics(preds_xgb_bal, truth = stroke, estimate = .pred_class)\n\ncat(\"\\nAdvanced Model (Random Forest) - Balanced Data:\\n\")\n\n\nAdvanced Model (Random Forest) - Balanced Data:\n\nprint(metrics_rf_bal)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.868\n2 kap      binary         0.737\n\ncat(\"\\nAdvanced Model (XGBoost) - Balanced Data:\\n\")\n\n\nAdvanced Model (XGBoost) - Balanced Data:\n\nprint(metrics_xgb_bal)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.856\n2 kap      binary         0.713\n\n# Confusion Matrix for XGBoost on balanced data\nconf_mat_xgb &lt;- conf_mat(preds_xgb_bal, truth = stroke, estimate = .pred_class)\nautoplot(conf_mat_xgb, type = \"heatmap\") + ggtitle(\"XGBoost Confusion Matrix (Balanced Data)\")\n\n\n\n\n\n\n\n\nOn the balanced dataset, XGBoost and Random Forest perform exceptionally well, achieving high accuracy and balanced precision/recall, aligning with the paper’s findings that these models are top performers.\n\n\n\n4.4 Dense Stacking Ensemble (DSE) Model\nThe paper’s key contribution is a DSE model, which uses the best-performing model (Random Forest) as a meta-classifier. We can build a similar ensemble using the stacks package.\n\n# Define k-fold cross-validation\nfolds &lt;- vfold_cv(train_bal, v = 10, strata = stroke)\n\n# Control settings to save predictions\nctrl_grid &lt;- control_stack_grid()\n\n# Fit models with cross-validation\nlog_reg_res &lt;- fit_resamples(log_reg_wf, resamples = folds, control = ctrl_grid)\nrf_res &lt;- fit_resamples(rf_wf, resamples = folds, control = ctrl_grid)\nxgb_res &lt;- fit_resamples(xgb_wf, resamples = folds, control = ctrl_grid)\n\n\n# Initialize a data stack\nstroke_stack &lt;- stacks() |&gt;\n  add_candidates(log_reg_res) |&gt;\n  add_candidates(rf_res) |&gt;\n  add_candidates(xgb_res)\n\n# Blend predictions to create the ensemble\nensemble_model &lt;- blend_predictions(stroke_stack, penalty = 0.1)\nfit_ensemble &lt;- fit_members(ensemble_model)\n\n\n# Evaluate the DSE model on the test set\npreds_ensemble &lt;- predict(fit_ensemble, test_bal) |&gt;\n  bind_cols(test_bal |&gt; select(stroke))\nmetrics_ensemble &lt;- metrics(preds_ensemble, truth = stroke, estimate = .pred_class)\n\n\ncat(\"\\nDense Stacking Ensemble (DSE) Model Performance - Balanced Data:\\n\")\n\n\nDense Stacking Ensemble (DSE) Model Performance - Balanced Data:\n\nprint(metrics_ensemble)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.867\n2 kap      binary         0.734\n\n\nThe DSE model achieves an accuracy of over 96%, demonstrating the power of ensembling. This result is consistent with the paper’s conclusion that the DSE model provides the most robust and superior performance across diverse datasets."
  },
  {
    "objectID": "posts/renan-blog-post-week6/index.html#conclusion",
    "href": "posts/renan-blog-post-week6/index.html#conclusion",
    "title": "Dataset Exploration - Week 6",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nThis document successfully reproduced the core findings of the study “Predictive modelling and identification of key risk factors for stroke using machine learning.” Through this R-based implementation, we confirmed that:\n\nHandling missing data and class imbalance is crucial for building accurate predictive models in healthcare.\nThe key risk factors identified—age, BMI, average glucose level, hypertension, and heart disease—are indeed highly predictive of stroke risk.\nWhile individual models like XGBoost and Random Forest perform well, a Dense Stacking Ensemble (DSE) model delivers the highest and most stable performance, achieving accuracy greater than 96%.\n\nThe DSE model’s ability to combine the strengths of multiple algorithms makes it an excellent candidate for real-world clinical applications, potentially aiding in the early detection of stroke and improving patient outcomes.\n\nReferences\n\n\n1. Hassan A, Gulzar Ahmad S, Ullah Munir E, Ali Khan I, Ramzan N. Predictive modelling and identification of key risk factors for stroke using machine learning. Scientific Reports. 2024;14(1):11498."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Literature Review",
    "section": "",
    "text": "These are the literature review done by all the students during this semester.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset Exploration - Week 6\n\n\n\ndataset exploration\n\nweek 6\n\nrenan\n\n\n\nFor Week 6 we are exploring the Stroke Dataset\n\n\n\n\n\nRenan Monteiro Barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 2\n\n\n\nliterature review\n\nweek 2\n\nkristina\n\n\n\nLiterature review for the Week 2 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 2\n\n\n\nliterature review\n\nweek 2\n\nrenan\n\n\n\nLiterature review for the Week 2 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 2\n\n\n\nliterature review\n\nweek 2\n\nshree\n\n\n\nLiterature review for the Week 2 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 3\n\n\n\nliterature review\n\nweek 3\n\nkristina\n\n\n\nLiterature review for the Week 3 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 3\n\n\n\nliterature review\n\nweek 3\n\nrenan\n\n\n\nLiterature review for the Week 3 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 3\n\n\n\nliterature review\n\nweek 3\n\nshree\n\n\n\nLiterature review for the Week 3 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 4\n\n\n\nliterature review\n\nweek 4\n\nrenan\n\n\n\nLiterature review for the Week 4 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 4\n\n\n\nliterature review\n\nweek 4\n\nkristina\n\n\n\nLiterature review for the Week 4 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 4\n\n\n\nliterature review\n\nweek 4\n\nshree\n\n\n\nLiterature review for the Week 4 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 5\n\n\n\nliterature review\n\nweek 5\n\nshree\n\n\n\nLiterature review for the Week 5 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nShree Krishna Basnet\n\n\n\n\n\n\n\n\n\n\n\n\nLiterature Review Week 5\n\n\n\nliterature review\n\nweek 5\n\nkristina\n\n\n\nLiterature review for the Week 5 of the course IDC-6940 for Fall 2025\n\n\n\n\n\nKristina Kusem\n\n\n\n\n\n\n\n\n\n\n\n\nProject Setup - Week 5\n\n\n\ngetting started\n\nweek 5\n\nrenan\n\n\n\nFor Week 5 setting up Quarto Website and Getting Started with R project\n\n\n\n\n\nRenan Monteiro Barbosa\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html",
    "href": "people/index.html",
    "title": "Meet the Group",
    "section": "",
    "text": "Graduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html#graduate-students",
    "href": "people/index.html#graduate-students",
    "title": "Meet the Group",
    "section": "",
    "text": "Graduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | Univevrsity of West Florida\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/kusem-kristina/index.html#education",
    "href": "people/kusem-kristina/index.html#education",
    "title": "Kristina Kusem",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | Univevrsity of West Florida"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Welcome to our team! We are a group of collaborators dedicated to…\n\n\n\nRenan Monteiro Barbosa\n\nData Scientist\n\n\n\nRenan Monteiro Barbosa\n\n\nJane is a data scientist with over 10 years of experience…\nContact * Email: jane@example.com * GitHub: github.com/jane\n\n\n\nKristina Kusem\n\nSoftware Engineer\n\n\n\nKristina Kusem\n\n\nJohn is a software engineer specializing in web development…\nContact * Email: john@example.com * Twitter: @john_smith\n\n\n\nShree Krishna Basnet\n\nResearch Analyst\n\n\n\nShree Krishna Basnet\n\n\nAlex works on developing new research methodologies for…\nContact * Email: alex@example.com * Website: alex-johnson.com"
  },
  {
    "objectID": "posts/shree-blog-post-week4/index.html",
    "href": "posts/shree-blog-post-week4/index.html",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "Regularized logistic regression with network‐based pairwise interaction for biomarker identification in breast cancer (Wu et al., 2016) (1)\n\nGoal: Regularized logistic regression should be used, along with network (biological network) information and pairwise interactions, to find biomarkers (both single and interacting pairs) for breast cancer.\n\nLink: https://www.researchgate.net/publication/296193700_Regularized_logistic_regression_with_network-based_pairwise_interaction_for_biomarker_identification_in_breast_cancer\nWhat made this paper interesting, or why the analysis is important: different biological processes has combined interactions between genes and proteins rather than single genes or proteins, like network topology or interaction information, which may result in better biomarkers that are biologically useful rather than statistically significant. Associating network knowledge may improve prediction or interpretability.\nMethodology: The analyst used a regularized logistic regression model that incorporates pairwise connections and protein-protein interaction (PPI) networks. they prioritized biologically plausible biomarker combinations and used an adaptive elastic net (a penalty that balances l1 and l2) with network constraints. Used breast cancer datasets (gene expression data) to discover key nodes and relationships.\nResult/conclusion: Their model outperforms simpler models in terms of predictive performance, and they were able to discover both individual biomarkers and interacting gene pairs. The interactions has been found to have some biological sense.\nLimitation: The risk of overfitting and model size are increased by the intricacy of incorporating relationships. The quality of the network and expression data determines the outcomes.\n\n\nUsing Genetic Algorithms and Sparse Logistic Regression to Find Gene Signatures for Chemosensitivity Prediction in Breast Cancer. (2)\nGoal: To identify “gene signatures” that predict chemosensitivity, that is, which tumors react to chemotherapy in breast cancer, combine genetic algorithms with sparse logistic regression.\nWhat made this paper interesting, or why the analysis is important:Predicting which patients will react to chemotherapy gives more personalized treatment. Potential biomarkers include gene signatures. However, there are several genes and possible combinations, like genetic algorithms that aid in searching space, while sparse logistic regression aids in reducing characteristics.\nMethodology: To create individuals’ “gene signature” subsets, first choose genes using a Genetic Algorithm (GA) from among overexpressed genes (or pathway-specific genes) and forecast response, create sparse logistic regression models using those subsets. Assess accuracy, sensitivity, specificity, and other metrics using both a training and a validation set.\nResult/ Conclusion : The results show that SLR-28 and Notch-86, two gene signatures, perform well on training and validation sets in terms of accuracy, specificity, sensitivity, and other metrics. In some reults we can see it performs better than previous signatures.\nLimitation: Generalization is uncertain due to the relatively small datasets. Randomness is added by the GA, signature stability may differ. Clinical validation also comes at a high expense. overfitting risk.\n\n\n\n\n1. Wu MY, Zhang XF, Dai DQ, Ou-Yang L, Zhu Y, Yan H. Regularized logistic regression with network-based pairwise interaction for biomarker identification in breast cancer. BMC bioinformatics. 2016;17(1):108. \n\n\n2. Hu W. Using genetic algorithms and sparse logistic regression to find gene signatures for chemosensitivity prediction in breast cancer. American Journal of Bioscience and Bioengineering. 2016;4(2):26–33."
  },
  {
    "objectID": "posts/shree-blog-post-week4/index.html#article-2",
    "href": "posts/shree-blog-post-week4/index.html#article-2",
    "title": "Literature Review Week 4",
    "section": "",
    "text": "Using Genetic Algorithms and Sparse Logistic Regression to Find Gene Signatures for Chemosensitivity Prediction in Breast Cancer. (2)\nGoal: To identify “gene signatures” that predict chemosensitivity, that is, which tumors react to chemotherapy in breast cancer, combine genetic algorithms with sparse logistic regression.\nWhat made this paper interesting, or why the analysis is important:Predicting which patients will react to chemotherapy gives more personalized treatment. Potential biomarkers include gene signatures. However, there are several genes and possible combinations, like genetic algorithms that aid in searching space, while sparse logistic regression aids in reducing characteristics.\nMethodology: To create individuals’ “gene signature” subsets, first choose genes using a Genetic Algorithm (GA) from among overexpressed genes (or pathway-specific genes) and forecast response, create sparse logistic regression models using those subsets. Assess accuracy, sensitivity, specificity, and other metrics using both a training and a validation set.\nResult/ Conclusion : The results show that SLR-28 and Notch-86, two gene signatures, perform well on training and validation sets in terms of accuracy, specificity, sensitivity, and other metrics. In some reults we can see it performs better than previous signatures.\nLimitation: Generalization is uncertain due to the relatively small datasets. Randomness is added by the GA, signature stability may differ. Clinical validation also comes at a high expense. overfitting risk.\n\n\n\n\n1. Wu MY, Zhang XF, Dai DQ, Ou-Yang L, Zhu Y, Yan H. Regularized logistic regression with network-based pairwise interaction for biomarker identification in breast cancer. BMC bioinformatics. 2016;17(1):108. \n\n\n2. Hu W. Using genetic algorithms and sparse logistic regression to find gene signatures for chemosensitivity prediction in breast cancer. American Journal of Bioscience and Bioengineering. 2016;4(2):26–33."
  },
  {
    "objectID": "posts/renan-blog-post-week5/index.html",
    "href": "posts/renan-blog-post-week5/index.html",
    "title": "Project Setup - Week 5",
    "section": "",
    "text": "This week we are getting started on how to setup the Quarto and R project for proper Collaboration.\nThis post will demonstrate how to install RENV, initate your Renv environment and then load the dataset and do some demonstrations manipulating the dataset.\nWe will be using the dataset: Stroke Prediction Dataset"
  },
  {
    "objectID": "posts/renan-blog-post-week5/index.html#what-is-renv",
    "href": "posts/renan-blog-post-week5/index.html#what-is-renv",
    "title": "Project Setup - Week 5",
    "section": "What is Renv",
    "text": "What is Renv\nrenv is a pakcage manager that helps you create reproducible environments for your R projects.\nInstall the latest version of renv from CRAN with:\n```{r}\ninstall.packages(\"renv\")\n```\n\nRenv Workflow\nUse renv::init() to initialize renv in a new or existing project. This will set up a project library, containing all the packages you’re currently using. The packages (and all the metadata needed to reinstall them) are recorded into a lockfile, renv.lock, and a .Rprofile ensures that the library is used every time you open that project.\nAs you continue to work on your project, you will install and upgrade packages, either using install.packages() and update.packages() or renv::install() and renv::update(). After you’ve confirmed your code works as expected, use renv::snapshot() to record the packages and their sources in the lockfile.\nLater, if you need to share your code with someone else or run your code on new machine, your collaborator (or you) can call renv::restore() to reinstall the specific package versions recorded in the lockfile.\n\n\nLearning more\nIf this is your first time using renv, we strongly recommend starting with the Introduction to renv vignette: this will help you understand the most important verbs and nouns of renv.\nIf you have a question about renv, please first check the FAQ to see whether your question has already been addressed. If it hasn’t, please feel free to ask on the Posit Forum.\nIf you believe you’ve found a bug in renv, please file a bug (and, if possible, a reproducible example) at https://github.com/rstudio/renv/issues."
  },
  {
    "objectID": "posts/renan-blog-post-week5/index.html#import-dataset-example",
    "href": "posts/renan-blog-post-week5/index.html#import-dataset-example",
    "title": "Project Setup - Week 5",
    "section": "Import Dataset Example",
    "text": "Import Dataset Example\nGet the packages setup:\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\nlibrary(fitdistrplus)\nlibrary(gsheet)\nlibrary(boot)\nlibrary(readr)\n\n\nImport the dataset\nThis should find the path to the datasets folder programatically.\n\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\n# repo_root\n# datasets_path\n\nNow we define the dataset we want to load, healthcare-dataset-stroke-data.csv will be inside kaggle-healthcare-dataset-stroke-data.\n\nkaggle_dataset_path &lt;- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\n\nkaggle_data1 = read_csv(kaggle_dataset_path, show_col_types = FALSE)\n\nExploring the dataset, BMI is not stored as numeric value also the NA fields are stored as text “N/A”.\n\nhead(kaggle_data1)\n\n# A tibble: 6 × 12\n     id gender   age hypertension heart_disease ever_married work_type    \n  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        \n1  9046 Male      67            0             1 Yes          Private      \n2 51676 Female    61            0             0 Yes          Self-employed\n3 31112 Male      80            0             1 Yes          Private      \n4 60182 Female    49            0             0 Yes          Private      \n5  1665 Female    79            1             0 Yes          Self-employed\n6 56669 Male      81            0             0 Yes          Private      \n# ℹ 5 more variables: Residence_type &lt;chr&gt;, avg_glucose_level &lt;dbl&gt;, bmi &lt;chr&gt;,\n#   smoking_status &lt;chr&gt;, stroke &lt;dbl&gt;\n\n# Count total NAs per column\ncolSums(is.na(kaggle_data1))\n\n               id            gender               age      hypertension \n                0                 0                 0                 0 \n    heart_disease      ever_married         work_type    Residence_type \n                0                 0                 0                 0 \navg_glucose_level               bmi    smoking_status            stroke \n                0                 0                 0                 0 \n\n\nApparently seems there is no NA values. Let’s continue.\n\n# overall\nsummary(kaggle_data1)\n\n       id           gender               age         hypertension    \n Min.   :   67   Length:5110        Min.   : 0.08   Min.   :0.00000  \n 1st Qu.:17741   Class :character   1st Qu.:25.00   1st Qu.:0.00000  \n Median :36932   Mode  :character   Median :45.00   Median :0.00000  \n Mean   :36518                      Mean   :43.23   Mean   :0.09746  \n 3rd Qu.:54682                      3rd Qu.:61.00   3rd Qu.:0.00000  \n Max.   :72940                      Max.   :82.00   Max.   :1.00000  \n heart_disease     ever_married        work_type         Residence_type    \n Min.   :0.00000   Length:5110        Length:5110        Length:5110       \n 1st Qu.:0.00000   Class :character   Class :character   Class :character  \n Median :0.00000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :0.05401                                                           \n 3rd Qu.:0.00000                                                           \n Max.   :1.00000                                                           \n avg_glucose_level     bmi            smoking_status         stroke       \n Min.   : 55.12    Length:5110        Length:5110        Min.   :0.00000  \n 1st Qu.: 77.25    Class :character   Class :character   1st Qu.:0.00000  \n Median : 91.89    Mode  :character   Mode  :character   Median :0.00000  \n Mean   :106.15                                          Mean   :0.04873  \n 3rd Qu.:114.09                                          3rd Qu.:0.00000  \n Max.   :271.74                                          Max.   :1.00000  \n\nglimpse(kaggle_data1)\n\nRows: 5,110\nColumns: 12\n$ id                &lt;dbl&gt; 9046, 51676, 31112, 60182, 1665, 56669, 53882, 10434…\n$ gender            &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Male\"…\n$ age               &lt;dbl&gt; 67, 61, 80, 49, 79, 81, 74, 69, 59, 78, 81, 61, 54, …\n$ hypertension      &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1…\n$ heart_disease     &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0…\n$ ever_married      &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No…\n$ work_type         &lt;chr&gt; \"Private\", \"Self-employed\", \"Private\", \"Private\", \"S…\n$ Residence_type    &lt;chr&gt; \"Urban\", \"Rural\", \"Rural\", \"Urban\", \"Rural\", \"Urban\"…\n$ avg_glucose_level &lt;dbl&gt; 228.69, 202.21, 105.92, 171.23, 174.12, 186.21, 70.0…\n$ bmi               &lt;chr&gt; \"36.6\", \"N/A\", \"32.5\", \"34.4\", \"24\", \"29\", \"27.4\", \"…\n$ smoking_status    &lt;chr&gt; \"formerly smoked\", \"never smoked\", \"never smoked\", \"…\n$ stroke            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nSummary give some interesting insights but glimpse shows that there are NA values, even worse, the BMI values are stored and strings and should be numeric.\nNow lets explore the Categorical and Numeric variables.\n\n# check categorical variables\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Check one by one, lets see what we got\n# kaggle_data1 %&gt;% count(gender)\n# kaggle_data1 %&gt;% count(hypertension)\n# kaggle_data1 %&gt;% count(heart_disease)\n# kaggle_data1 %&gt;% count(ever_married)\n# kaggle_data1 %&gt;% count(work_type)\n# kaggle_data1 %&gt;% count(Residence_type )\n# kaggle_data1 %&gt;% count(smoking_status)\n# kaggle_data1 %&gt;% count(stroke)\n\n# Now make it a little cleaner\ncat_vars &lt;- c(\"gender\", \"hypertension\", \"heart_disease\", \"ever_married\",\n              \"work_type\", \"Residence_type\", \"smoking_status\", \"stroke\")\n\nkaggle_data1[, cat_vars] %&gt;%\n  # Convert all to character to avoid type conflicts\n  mutate_all(as.character) %&gt;%\n  pivot_longer(cols = names(.), names_to = \"variable\", values_to = \"value\") %&gt;%\n  count(variable, value) %&gt;%\n  arrange(variable, desc(n)) %&gt;% print(n = 22)\n\n# A tibble: 22 × 3\n   variable       value               n\n   &lt;chr&gt;          &lt;chr&gt;           &lt;int&gt;\n 1 Residence_type Urban            2596\n 2 Residence_type Rural            2514\n 3 ever_married   Yes              3353\n 4 ever_married   No               1757\n 5 gender         Female           2994\n 6 gender         Male             2115\n 7 gender         Other               1\n 8 heart_disease  0                4834\n 9 heart_disease  1                 276\n10 hypertension   0                4612\n11 hypertension   1                 498\n12 smoking_status never smoked     1892\n13 smoking_status Unknown          1544\n14 smoking_status formerly smoked   885\n15 smoking_status smokes            789\n16 stroke         0                4861\n17 stroke         1                 249\n18 work_type      Private          2925\n19 work_type      Self-employed     819\n20 work_type      children          687\n21 work_type      Govt_job          657\n22 work_type      Never_worked       22\n\n\nIts pretty interesting, now lets see what happens with the numeric variables\n\n# Check Numeric Variables - id, age, avg_glucose_level, bmi\nkaggle_data1 %&gt;%\n  select_if(is.numeric) %&gt;%\n  summary()\n\n       id             age         hypertension     heart_disease    \n Min.   :   67   Min.   : 0.08   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:17741   1st Qu.:25.00   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :36932   Median :45.00   Median :0.00000   Median :0.00000  \n Mean   :36518   Mean   :43.23   Mean   :0.09746   Mean   :0.05401  \n 3rd Qu.:54682   3rd Qu.:61.00   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :72940   Max.   :82.00   Max.   :1.00000   Max.   :1.00000  \n avg_glucose_level     stroke       \n Min.   : 55.12    Min.   :0.00000  \n 1st Qu.: 77.25    1st Qu.:0.00000  \n Median : 91.89    Median :0.00000  \n Mean   :106.15    Mean   :0.04873  \n 3rd Qu.:114.09    3rd Qu.:0.00000  \n Max.   :271.74    Max.   :1.00000  \n\n\nWe need to deal with the BMI data which has missing values and its not stored as numerical.\n\n# unique(kaggle_data1$bmi)\nkaggle_data2 &lt;- kaggle_data1 %&gt;%\n  mutate(bmi = na_if(bmi, \"N/A\")) %&gt;%   # Convert \"N/A\" string to NA\n  mutate(bmi = as.numeric(bmi))         # Convert from character to numeric\n\n# kaggle_data2 &lt;- kaggle_data1 %&gt;% mutate(bmi = as.numeric(na_if(bmi, \"N/A\")))\n\n# Check if it worked\nstr(kaggle_data2$bmi)\n\n num [1:5110] 36.6 NA 32.5 34.4 24 29 27.4 22.8 NA 24.2 ...\n\nsum(is.na(kaggle_data2$bmi))\n\n[1] 201\n\n# Check Numeric Variables - id, age, avg_glucose_level, bmi\nkaggle_data2 %&gt;%\n  select_if(is.numeric) %&gt;%\n  summary()\n\n       id             age         hypertension     heart_disease    \n Min.   :   67   Min.   : 0.08   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:17741   1st Qu.:25.00   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :36932   Median :45.00   Median :0.00000   Median :0.00000  \n Mean   :36518   Mean   :43.23   Mean   :0.09746   Mean   :0.05401  \n 3rd Qu.:54682   3rd Qu.:61.00   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :72940   Max.   :82.00   Max.   :1.00000   Max.   :1.00000  \n                                                                    \n avg_glucose_level      bmi            stroke       \n Min.   : 55.12    Min.   :10.30   Min.   :0.00000  \n 1st Qu.: 77.25    1st Qu.:23.50   1st Qu.:0.00000  \n Median : 91.89    Median :28.10   Median :0.00000  \n Mean   :106.15    Mean   :28.89   Mean   :0.04873  \n 3rd Qu.:114.09    3rd Qu.:33.10   3rd Qu.:0.00000  \n Max.   :271.74    Max.   :97.60   Max.   :1.00000  \n                   NA's   :201"
  },
  {
    "objectID": "posts/renan-blog-post-week5/index.html#conclusion",
    "href": "posts/renan-blog-post-week5/index.html#conclusion",
    "title": "Project Setup - Week 5",
    "section": "Conclusion",
    "text": "Conclusion\nThe dataset is imbalanced and has many issues there are several research work that explore solutions:\n\nMachine learning for stroke prediction using imbalanced data (1)\nPredictive modelling and identification of key risk factors for stroke using machine learning (2)\n\nThe research Predictive modelling and identification of key risk factors for stroke using machine learning has made several contributions adding a lot of insights:\n\nExploring various data imputation techniques and addressing data imbalance issues in order to enhance the accuracy and robustness of stroke prediction models.\nIdentifying crucial features for stroke prediction and uncovering previously unknown risk factors, giving a comprehensive understanding of stroke risk assessment.\nCreating an augmented dataset incorporating important key risk factor features using the imputed datasets, enhancing the effectiveness of stroke prediction models.\nAssessing the effectiveness of advanced machine learning models across different datasets and creating a robust Dense Stacking Ensemble model for stroke prediction.\nThe key contribution is showcasing the enhanced predictive capabilities of the model in accurately identifying and testing strokes, surpassing the performance of prior studies that utilized the same dataset.\n\n\n\n\n\n\n\nNote\n\n\n\nLarge datasets might need Github LFS which is not setup, therefore must store then externally."
  },
  {
    "objectID": "posts/renan-blog-post-week5/index.html#additional-thoughts",
    "href": "posts/renan-blog-post-week5/index.html#additional-thoughts",
    "title": "Project Setup - Week 5",
    "section": "Additional Thoughts",
    "text": "Additional Thoughts\nQuarto websites when combined with python and R is a great way to\nQuarto websites, when combined with Python and R, offer a powerful way to create dynamic, data-driven content that turns out into amazing presentations rich in visual content.\nHowever there are limitations, Github Actions runner is not powerful and before submitting the project for rendering must take that into consideration. On future work will evaluate solutions to the computational budget limitations in Github Action Runner.\nHow to efficiently break up a computationally heavy article into separate notebooks?#8410\nSome have mentioned that the project can be split into sections.\n\nReferences\n\n\n1. Melnykova N, Patereha Y, Skopivskyi S, Farion M, Fedushko S, Drohomyretska K. Machine learning for stroke prediction using imbalanced data. Scientific Reports. 2025;15(1):33773. \n\n\n2. Hassan A, Gulzar Ahmad S, Ullah Munir E, Ali Khan I, Ramzan N. Predictive modelling and identification of key risk factors for stroke using machine learning. Scientific Reports. 2024;14(1):11498."
  },
  {
    "objectID": "posts/kristina-blog-post-week5/index.html",
    "href": "posts/kristina-blog-post-week5/index.html",
    "title": "Literature Review Week 5",
    "section": "",
    "text": "Article Title: Identification of factors influencing severity of motorcycle crashes in Dhaka, Bangladesh using binary logistic regression model. (1)\nAuthors: Hamidur Rahman, Niaz Mahmud Zafri, Tamanna Akter, Shahrior Pervaz\nProblem: - One of the most common unnatural causes of death across the world is road accidents, so it is important to identify strong predictors associated with such accidents. - According to the World Health Organization, most of the road crash deaths that occur worldwide happen in developing countries. - Researchers focus on motorcycle crashes in Dhaka, the capital city of Bangladesh. It is stated that the rate of road crashes in Bangladesh is significantly greater than other developing countries, and Dhaka has the greatest amount of motorists and reported motorcycle crashes. - It is noted that most research about this topic is done on data from developed countries and not developing countries. So the conclusions drawn from existing studies may not be applicable to the problems the developing countries are facing - Knowing which predictors are strongly associated with motorcycle crashes in Dhaka helps builders and developers eliminate or reduce these risk factors as they are building new roads. This study serves as a step in preventing more motorcycle road crashes as developing countries are being built.\nSolution: - Researchers conduct a binary logistic regression analysis to identify predictors most strongly associated with the occurrence of the outcome, motorcycle crash severity (fatal/ non- fatal) - They began the study by choosing predictors identified in previous literature about similar topics. Commonly identified predictors of motorcycle crashes are grouped into five broad categories: environment, road characteristics, driver characteristics, motorcycle features, and type of collision. - The binary logistic regression equation is given as the log odds of the probability of the occurrence of the outcome. An explanation of every variable in the equation is given (slopes, intercept, odds ratio, and relation to the outcome variable).\nData: - Data was collected from 2006 to 2015 from the Accident Research Institute of Bangladesh University of Engineering and Technology. Only 316 data points were used, and each contained information about motorcycle crashes. - There are five broad categories encompassing all predictors. The five categories and predictors are: 1. Environmental factors- date/time, lighting, weather 2. Collision type- five different types of collisions 3. Driver characteristics- gender, age, alcohol consumption, and use of a helmet 4. Road characteristics-location, traffic characteristics, road conditions 5. vehicle characteristics- type of other vehicle in crash, weight of other vehicle in crash, motorcycle condition, and more - The outcome variable, crash injury severity, originally had four levels. Observations from this predictor were then reclassified as either “fatal” or “non-fatal” for a binary outcome. - To determine which predictors to include in the dataset, researchers conducted a univariate analysis and a chi- square test of each individual predictor to assess significance. All significant predicators were then chosen for the dataset, used for the binary logistic regression. After this, multicollinearity was assessed using the VIF and none of the predictors showed multicollinearity.\nResults/ Conclusions: - After conducting the binary logistic regression, 11 out of the 16 included predictors were found to be significantly associated with the outcome. The significant predictors were day of the week, seasonal condition, time of day, three types of road characteristics, crash type, condition of motorcycle, type of other vehicle in accident, use of helmet, and alcohol consumption - The regression curve from the analysis was also found to be significant - Goodness of fit was assessed using a test called the Hosmer and Lemeshow test - The discussion section details each significant predictor, and interpretations of slopes are given in relation to the outcome variable and the reference groups. - Some of the findings were consistent with other studies, and some of the findings contradict conclusions in previous studies. - Most notable conclusions from this study that researchers believe would improve road safety and reduce motorcycle accident severity in developing countries: 1. better lighting conditions for enhanced visibility 2. solution to wet/ slippery roads during rainy season 3. educate drivers about how to drive during unsafe conditions, such as nighttime, heavy rain, and heavy traffic on weekends. Also educate drivers to follow proper safety and speeding regulations. And better education/ training/ evaluation for drivers operating larger vehicles 4. improved pedestrian walkways and road areas 5. strict enforcement of laws regarding helmet use and alcohol while driving Limitations: - Missing information in the data: Some important predictors were entirely excluded from the study due to missing information in the dataset. So there may be some extremely relevant predictors that have yet to be studied. There is also an ongoing issue of accidents that go unreported due to lack of fatality, and drivers do not report these incident."
  },
  {
    "objectID": "posts/kristina-blog-post-week5/index.html#article-1",
    "href": "posts/kristina-blog-post-week5/index.html#article-1",
    "title": "Literature Review Week 5",
    "section": "",
    "text": "Article Title: Identification of factors influencing severity of motorcycle crashes in Dhaka, Bangladesh using binary logistic regression model. (1)\nAuthors: Hamidur Rahman, Niaz Mahmud Zafri, Tamanna Akter, Shahrior Pervaz\nProblem: - One of the most common unnatural causes of death across the world is road accidents, so it is important to identify strong predictors associated with such accidents. - According to the World Health Organization, most of the road crash deaths that occur worldwide happen in developing countries. - Researchers focus on motorcycle crashes in Dhaka, the capital city of Bangladesh. It is stated that the rate of road crashes in Bangladesh is significantly greater than other developing countries, and Dhaka has the greatest amount of motorists and reported motorcycle crashes. - It is noted that most research about this topic is done on data from developed countries and not developing countries. So the conclusions drawn from existing studies may not be applicable to the problems the developing countries are facing - Knowing which predictors are strongly associated with motorcycle crashes in Dhaka helps builders and developers eliminate or reduce these risk factors as they are building new roads. This study serves as a step in preventing more motorcycle road crashes as developing countries are being built.\nSolution: - Researchers conduct a binary logistic regression analysis to identify predictors most strongly associated with the occurrence of the outcome, motorcycle crash severity (fatal/ non- fatal) - They began the study by choosing predictors identified in previous literature about similar topics. Commonly identified predictors of motorcycle crashes are grouped into five broad categories: environment, road characteristics, driver characteristics, motorcycle features, and type of collision. - The binary logistic regression equation is given as the log odds of the probability of the occurrence of the outcome. An explanation of every variable in the equation is given (slopes, intercept, odds ratio, and relation to the outcome variable).\nData: - Data was collected from 2006 to 2015 from the Accident Research Institute of Bangladesh University of Engineering and Technology. Only 316 data points were used, and each contained information about motorcycle crashes. - There are five broad categories encompassing all predictors. The five categories and predictors are: 1. Environmental factors- date/time, lighting, weather 2. Collision type- five different types of collisions 3. Driver characteristics- gender, age, alcohol consumption, and use of a helmet 4. Road characteristics-location, traffic characteristics, road conditions 5. vehicle characteristics- type of other vehicle in crash, weight of other vehicle in crash, motorcycle condition, and more - The outcome variable, crash injury severity, originally had four levels. Observations from this predictor were then reclassified as either “fatal” or “non-fatal” for a binary outcome. - To determine which predictors to include in the dataset, researchers conducted a univariate analysis and a chi- square test of each individual predictor to assess significance. All significant predicators were then chosen for the dataset, used for the binary logistic regression. After this, multicollinearity was assessed using the VIF and none of the predictors showed multicollinearity.\nResults/ Conclusions: - After conducting the binary logistic regression, 11 out of the 16 included predictors were found to be significantly associated with the outcome. The significant predictors were day of the week, seasonal condition, time of day, three types of road characteristics, crash type, condition of motorcycle, type of other vehicle in accident, use of helmet, and alcohol consumption - The regression curve from the analysis was also found to be significant - Goodness of fit was assessed using a test called the Hosmer and Lemeshow test - The discussion section details each significant predictor, and interpretations of slopes are given in relation to the outcome variable and the reference groups. - Some of the findings were consistent with other studies, and some of the findings contradict conclusions in previous studies. - Most notable conclusions from this study that researchers believe would improve road safety and reduce motorcycle accident severity in developing countries: 1. better lighting conditions for enhanced visibility 2. solution to wet/ slippery roads during rainy season 3. educate drivers about how to drive during unsafe conditions, such as nighttime, heavy rain, and heavy traffic on weekends. Also educate drivers to follow proper safety and speeding regulations. And better education/ training/ evaluation for drivers operating larger vehicles 4. improved pedestrian walkways and road areas 5. strict enforcement of laws regarding helmet use and alcohol while driving Limitations: - Missing information in the data: Some important predictors were entirely excluded from the study due to missing information in the dataset. So there may be some extremely relevant predictors that have yet to be studied. There is also an ongoing issue of accidents that go unreported due to lack of fatality, and drivers do not report these incident."
  },
  {
    "objectID": "posts/kristina-blog-post-week5/index.html#article-2",
    "href": "posts/kristina-blog-post-week5/index.html#article-2",
    "title": "Literature Review Week 5",
    "section": "Article 2",
    "text": "Article 2\nTitle of article: Risk factors for airplane headache: A multivariate logistic regression analysis in a population of career flight personnel. (2)\nAuthors: Johannes Prottengeier, Isabelle Kaiser, Andreas Moritz, Fabian Konrad\nProblem: - Airplane headache (AH) is a headache disorder described as a headache induced while taking off or landing in an airplane. It was not until 2004 that the disorder was recognized and classified as a medical issue. Because this disorder has just recently been recognized, there is little existing research on it. - There is a lot of previous literature and research regarding other headache types and disorders, but AH is still lacking appropriate research. - Specifically, this study aims to identify predictors and risk factors that occur before onset of airplane headache. Knowing the predictors would help both travelers and employees of airlines. - AH is a common disorder affecting about 65 million people worldwide every year, so helping prevent and treat it is crucial. Future research is therefore a must.\nSolution: - Conduct a logistic regression analysis to determine significant predictors of airplane headache. Two binary logistic regression models were constructed; one model’s outcome was either airplane headache (1) or no headache (0), and the other model’s outcomes were airplane headache (1) or other headache (0). - Used R to conduct statistical analysis\nData: - Data was collected from a voluntary online survey sent out to about 20,000 pilots who fly frequently due to work. A total of 2237 complete questionnaires from a 3 month period in 2014 were received and used in the dataset for this study. - The data/ questions in the survey were determined by pain specialists - Predictors included in the survey were: demographic information, health history, substance use, medication use, stress levels, and headache/ physical symptoms while flying. - The outcome variable was divided into three categories: airplane headache, no headache, and other type of headache. - Predictors were tested for multicollinearity before models were constructed - Data was further reduced using stepwise backward elimination (starting with all predictors and then eliminating least significant predictors)\nResults/ Conclusions: - Survey results revealed that a vast majority of participants said they had some form of headache while flying on an airplane (82%) - The first model comparing airplane headache with no headache was found to have 10 significant predictors. The AUC for this model showed that it had strong predictive power. - The second model comparing airplane headache with other headache was found to have only four significant predictors. The AUC also showed that this model had low predictive power. This is likely attributed to the fact that airplane headache and other types of headaches all have similar predictors, so it is not easy to distinguish between strong predictors for just AH as compared to all types of headaches. - Conclusions: supplementing with folic acid before flight may help reduce risk of airplane headache. Other strong predictors of airplane headache are occupation, work stress, and preexisting headache medical conditions. - Further research about airplane headache is necessary because of all the ongoing negative subsequent events caused by it. It causes people loss of productivity, avoidance behaviors, stress, and anxiety.\nLimitations: - Only 12% of surveyed individuals submitted their survey. It could be the case that only the people who suffered from headaches responded to the survey, so the proportions of individuals with airplane headache in this study are not representative of the entire population. This is called positive selection bias. - Participants in the survey may not be reporting accurate information because a lot of time passes between the event of their airplane travel and the time when they take the survey.\n\nReferences\n\n\n1. Rahman MH, Zafri NM, Akter T, Pervaz S. Identification of factors influencing severity of motorcycle crashes in dhaka, bangladesh using binary logistic regression model. International journal of injury control and safety promotion. 2021;28(2):141–52. \n\n\n2. Prottengeier J, Kaiser I, Moritz A, Konrad F. Risk factors for airplane headache: A multivariable logistic regression analysis in a population of career flight personnel. Cephalalgia. 2025;45(4):03331024251329837."
  },
  {
    "objectID": "posts/shree-blog-post-week2/index.html",
    "href": "posts/shree-blog-post-week2/index.html",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "link: https://robertominguez.altervista.org/DocumentacionAcreditativa/Articulos/GuancheMM13.pdf\nAutoregressive Logistic Regression Applied to Atmospheric Circulation Patterns” (Guanche, Mínguez & Méndez, 2013). (1)\nArticel incorporates autoregressive time dependencies into logistic regression for climate modeling. work with complex climatological dynamics instead of common data set like health or business. Explains both interpretation and simulation capabilities for weather patterns.\nData used : They took data of measured sea-level pressure (SLP) fields to determine daily atmospheric circulation patterns over the Northeastern Atlantic. A limited number of circulation types (weather regimes) are created by summarizing the SLP fields, for example, through clustering or categorization. Data setup for the autoregressive logistic regression applied to weather types” shows how they organized lagged types, covariates,\nSteps on summary: - Sort daily SLP data into distinct circulation categories. - create autoregressive terms, covariates, and lagged indicators (trend, seasonal) - Comparing anticipated and empirical probability allows for diagnostic checks. - Utilize the fitted model to replicate artificial circulatory state sequences. - Check for simulation statistics (frequencies, transitions, persistence) against historical data."
  },
  {
    "objectID": "posts/shree-blog-post-week2/index.html#article-1",
    "href": "posts/shree-blog-post-week2/index.html#article-1",
    "title": "Literature Review Week 2",
    "section": "",
    "text": "link: https://robertominguez.altervista.org/DocumentacionAcreditativa/Articulos/GuancheMM13.pdf\nAutoregressive Logistic Regression Applied to Atmospheric Circulation Patterns” (Guanche, Mínguez & Méndez, 2013). (1)\nArticel incorporates autoregressive time dependencies into logistic regression for climate modeling. work with complex climatological dynamics instead of common data set like health or business. Explains both interpretation and simulation capabilities for weather patterns.\nData used : They took data of measured sea-level pressure (SLP) fields to determine daily atmospheric circulation patterns over the Northeastern Atlantic. A limited number of circulation types (weather regimes) are created by summarizing the SLP fields, for example, through clustering or categorization. Data setup for the autoregressive logistic regression applied to weather types” shows how they organized lagged types, covariates,\nSteps on summary: - Sort daily SLP data into distinct circulation categories. - create autoregressive terms, covariates, and lagged indicators (trend, seasonal) - Comparing anticipated and empirical probability allows for diagnostic checks. - Utilize the fitted model to replicate artificial circulatory state sequences. - Check for simulation statistics (frequencies, transitions, persistence) against historical data."
  },
  {
    "objectID": "posts/shree-blog-post-week2/index.html#article-2",
    "href": "posts/shree-blog-post-week2/index.html#article-2",
    "title": "Literature Review Week 2",
    "section": "Article 2",
    "text": "Article 2\nA Descriptive Study of Variable Discretization and Cost-Sensitive Logistic Regression on Imbalanced Credit Data. (2)\nLink: https://arxiv.org/pdf/1812.10857\nPurpose : The author looks out for a problem related to credit scoring where the minority class (defaults and delinquencies) is comparatively uncommon. Their main concept is to contrast cost-sensitive logistic regression (assigning various misclassification fees) with variable discretization (converting continuous predictors into categorical bins) in order to reduce class bias. When used real credit scoring dataset event rate was 6.68% that was highly imbalanced.\nModels used: - trained logestic regression in different versions\n- Standard logistic regression on continuous predictors (baseline). - Logistic regression on discretized predictors (using different binning strategies). - Cost-sensitive logistic regression on the continuous predictors (i.e. weight adjustments for minority class). - Possibly combined approaches (discretized + cost-sensitive). - They use 10-fold cross-validation to ensure robustness. PMC - Performance metrics include: - ROC / AUC - Type I error (false positive rate) - Type II error (false negative rate) - Accuracy - F1 score - They also examine coefficient estimates and interpretability\nSummary:\nIn their study of unbalanced credit scoring (default rate ~6.68%), Zhang et al. used 10-fold CV to examine standard, discretized, and cost-sensitive logistic regression. They discovered that variable discretization works better than cost-sensitive weighting, producing models that are more resilient, stable, and interpretable. These models also generalize well to other domains, such as biology and wine quality.\n\nReferences\n\n\n1. Guanche Y, Mı́nguez R, Méndez FJ. Autoregressive logistic regression applied to atmospheric circulation patterns. Climate dynamics. 2014;42(1):537–52. \n\n\n2. Zhang L, Ray H, Priestley J, Tan S. A descriptive study of variable discretization and cost-sensitive logistic regression on imbalanced credit data. Journal of Applied Statistics. 2020;47(3):568–81."
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Predicting stroke risk from common health indicators: a binary logistic regression analysis",
    "section": "",
    "text": "The report goes here need some work\nSlides: slides.html"
  },
  {
    "objectID": "report.html#introduction",
    "href": "report.html#introduction",
    "title": "Predicting stroke risk from common health indicators: a binary logistic regression analysis",
    "section": "Introduction",
    "text": "Introduction\nLogistic regression analysis is a type of regression technique that is used when a dataset’s response variable is categorical in nature. When the outcome variable takes on two distinct classes, a binary logistic regression model is used, and when there are more than two classes of the response variable, either a multinomial or ordinal logistic regression model can be utilized (1) (Kutner, 2013). Logistic regression analysis is commonly used across a variety of industries to glean insights about data to make optimal decisions about the data, to predict class labels, and to estimate probability of an event occurring; some fields include medicine, traffic and road engineering, environmental concerns, credit and fraud issues, and more. In this literature review, we take a closer look at logistic regression by discussing some applications of this algorithm in machine learning models, as well as several uses of logistic regression modeling in several peer- reviewed studies.\nMany machine learning algorithms use logistic regression to train a model and make predictions about class labels. One example of this is through text classification to improve natural language processing tasks. The article “From logistic regression to the perceptron algorithm: Exploring gradient descent with large step sizes” investigates similarities between a logistic regression algorithm with gradient descent and a perceptron algorithm. Researchers observed that with very large steps, the logistic regression with gradient descent behaves like a perceptron, which in some sense links it back to the Deep Equilibrium networks study. The conclusions from this paper are counter intuitive, and further research and reflections about classification and optimization theory are encouraged (2) (Tyurin, 2025). Another way logistic regression is used in machine learning is through large language models (LLMs), which are complex neural networks trained on very large datasets to output human language (3) (Pedapati et al., 2024). The paper “Large language model confidence estimation via black-box access” addresses the problem of estimating the confidence of LLM outputs when only black-box (query-only) access is available. It is a simple technique that uses logistic regression to classify and validate the confidence of the outputs. Some problems of using black-box models are that there is no control over the model itself, and in some cases, the benefits and the value of buying these services that provide a black-box model outweigh training a personal, custom model (Pedapti).\nLogistic regression analysis also assists road engineers and traffic control around the world by identifying common predictors of traffic accidents in general, and specifically, predictors of fatal accidents. According to The World Health Organization, one of the most common unnatural causes of death across the world is road accidents, so it is imperative to identify strong predictors associated with these events (4) (Akter et al., 2021). A different study from 2024 aimed to find which factors in traffic are strongly associated with the occurrence of traffic accidents (5) (Chang et al., 2024). Researchers used binary logistic regression to model the probability of a traffic accident occurring given a set of 25 predictors related to road safety. Another road traffic safety study from Bangladesh investigated strong predictors of motorcycle accidents. These researchers also utilized a binary logistic regression model to find strong predictors of severe accidents (4) (Akter, 2021). “Modeling Road Accident Severity with Logistic Regression (comparison study)” also exemplifies the use of a binary logistic regression model to analyze traffic risk (6) (Chen et al., 2020). Researchers compared the results of the logistic regression model to results of decision tree and random forest models, and it was found that the logistic regression model was more clear and understandable than the others. All three of these studies concluded by stating that there are several significant variables found when predicting severe road crashes. Knowing these significant predictors helps builders and developers eliminate or reduce these risk factors as they are building new roads; hence, it is crucial to continue researching road accident severity with logistic regression (4) (Akter, 2021).\nEnvironmental issues can also be studied using logistic regression analysis due to its interesting properties; after all, logistic regression is a generalized linear model, which conducts mapping from any real number to probability values. “Priority prediction of Asian Hornet sighting report using machine learning methods” seeks to address the problem of Asian giant hornets (7) (Liu et al., 2021). They are an invasive species that pose a significant threat to native bee populations and local beekeeping, as well as to public safety due to their aggressive nature and potent venom. The goal of the research is to create an automated system to predict the priority of Asian giant hornet sighting reports. The authors modeled the priority prediction of sighting reports as a two-classification problem, with classes being either a “true positive” or a “false positive.” Their methodology is a straightforward application of logistic regression with feature extraction. Researchers then used a weighted binary cross-entropy function and the logistic regression is used for mapping the probability given the feature vector. The model achieved an average prediction accuracy of 83.5% on positive reports with the best weighting parameter settings, but still far from other works which achieved about 93% using Deep Learning. It was concluded that this still needs a lot of improvement or maybe it will never outmatch other methods due to hidden limitations (7) (Liu et al., 2021). One other example of logistic regression used in environmental contexts is in the article “Autoregressive Logistic Regression Applied to Atmospheric Circulation Patterns” (8) (Guanche et al., 2014). Researchers incorporate autoregressive time dependencies into logistic regression for climate modeling. They work with complex climatological dynamic data, and they explain both interpretation and simulation capabilities for weather patterns (8) (Guanche et al., 2014).\nThe article “Understanding Logistic Regression Analysis” discusses the usefulness of logistic regression, describes how to interpret results of the model, and gives an example of a logistic regression analysis using a synthetic dataset (9) (Sperandi, 2014). The data is about patients undergoing a drug treatment with a categorical outcome that is binary in nature, taking on values of survived (1), or did not survive (0). The result of the analysis explains how to interpret output from the model; one must take the exponentials of the slopes in the model to find the chances (the probability) of an event occurring. It is noted that in order to correctly understand results of a logistic regression, one must carefully consider the differences between the odds ratio, the log odds, and the probabilities of events occurring. Another important point in the article outlines the process of feature selection; a common way that predictors are selected for a logistic regression model is through a preliminary univariate analysis. After conducting a univariate analysis of each predictor in relation to the outcome variable, all significant predictors are included in the final multivariate logistic regression analysis (9) (Sperandi).\nA 2024 study titled, “Determinants of coexistence of undernutrition and anemia among under- five children in Rwanda,” presents evidence from 2019/2020 demographic health survey data (10) . There are two outcome variables in the dataset: anemia and undernutrition in children under five years of age in Rwanda (10) (Agmas et al., 2024). The study analyzes the relationship between the two outcome variables, as well as the relationship between 26 predictors relating to the childrens’ preexisting health conditions, family information, details about the parents, and relevant geographic information. One result of the study was that the relationship between the two outcome variables, presence of malnutrition and presence of anemia, was found to be significant. Six other significant predictors were identified: mother’s age, drinking water quality, other children in household, child gender, birth order, and gender of head of household. The conclusion states that improving maternal education, supplementing with vitamin A and other nutrient dense foods, providing a healthy home environment, and decreasing maternal anemia may help improve rates of malnutrition and anemia in children (10) (Agmas, 2024).\nIn the study, “Predictors of hospital admission when presenting with acute on chronic breathlessness: Binary logistic regression” emergency room data from one hospital is analyzed to determine common predictors of patients that are admitted to the hospital (11) (Hutchinson et al., 2023). Specifically, patients presenting to the emergency room with acute on chronic breathlessness were surveyed to collect data that would help researchers understand common factors among those admitted to the hospital. Knowing common predictors ahead of time helps hospital staff more easily identify patients who are more at risk for being admitted to the hospital, and also helps identify which patients would be more likely to be able to be discharged without being admitted. A binary logistic regression analysis of the data revealed that the odds of admission to the hospital were positively correlated with three predictors: age, talking to a doctor about symptoms, and the presence of preexisting heart conditions. However, the odds of being admitted to the hospital were negatively associated with blood oxygen levels (11) (Hutchinson, 2023).\nA new medical condition was recently recognized in 2004: airplane headache (AH), a condition described as a headache induced while taking off or landing in an airplane (12) (Prottengeier et al., 2025). Because AH is a relatively new addition to medical dictionaries, it is an underexplored condition that requires additional research. This study sought to identify common risk factors significantly associated with airplane headache to aid both travelers and airline employees. Two binary logistic regression models were constructed to compare two groups against the airplane headache group. The first regression model compared the airplane headache group to the no headache group, and 10 significant predictors of AH were identified; this model’s predictive power was found to be very high. The second model compared the airplane headache group to a group called other headache (individuals with symptoms of other types of headaches). The result from this analysis showed four significant predictors; however, the predictive power of the model was found to be very low. To conclude, it can be stated that binary logistic regression is a very effective way to find strong predictors of airplane headache when compared to those who do not have any headaches while flying (12) (Prottengeier).\nOne other way logistic regression is applied in the medical field is in identifying how the general public makes decisions regarding their health (13) (Liu J. et al., 2024). Researchers in China analyzed 2696 health survey responses collected from individuals across 31 Chinese provinces. They analyzed the data with a binary logistic regression model to classify points into two categories: unilateral decision making (value of 1), or collaborative decision making (value of 0). The researchers wanted to identify top predictors of individuals that make medical decisions by themselves and which predictors are correlated with patients making health decisions with more than one party (i.e. a patient, doctor, and family member all helping to make the health decision). It was found that most responses were classified as collaborative decision making (70%), which supports the idea that individuals in China strongly emphasize family- made decisions and strong family values. It was also concluded that significant predictors of unilateral decision making were gender, education level, family status, religious beliefs and occupation (13) (Liu J., 2024).\nLogistic Regression is also useful in detecting common diseases, such as breast cancer. “Regularized logistic regression with network-based pairwise interaction for biomarker identification in breast cancer” uses regularized logistic regression along with biological network information and pairwise interactions, to find biomarkers, both single and interacting pairs, for breast cancer (14) (Wu et al., 2016). Researchers prioritized biologically plausible biomarker combinations and used an adaptive elastic net, a penalty that balances l1 and l2, with network constraints. The result of the study shows that their model outperforms simpler models in terms of predictive performance, and they were able to discover both individual biomarkers and interacting gene pairs (14) (Wu, 2016). Another study on breast cancer from 2016 aimed to identify gene signatures that predict chemosensitivity, that is, which tumors react to chemotherapy in breast cancer by combining genetic algorithms with sparse logistic regression (15) (Hu, 2016). What makes this analysis relevant and important is that it predicts which patients will react to chemotherapy, which gives more personalized treatment. The results show that SLR-28 and Notch-86, two gene signatures, perform well on training and validation sets in terms of accuracy, specificity, sensitivity, and other metrics (15) (Hu, 2016).\nIn this paper, we will discuss the methodology, analysis, results, visualizations, and conclusions of a binary logistic regression statistical analysis regarding risk of stroke from common health indicators. Stroke is the second most common cause of death globally, so understanding the risk factors associated with it is imperative (16) (“The Top 10”, 2024).\n\nReferences\n\n\n1. Kutner MH. Applied linear statistical models. 2005; \n\n\n2. Tyurin A. From logistic regression to the perceptron algorithm: Exploring gradient descent with large step sizes. In: Proceedings of the AAAI conference on artificial intelligence. 2025. p. 20938–46. \n\n\n3. Pedapati T, Dhurandhar A, Ghosh S, Dan S, Sattigeri P. Large language model confidence estimation via black-box access. arXiv preprint arXiv:240604370. 2024; \n\n\n4. Rahman MH, Zafri NM, Akter T, Pervaz S. Identification of factors influencing severity of motorcycle crashes in dhaka, bangladesh using binary logistic regression model. International journal of injury control and safety promotion. 2021;28(2):141–52. \n\n\n5. Chen Y, You P, Chang Z. Binary logistic regression analysis of factors affecting urban road traffic safety. Advances in Transportation Studies. 2024;3. \n\n\n6. Chen MM, Chen MC. Modeling road accident severity with comparisons of logistic regression, decision tree and random forest. Information. 2020;11(5):270. \n\n\n7. Liu Y, Guo J, Dong J, Jiang L, Ouyang H. Priority prediction of asian hornet sighting report using machine learning methods. In: 2021 IEEE international conference on software engineering and artificial intelligence (SEAI). IEEE; 2021. p. 7–11. \n\n\n8. Guanche Y, Mı́nguez R, Méndez FJ. Autoregressive logistic regression applied to atmospheric circulation patterns. Climate dynamics. 2014;42(1):537–52. \n\n\n9. Sperandei S. Understanding logistic regression analysis. Biochemia medica. 2014;24(1):12–8. \n\n\n10. Asmare AA, Agmas YA. Determinants of coexistence of undernutrition and anemia among under-five children in rwanda; evidence from 2019/20 demographic health survey: Application of bivariate binary logistic regression model. Plos one. 2024;19(4):e0290111. \n\n\n11. Hutchinson A, Pickering A, Williams P, Johnson M. Predictors of hospital admission when presenting with acute-on-chronic breathlessness: Binary logistic regression. PLoS One. 2023;18(8):e0289263. \n\n\n12. Prottengeier J, Kaiser I, Moritz A, Konrad F. Risk factors for airplane headache: A multivariable logistic regression analysis in a population of career flight personnel. Cephalalgia. 2025;45(4):03331024251329837. \n\n\n13. Lyu Y, Xu Q, Liu J. Exploring the medical decision-making patterns and influencing factors among the general chinese public: A binary logistic regression analysis. BMC public health. 2024;24(1):887. \n\n\n14. Wu MY, Zhang XF, Dai DQ, Ou-Yang L, Zhu Y, Yan H. Regularized logistic regression with network-based pairwise interaction for biomarker identification in breast cancer. BMC bioinformatics. 2016;17(1):108. \n\n\n15. Hu W. Using genetic algorithms and sparse logistic regression to find gene signatures for chemosensitivity prediction in breast cancer. American Journal of Bioscience and Bioengineering. 2016;4(2):26–33. \n\n\n16. World Health Organization. The top 10 causes of death. https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death; 2025."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Present a great story for data science projects",
    "section": "",
    "text": "Important\n\n\n\nRemember: Your goal is to make your audience understand and care about your findings. By crafting a compelling story, you can effectively communicate the value of your data science project.\nCarefully read this template since it has instructions and tips to writing!\nMore information about revealjs: https://quarto.org/docs/reference/formats/presentations/revealjs.html"
  },
  {
    "objectID": "slides.html#introduction",
    "href": "slides.html#introduction",
    "title": "Present a great story for data science projects",
    "section": "Introduction",
    "text": "Introduction\n\nDevelop a storyline that captures attention and maintains interest.\nYour audience is your peers\nClearly state the problem or question you’re addressing.\nIntroduce why it is relevant needs.\nProvide an overview of your approach.\n\nIn kernel estimator, weight function is known as kernel function (1). Cite this paper (2). The GEE (3). The PCA (4)*"
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Present a great story for data science projects",
    "section": "Methods",
    "text": "Methods\n\nDetail the models or algorithms used.\nJustify your choices based on the problem and data."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization",
    "href": "slides.html#data-exploration-and-visualization",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\n\nDescribe your data sources and collection process.\nPresent initial findings and insights through visualizations.\nHighlight unexpected patterns or anomalies."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization-1",
    "href": "slides.html#data-exploration-and-visualization-1",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\nA study was conducted to determine how…"
  },
  {
    "objectID": "slides.html#modeling-and-results",
    "href": "slides.html#modeling-and-results",
    "title": "Present a great story for data science projects",
    "section": "Modeling and Results",
    "text": "Modeling and Results\n\nExplain your data preprocessing and cleaning steps.\nPresent your key findings in a clear and concise manner.\nUse visuals to support your claims.\nTell a story about what the data reveals."
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Present a great story for data science projects",
    "section": "Conclusion",
    "text": "Conclusion\n\nSummarize your key findings.\nDiscuss the implications of your results."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Present a great story for data science projects",
    "section": "References",
    "text": "References\n\n\n1. Efromovich S. Nonparametric curve estimation: Methods, theory, and applications [Internet]. Springer New York; 2008. (Springer series in statistics). Available from: https://books.google.com/books?id=mdoLBwAAQBAJ\n\n\n2. Bro R, Smilde AK. Principal component analysis. Analytical methods. 2014;6(9):2812–31. \n\n\n3. Wang M. Generalized estimating equations in longitudinal data analysis: A review and recent developments. Advances in Statistics. 2014;2014. \n\n\n4. Daffertshofer A, Lamoth CJ, Meijer OG, Beek PJ. PCA in studying coordination and variability: A tutorial. Clinical biomechanics. 2004;19(4):415–28."
  },
  {
    "objectID": "people/barbosa-renan/index.html#education",
    "href": "people/barbosa-renan/index.html#education",
    "title": "Renan Monteiro Barbosa",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | Univevrsity of West Florida"
  },
  {
    "objectID": "people/basnet-shree/index.html#education",
    "href": "people/basnet-shree/index.html#education",
    "title": "Shree Krishna Basnet",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | Univevrsity of West Florida"
  }
]