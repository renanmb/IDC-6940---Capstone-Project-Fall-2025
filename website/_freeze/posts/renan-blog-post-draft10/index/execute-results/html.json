{
  "hash": "b93f6290d5fd9f4f50b621ae3c77a30f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Draft Final Report - v10\"\ndescription: \"Return to the original project as done in Week 06\"\nauthor:\n  - name: Renan Monteiro Barbosa\n    url: https://github.com/renanmb\ncategories: [draft, renan]\nimage: images/spongebob-imagination.jpg\ndraft: false\nbibliography: references.bib\nlink-citations: true\n---\n\nThis draft attempts to return to original research goals as presented on the Week 06\n\n## 1. Introduction\n\nStroke is one of the leading causes of death and disability worldwide and remains a major public health challenge [@WHO2025]. Because stroke often occurs suddenly and can result in long-term neurological impairment, early identification of individuals at elevated risk is critical for prevention and timely intervention. Data-driven risk prediction models enable clinicians and public health professionals to quantify individual-level risk and to target high-risk groups for lifestyle counselling and clinical management.\n\nLogistic Regression (LR) is one of the most widely used approaches for modelling binary outcomes such as disease presence or absence [@sperandei2014understanding]. It extends linear regression to cases where the outcome is categorical and provides interpretable coefficients and odds ratios that describe how each predictor is associated with the probability of the event. LR has been applied across a wide range of domains, including child undernutrition and anaemia [@asmare2024determinants], road traffic safety [@rahman2021identification; @chen2024binary; @chen2020modeling], health-care utilisation and clinical admission decisions [@hutchinson2023predictors], and fraud detection [@samara2024using]. These applications highlight both the flexibility of LR and its suitability for real-world decision-making problems.\n\nIn this project, we analyse a publicly available stroke dataset that includes key demographic, behavioural, and clinical predictors such as **age**, **gender**, **hypertension status**, **heart disease**, **marital status**, **work type**, **residence type**, **smoking status**, **body mass index (BMI)**, and **average glucose level**. These variables are commonly reported in the stroke and cardiovascular literature as important determinants of risk. Using this dataset, we first clean and encode the variables into the appropriate data types in order to develop and fit a Logistic Regression model for predicting the outcome of **stroke**.\n\nThen we proceed to analyse one of the fundamental issues in the application of Logistic Regression or statistical models overal, the data imbalance issue. Data imbalance is a big problem for stroke ­prediction @kokkotis2022explainable. Because of many reasons ranging from privacy to the difficulty of doing cohort studies, the fact that pre-stroke datasets are rare, dataset often contain imbalanced classifications, with most instances being non-stroke c­ases @sirsat2020machine. So its unnecessary to say that this imbalance can result in biased models that favour the majority and ignore the minority, resulting in low forecast accuracy. To solve this issue and increase the effectiveness of the predictive models, we plan on exploring several oversampling and undersampling methods and much more are explored and employed, the popular of which is the ­SMOTE @wongvorachan2023comparison, @sowjanya2023effective. \n\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n\n## 2. Methods\n\nThe binary logistic regression model is part of a family of statistical models called generalised linear models. The main characteristic that differentiates binary logistic regression from other generalised linear models is the type of dependent (or outcome) variable. @harris2019statistics A dependent variable in a binary logistic regression has two levels. For example, a variable that records whether or not someone has ever been diagnosed with a health condition like Stroke could be measured in two categories, yes and no. Likewise, someone might have coronary heart disease or not, be physically active or not, be a current smoker or not, or have any one of thousands of diagnoses or personal behaviours and characteristics that are of interest in family medicine.\n\nThe binary logistic regression algorithm below:\n\n$$ln\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_{0} + \\beta_{1}x_{1} + \\cdots + \\beta_{k}x_{k}$$ \n\nWhere $\\pi = P[Y =1]$ is the probability of the outcome.\n\n\n### Assumptions\n\nBinary logistic regression relies on the following underlying assumptions to be true:\n\n* The observations must be independent.\n* There must be no perfect multicollinearity among independent variables.\n* Logistic regression assumes linearity of independent variables and log odds.\n* There are no extreme outliers\n* The Sample Size is Sufficiently Large. Field recommends a minimum of 50 cases. @field2024discovering Hosmer, Lemeshow, and Sturdivant @hosmer2013applied suggest a minimum sample of 10 observations per independent variable in the model. Leblanc and Fitzgerald (2000) @leblanc2000logistic suggest a minimum of 30 observations per independent variable.\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n\n## 3. Analysis and Results\n\nImport all the dependencies:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npackages <- c(\"dplyr\", \"car\", \"ResourceSelection\", \"caret\", \"pROC\",  \"logistf\", \"Hmisc\", \"rcompanion\", \"ggplot2\", \"summarytools\", \"tidyverse\", \"knitr\", \"ggpubr\", \"ggcorrplot\", \"randomForest\", \"gbm\", \"kernlab\", \"skimr\", \"corrplot\", \"scales\", \"tidyr\", \"RColorBrewer\", \"mice\", \"ROSE\", \"ranger\", \"stacks\", \"tidymodels\", \"themis\", \"gghighlight\")\n# Load Libraries\nlapply(packages, library, character.only = TRUE)\n# Set seed for reproducibility\nset.seed(123)\n\n# library(mice)\n# library(ROSE) # For SMOTE\n# library(ranger) # A fast implementation of random forests\n# library(stacks)\n# library(tidymodels)\n# library(themis)\n# library(gghighlight)\n```\n:::\n\n\n<!-- Maybe add a list describing the packages used with the citations attached -->\n\n### 3.1. Data Ingestion\n\nData source: [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset) @kaggle01\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfind_git_root <- function(start = getwd()) {\n  path <- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path <- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root <- find_git_root()\ndatasets_path <- file.path(repo_root, \"datasets\")\n\n# Reading the datafile healthcare-dataset-stroke-data\nstroke_path <- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\nstroke1 = read_csv(stroke_path, show_col_types = FALSE)\n```\n:::\n\n\n### 3.2. Exploratory Data Analysis (EDA)\n\n**Dataset Description**\n\nThe **Stroke Prediction Dataset** @kaggle01 is a publically available dataset for educational purposes containing 5,110 observations containing predictors commonly associated with cerebrovascular risk. The dataset is composed of 11 clinical and demographic features and 1 feature which is **id** a unique identifier for the patient. The dataset has features including patient's **age**, **gender**, presence of conditions like **hypertension** and **heart disease**, **work type**, **residence type**, **average glucose level**, and **BMI**. This dataset is primarily intended for educational purposes as it shares a lot of similarities with the Jackson Heart Study (JHS) dataset but it is not as descriptive.\n\n| Feature Name       | Description                                              | Data Type          | Key Values/Range                                           |\n|--------------------|----------------------------------------------------------|--------------------|------------------------------------------------------------|\n| id                 | Unique identifier for the patient                        | Numeric            | Unique numeric ID                                          |\n| gender             | Patient's gender                                         | Character          | Male, Female, Other                                        |\n| age                | Patient's age in years                                   | Numeric            | 0.08 to 82                                                 |\n| hypertension       | Indicates if the patient has hypertension                | Numeric (binary)   | 0 (No), 1 (Yes)                                            |\n| heart_disease      | Indicates if the patient has any heart diseases          | Numeric (binary)   | 0 (No), 1 (Yes)                                            |\n| ever_married       | Whether the patient has ever been married                | Character          | No, Yes                                                    |\n| work_type          | Type of occupation                                       | Character          | Private, Self-employed, Govt_job, children, Never_worked   |\n| Residence_type     | Patient's area of residence                              | Character          | Rural, Urban                                               |\n| avg_glucose_level  | Average glucose level in blood                           | Numeric            | ≈55.12 to 271.74                                           |\n| bmi                | Body Mass Index                                          | Character          | ≈10.3 to 97.6 (has NA values)                              |\n| smoking_status     | Patient's smoking status                                 | Character          | formerly smoked, never smoked, smokes, Unknown             |\n| stroke             | Target Variable: Whether the patient had a stroke        | Numeric (binary)   | 0 (No Stroke), 1 (Stroke)                                  |\n\n<!-- TODO add a footnote for table -->\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n\n#### **3.2.1 Dataset Preprocessing**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Handle dataset features\nstroke1[stroke1 == \"N/A\" | stroke1 == \"Unknown\" | stroke1 == \"children\" | stroke1 == \"other\"] <- NA\nstroke1$bmi <- round(as.numeric(stroke1$bmi), 2)\nstroke1$gender[stroke1$gender == \"Male\"] <- 1\nstroke1$gender[stroke1$gender == \"Female\"] <- 0\nstroke1$gender <- as.numeric(stroke1$gender)\nstroke1$ever_married[stroke1$ever_married == \"Yes\"] <- 1\nstroke1$ever_married[stroke1$ever_married == \"No\"] <- 0\nstroke1$ever_married <- as.numeric(stroke1$ever_married)\nstroke1$work_type[stroke1$work_type == \"Govt_job\"] <- 1\nstroke1$work_type[stroke1$work_type == \"Private\"] <- 2\nstroke1$work_type[stroke1$work_type == \"Self-employed\"] <- 3\nstroke1$work_type[stroke1$work_type == \"Never_worked\"] <- 4\nstroke1$work_type <- as.numeric(stroke1$work_type)\nstroke1$Residence_type[stroke1$Residence_type == \"Urban\"] <- 1\nstroke1$Residence_type[stroke1$Residence_type == \"Rural\"] <- 2\nstroke1$Residence_type <- as.numeric(stroke1$Residence_type)\nstroke1$avg_glucose_level <- as.numeric(stroke1$avg_glucose_level)\nstroke1$heart_disease <- as.numeric(stroke1$heart_disease)\nstroke1$hypertension <- as.numeric(stroke1$hypertension)\nstroke1$age <- round(as.numeric(stroke1$age), 2)\nstroke1$stroke <- as.numeric(stroke1$stroke)\nstroke1$smoking_status[stroke1$smoking_status == \"never smoked\"] <- 1\nstroke1$smoking_status[stroke1$smoking_status == \"formerly smoked\"] <- 2\nstroke1$smoking_status[stroke1$smoking_status == \"smokes\"] <- 3\nstroke1$smoking_status <- as.numeric(stroke1$smoking_status)\nstroke1 <- stroke1[, !(names(stroke1) %in% \"id\")]\n\n# Remove NAs and clean dataset\nstroke1$stroke <- as.factor(stroke1$stroke)\nstroke1_clean <- na.omit(stroke1)\nstrokeclean <- stroke1_clean\nfourassume <- stroke1_clean\n\nstrokeclean$stroke <- factor(\n  strokeclean$stroke,\n  levels = c(\"0\", \"1\"),\n  labels = c(\"No\", \"Yes\")\n)\n\nfourassume$stroke <- factor(\n  fourassume$stroke,\n  levels = c(\"0\", \"1\"),\n  labels = c(\"No\", \"Yes\")\n)\n```\n:::\n\n\nThe initial exploration demonstrated that the **Stroke Prediction Dataset** @kaggle01 has several issues requiring changes for handling missing values, converting character (categorical) features into numerical codes, and removing the identifier column.\n\n<!-- TODO -->\nSo as part of data preprocessing we will be focused on establishing consistency and ensuring all variables are in a format suitable for predictive modeling. This process starts by systematically addressing non-standard representations of missing data. Specifically, all instances of the string values \"N/A\", \"Unknown\", \"children\", and \"other\" found across the dataset were unified and replaced with the standard statistical missing value representation, NA.\n\nThen we proceed with converting several character-based (categorical) features into numerical features, which is necessary for predictive modeling.\n<!-- TODO -->\n\nThe feature **bmi**, initially read as a character variable was first converted to a numeric data type and subsequently rounded to two decimal places.\n\nThe binary categorical features were encoded into numerical indicators. The feature **gender** was transformed so that **\"Male\"** was encoded to 1 and **\"Female\"** was encoded to 0, and the **ever_married** was transformed so that **\"Yes\"** encoded to 1 and **\"No\"** encoded to 0.\n\nFeatures with multiple categories were also numerically encoded into numerical indicators. The **work_type** feature had its categories encoded so that **\"Govt_job\"** = 1, **\"Private\"** = 2, **\"Self-employed\"** = 3, and **\"Never_worked\"** = 4. The **Residence_type** was encoded so that **\"Urban\"** = 1 and **\"Rural\"** = 2. Finally, the **smoking_status** feature was encoded into three numerical levels, those being **\"never smoked\"** = 1, **\"formerly smoked\"** = 2, and **\"smokes\"** = 3.\n\nAdditionally, the continuous numerical variables **avg_glucose_level**, **heart_disease**, and **hypertension** were explicitly confirmed as numeric data types, with the **age** feature also being rounded to two decimal places for consistency.\n\n<!-- TODO -->\nThe final stage of preprocessing involved removing the **id** column, which served only as a unique identifier and held no predictive value. This action left the dataset with 11 core predictors. The target variable, **stroke**, was then converted into a factor (a categorical data type in R) named **stroke1**, and its levels were explicitly labeled as $\\text{\"No\"} = 0$ and $\\text{\"Yes\"} = 1$. The entire process concluded with the removal of all remaining observations containing missing or inconsistent entries, resulting in the creation of the final, clean data frames, **strokeclean** and **fourassume**.\n\n\n**Dataset Preprocessing Conclusion**\n\nThe **Stroke Prediction Dataset** @kaggle01 that started containing 5,110 observations and 12 features. After cleaning missing and inconsistent entries among other necessarychanges, ended as a dataset containing 3,357 observations and 11 predictors commonly associated with cerebrovascular risk. Those key predictors are listed below.\n\n<!-- TODO modify this table to add a better description and be more visually engaging -->\n\n| Feature Name      | Description                                      | Data Type | Values                                                               |\n|-------------------|--------------------------------------------------|-----------|----------------------------------------------------------------------|\n| gender            | Patient's gender                                 | Numeric   | 1 (Male), 0 (Female)                                                 |\n| age               | Patient's age in years                           | Numeric   | Range 0.08 to 82; rounded to 2 decimal places                        |\n| hypertension      | Indicates if the patient has hypertension        | Numeric   | 0 (No), 1 (Yes)                                                      |\n| heart_disease     | Indicates if the patient has any heart diseases  | Numeric   | 0 (No), 1 (Yes)                                                      |\n| ever_married      | Whether the patient has ever been married        | Numeric   | 1 (Yes), 0 (No)                                                      |\n| work_type         | Type of occupation                               | Numeric   | 1 (Govt_job), 2 (Private), 3 (Self-employed), 4 (Never_worked)       |\n| Residence_type    | Patient's area of residence                      | Numeric   | 1 (Urban), 2 (Rural)                                                 |\n| avg_glucose_level | Average glucose level in blood                   | Numeric   | Range ≈55.12 to 271.74                                               |\n| bmi               | Body Mass Index                                  | Numeric   | Range ≈10.3 to 97.6; converted from character, rounded to 2 decimals |\n| smoking_status    | Patient's smoking status                         | Numeric   | 1 (never smoked), 2 (formerly smoked), 3 (smokes)                    |\n| stroke            | Target Variable: Whether the patient had stroke  | Numeric   | 0 (No Stroke), 1 (Stroke)                                            |\n\n<!-- TODO add table footnote -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# skim(stroke1)\n# nrow(fourassume)\n# class(strokeclean$stroke)\n# unique(strokeclean$gender)\n```\n:::\n\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n#### **3.2.2 Dataset Visualization**\n\nBefore developing predictive models, an exploratory analysis was conducted to understand the distribution, structure, and relationships within the cleaned dataset (N = 3,357). This step is crucial in rare-event medical modeling because data imbalance, skewed predictors, or correlated variables can directly influence model behavior and classification performance.\n\n**Histograms**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# 1. Get the total number of rows in your data frame\nTOTAL_ROWS <- nrow(strokeclean)\n\n# 2. Use the modified ggplot code\np1a <- ggplot(strokeclean, aes(x = gender, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    # The calculation is (bar_count / TOTAL_ROWS) * 100, rounded to 1 decimal place.\n    position = position_dodge(width = 0.9),\n    aes(\n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    vjust = -0.5,\n    size = 3\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  scale_x_continuous(\n    breaks = c(0, 1), \n    labels = c(\"Female\", \"Male\")\n  ) +\n  labs(title = \"(a) Gender\", x = \"Gender\", y = \"Count\")\n\n# (b) Histogram of Age\np1b <- ggplot(strokeclean, aes(x = age, fill = stroke)) +\n  geom_histogram(binwidth = 1, position = \"identity\", alpha = 0.7) +\n  # stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(b) Age\", x = \"Age\", y = \"Frequency\")\n\n# (b) Bivariate Density Plot of Age\n# p1b <- ggplot(strokeclean, aes(x = age, fill = stroke)) + # Keep fill=stroke\n#   geom_density(alpha = 0.5) + # Overlap the two density curves\n#   labs(title = \"(b) Age\", x = \"Age\", y = \"Density\")\n\n# (c) Histogram of hypertension\np1c <- ggplot(strokeclean, aes(x = hypertension, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    position = position_dodge(width = 0.9),\n    aes(\n      group = stroke,\n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    vjust = -0.5,\n    size = 3\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  # Map 0/1 to Yes/No\n  scale_x_continuous(\n    breaks = c(0, 1),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(title = \"(c) Hypertension\", x = \"Hypertension\", y = \"Frequency\")\n\n# (d) Histogram of heart_disease\np1d <- ggplot(strokeclean, aes(x = heart_disease, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    position = position_dodge(width = 0.9),\n    aes(\n      group = stroke,\n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    vjust = -0.5,\n    size = 3\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  # Map 0/1 to Yes/No\n  scale_x_continuous(\n    breaks = c(0, 1),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(title = \"(d) Heart Disease\", x = \"Heart Disease\", y = \"Frequency\")\n\n# (e) Histogram of ever_married\np1e <- ggplot(strokeclean, aes(x = ever_married, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    position = position_dodge(width = 0.9),\n    aes(\n      group = stroke,\n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    vjust = -0.5,\n    size = 3\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  scale_x_continuous(\n    breaks = c(0, 1),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  # Assuming 'No'/'Yes' are string/factor values, use scale_x_discrete if needed\n  labs(title = \"(e) Ever Married\", x = \"Ever Married\", y = \"Frequency\")\n\n# (f) Histogram of work_type\np1f <- ggplot(strokeclean, aes(y = work_type, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    position = position_dodge(width = 0.9), \n    aes(\n      group = stroke,\n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    hjust = -0.1, # Shift text right for horizontal bar\n    size = 3,\n    color = \"black\"\n  ) +\n  # Expand X-axis (Frequency) for horizontal bar\n  scale_x_continuous(expand = expansion(mult = c(0, 0.5))) +\n  # Adding Work type labels make it too convoluted\n  # scale_y_continuous(\n  #   breaks = c(1, 2, 3, 4), \n  #   labels = c(\"Govt_job\", \"Private\", \"Self-employed\", \"Never_worked\")\n  # ) + \n  labs(title = \"(f) Work Type\", y = \"Work Type\", x = \"Frequency\")\n\n# (g) Histogram of Residence_type\np1g <- ggplot(strokeclean, aes(x = Residence_type, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    # Crucial for aligning text labels with the dodged bars\n    position = position_dodge(width = 0.9), \n    aes(\n      # Defines the group for position_dodge to work correctly on text\n      group = stroke, \n      \n      # Combined label: Percentage (top line) + Count (bottom line)\n      label = paste0(\n        # Percentage calculation: (count / TOTAL_ROWS) * 100\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    vjust = -0.5, # Moves the two-line label slightly above the bar\n    size = 3,\n    color = \"black\" # Ensures better visibility\n  ) +\n  # Adds 15% extra space to the top of the y-axis to prevent label clipping\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) + \n  scale_x_continuous(\n    breaks = c(1, 2),\n    labels = c(\"Urban\", \"Rural\")\n  ) +\n  labs(title = \"(g) Residence Type\", x = \"Residence Type\", y = \"Frequency (Count)\")\n\n# (h) Histogram of avg_gloucose_level\np1h <- ggplot(strokeclean, aes(x = avg_glucose_level, fill = stroke)) +\n  geom_histogram(binwidth = 5, position = \"identity\", alpha = 0.7) +\n  # stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(h) Avg. Glucose Level\", x = \"Glucose Level\", y = \"Frequency\")\n\n# (h) Bivariate Density plot of avg_gloucose_level\n# p1h <- ggplot(strokeclean, aes(x = avg_glucose_level, fill = stroke)) +\n#   geom_density(alpha = 0.5) +\n#   labs(title = \"Avg. Glucose Level by Stroke Status\", x = \"Average Glucose Level\", y = \"Density\")\n\n# (i) Histogram of bmi\np1i <- ggplot(strokeclean, aes(x = bmi, fill = stroke)) +\n  geom_histogram(binwidth = 2, position = \"identity\", alpha = 0.7) +\n  labs(title = \"(i) BMI\", x = \"BMI\", y = \"Frequency\")\n\n# (i) Bivariate Density plot of bmi\n# p1i <- ggplot(strokeclean, aes(x = bmi, fill = stroke)) +\n#   geom_density(alpha = 0.5) +\n#   labs(title = \"BMI Distribution by Stroke Status\", x = \"BMI\", y = \"Density\")\n\n# (j) smoking_status\np1j <- ggplot(strokeclean, aes(y = smoking_status, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    position = position_dodge(width = 0.9), \n    aes(\n      group = stroke, \n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    hjust = -0.1, \n    size = 3,\n    color = \"black\" \n  ) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.5))) + \n  labs(title = \"(j) Smoking Status\", y = \"Smoking Status\", x = \"Frequency (Count)\")\n```\n:::\n\n\nWe can observe from the histograms (a), (b), (c) and (d) the following: \n\nThe data appears to be slightly imbalanced towards **female** gender and the proportion of stroke cases relative to the total number of individuals in each gender appears similar for both genders, even if it looks slightly higher in the male doesnt seem to be significant difference. \n\nThe number of stroke cases increases dramatically after the **age** of $\\approx 50$ and peaks in the 60 to 80 age range. This strongly suggests **age is a critical risk factor for stroke**.\n\nThe majority of patients do not have **hypertension** and the proportion of stroke cases (blue bar) is visibly much higher in the group with **hypertension**. This indicates that **hypertension is a strong risk factor for stroke**.\n\nSimilar to hypertension, the majority of patients do not have **heart disease** and the proportion of stroke cases (blue bar) is visibly much higher in the group with **heart disease**. This indicates that **heart disease is a very strong risk factor** for stroke, even stronger than hypertension when based alone on the observed proportions.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# p1a, p1b, p1c, p1d\n# (a) Histogram of gender \n# (b) Histogram of Age\n# (c) Histogram of hypertension\n# (d) Histogram of heart_disease\nggarrange(p1a, p1b, p1c, p1d, \n          ncol = 2, nrow = 2, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Histogram of (a)gender, (b)age, (c)hypertension, (d)heart_disease.](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWe can observe from the histograms (e), (f), (g) and (h) the following: \n\nThe stroke rate appears higher for those who have ever been married which is a fascinating plot that catches our attention, this must be correlated with another variable. Our guess is that having been married being associated with a higher stroke risk in this dataset, is possibly due to the married group skewing toward older ages\n\nAcross the four work types encoded, \"Govt_job\" = 1, \"Private\" = 2  \"Self-employed\" = 3, \"Never Worked\" = 4. **Self-employed** individuals appear to have the highest risk proportion among the working groups. Followed by the **Private** which is the largest group (total $\\approx 2200$) and naturally accounts for the highest raw count of stroke cases (109) with a proportion of stoke incidence sligthly higher than **Govt_job**.\n\nThe stroke outcomes based on the patient's **residence type** has a very similar raw count their proportions seems to be similar as well. This suggests that **residence type** does not appear to be a significant factor for stroke risk.\n\nFrom the distribution of **average glucose** (HbA1c) we can visually spot that the stroke cases are more frequent for high-glucose relative to the total population at those high levels. This higher propportion indicates that **high average glucose (HbA1c) level is a significant risk factor** for stroke.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# p1e p1f p1g p1h\n# (e) Histogram of ever_married\n# (f) Histogram of work_type\n# (g) Histogram of Residence_type\n# (h) Histogram of avg_gloucose_level\nggarrange(p1e, p1f, p1g, p1h,\n          ncol = 2, nrow = 2, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Histogram of (e)ever_married, (f)work_type, (g)Residence_type, (h)avg_gloucose_level.](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe can observe from the histograms (i) and (j) the following: \n\nFor the BMI distribution we can observe that the majority of the patient population (pink bars) falls within the overweight to obese range (BMI $\\approx 25$ to $35$). So as a consequence we can expect that the frequency of stroke cases (blue bars) will follow the distribution of the overall population, meaning most strokes occur where the largest number of people are located which are the BMI values between $25$ and $35$.\n\nHowever, we can visually spot that the stroke occurence is drops significantly closer to a healthy BMI of 20. So although the risk of stroke does seem to be generally higher than average once BMI exceeds the ideal range and moves into the overweight and obese categories because there is a larger distribution within the overweight to obese range, we can conclude that because the skewed distributin that **BMI is a significant risk factor predictor for stroke**.\n\nThe stroke outcomes are compared across the three smoking status categories encoded: **smokes = 3**, **formerly smoked = 2**, and **never smoked = 1**.\n\nThis plot is highlights a particularly interesting aspect of this dataset. The highest proportional risk of stroke appears to be in the **formerly smoked** group. This finding is common in medical literature @oshunbade2020cigarette, as individuals who have a history of smoking may have accrued vascular damage that persists, but their stroke risk is still lower than the risk for current smokers if they continue to smoke. \n\nThis information is importante, because the **formerly smoked** group shows the highest rate, suggesting that **a history of smoking is a significant indicator of risk.**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# p1i p1j\n# (i) Histogram of bmi\n# (j) smoking_status\nggarrange(p1i, p1j,\n          ncol = 2, nrow = 1, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Histogram of (i)bmi, (j)smoking_status.](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n\n#### **3.2.3 Correlation Analysis**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf_numeric <- model.matrix(~.-1, data = strokeclean) |>\n  as.data.frame()\n\n# Rename columns for clarity (model.matrix adds prefixes)\ncolnames(df_numeric) <- gsub(\"gender|work_type|smoking_status|Residence_type|ever_married\", \"\", colnames(df_numeric))\n\n# 1. Calculate the correlation matrix\ncorrelation_matrix <- cor(df_numeric)\n\n# 2. Define a green sequential color palette\n# green_palette <- colorRampPalette(c(\"#E5F5E0\", \"#31A354\"))(200) # Light to dark green\ngreen_palette <- colorRampPalette(c(\"#d5ffc8ff\", \"#245332ff\"))(200) \n\n# corrplot(correlation_matrix, method = 'number') # colorful number\n# 3. Create the heatmap with the correct palette\np2 <- corrplot(correlation_matrix, \n         method = \"color\",\n         type = \"full\", # change to full or upper\n         order = \"hclust\",\n         tl.col = \"black\",\n         tl.srt = 45,\n         addCoef.col = \"black\",\n         number.cex = 0.7,\n         col = green_palette, # Use the new palette here\n         diag = FALSE)\n```\n\n::: {.cell-output-display}\n![Correlation Analysis.](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThe correlation analysis confirms that the strongest linear predictors for stroke outcome in this dataset are age, hypertension, and average glucose level. Furthermore, age is highly correlated with hypertension, suggesting these factors may have overlapping or compounding effects on stroke risk.\n\nThis information will be further explored during statistical modelling were we will evaluate the statistical significance of those variables.\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n<!-- Must fix the statistical modelling -->\n\n### 3.3. Statistical Modelling\n\nInitially, we split the dataset into a training set (70%) and a test set (30%) to evaluate out-of-sample performance, then we used this training data for our statistical modelling. It is important to note that during splitting, stratified sampling was used (via caret::createDataPartition) to maintain the stroke/no-stroke ratio. @chen2020modeling\n\nAlso the categorical variables were converted into the appropiate Data Types for correctly fitting the GLM binomial regression model.\n\n<!-- TODO ----- Data splitting for logistic regression involves partitioning your dataset into training and testing sets to build and validate the model's probability of success predictions -->\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_df <- strokeclean\nmodel_df <- na.omit(model_df)\nmodel_df$stroke <- factor(model_df$stroke)\nlevels(model_df$stroke) <- c(\"No\", \"Yes\")\ntable(model_df$stroke)\n\nindex <- createDataPartition(strokeclean$stroke, p = 0.70, list = FALSE)\ntrain_data <- strokeclean[index, ]\ntest_data  <- strokeclean[-index, ]\n\ntrain_data$stroke <- factor(train_data$stroke, levels = c(\"No\",\"Yes\"))\ntest_data$stroke  <- factor(test_data$stroke,  levels = c(\"No\",\"Yes\"))\n\n# ---------------------------------------------\n# Convert all multi-level categoricals to factors with a clear reference level\ntrain_data$work_type     <- factor(train_data$work_type)\ntrain_data$Residence_type<- factor(train_data$Residence_type)\ntrain_data$smoking_status<- factor(train_data$smoking_status)\n\n# The same should be done for test_data and the binary variables \ntest_data$work_type     <- factor(test_data$work_type)\ntest_data$Residence_type<- factor(test_data$Residence_type)\ntest_data$smoking_status<- factor(test_data$smoking_status)\n# ---------------------------------------------\n# Note: if you want the output to label the levels (e.g., \"Male\" vs \"Female\") instead of \"gender\" and \"gender1\" (for Male = 1 vs Female = 0).\n# For 0/1, R's glm is usually fine, but for clean output factors are better.\n# For multi-level, it's essential.\n```\n:::\n\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n<!-- Must changed the K fold cross validation -->\n\n#### **3.3.1. Repeated K-fold cross-validation**\n\nThe trainControl() function in the R caret package is used to control the computational nuances and resampling methods employed by the train() function. It allows us to implement Repeated K-fold cross-validation (\"repeatedcv\").\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nctrl <- trainControl(\nmethod = \"repeatedcv\",\nnumber = 5,\nrepeats = 3,\nclassProbs = TRUE,\nsummaryFunction = twoClassSummary,\nverboseIter = FALSE\n)\n```\n:::\n\n\n#### **3.3.2. Logistic Regression**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Using the GLM package without K fold cross validation\nmodel_lr <- glm(\n  stroke ~ . , \n  data=train_data , \n  family = \"binomial\" (link=logit)\n  )\n\n# Checking if the wrapper as.factor has any difference\n# model_lr <- glm(\n#   stroke ~ age +\n#   avg_glucose_level +\n#   bmi +\n#   as.factor(gender) +\n#   as.factor(hypertension) +\n#   as.factor(heart_disease) +\n#   as.factor(ever_married) +\n#   as.factor(work_type) +\n#   as.factor(Residence_type) +\n#   as.factor(smoking_status)\n#   , \n#   data=train_data , \n#   family = \"binomial\" (link=logit)\n#   )\n\ns1 <- summary(model_lr)\nc1 <- coefficients(model_lr)\nanova1 <- car::Anova(model_lr, type = 3)\nconfint1 <- confint(model_lr, level=0.95)\n\n# Logistic Regression with the Caret package\nmodel_lr2 <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"glm\",\nfamily = \"binomial\",\nmetric = \"ROC\",\ntrControl = ctrl\n)\n```\n:::\n\n\n**Logistic Regression Preliminary conclusions**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = stroke ~ ., family = binomial(link = logit), data = train_data)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        -8.113391   0.854545  -9.494  < 2e-16 ***\ngender             -0.112742   0.204658  -0.551  0.58172    \nage                 0.078380   0.008614   9.099  < 2e-16 ***\nhypertension        0.914733   0.214191   4.271 1.95e-05 ***\nheart_disease       0.339604   0.277662   1.223  0.22130    \never_married       -0.532738   0.293381  -1.816  0.06939 .  \nwork_type2          0.072261   0.288269   0.251  0.80207    \nwork_type3         -0.290608   0.324634  -0.895  0.37069    \nwork_type4         -9.306169 649.652359  -0.014  0.98857    \nResidence_type2    -0.072792   0.198250  -0.367  0.71349    \navg_glucose_level   0.005488   0.001670   3.287  0.00101 ** \nbmi                 0.002103   0.015554   0.135  0.89245    \nsmoking_status2     0.208775   0.226763   0.921  0.35722    \nsmoking_status3     0.345133   0.266423   1.295  0.19517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 982.44  on 2349  degrees of freedom\nResidual deviance: 767.21  on 2336  degrees of freedom\nAIC: 795.21\n\nNumber of Fisher Scoring iterations: 14\n```\n\n\n:::\n\n```{.r .cell-code}\nanova1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: stroke\n                  LR Chisq Df Pr(>Chisq)    \ngender               0.305  1   0.580683    \nage                107.200  1  < 2.2e-16 ***\nhypertension        17.103  1   3.54e-05 ***\nheart_disease        1.439  1   0.230228    \never_married         3.064  1   0.080044 .  \nwork_type            2.479  3   0.479126    \nResidence_type       0.135  1   0.713341    \navg_glucose_level   10.535  1   0.001171 ** \nbmi                  0.018  1   0.892611    \nsmoking_status       1.905  2   0.385861    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n<!-- ANOVA conclusion -->\n\n| Factor            | LR χ²   | Df | p-value    | Signif. | Interpretation (α=0.05)                                                                                 |\n|-------------------|---------|----|------------|---------|---------------------------------------------------------------------------------------------------------|\n| age               | 107.200 | 1  | <2.2e-16   | ***     | Reject H0. Age is statistically significant in predicting stroke.                                       |\n| hypertension      | 17.103  | 1  | 3.54e-05   | ***     | Reject H0. Hypertension status is statistically significant in predicting stroke.                       |\n| avg_glucose_level | 10.535  | 1  | 0.001171   | **      | Reject H0. The average glucose level is statistically significant in predicting stroke.                 |\n| gender            | 0.305   | 1  | 0.580683   |         | Fail to Reject H0. Gender is not statistically significant in predicting stroke.                        |\n| heart_disease     | 1.439   | 1  | 0.230228   |         | Fail to Reject H0. Heart Disease is not statistically significant in predicting stroke.                 |\n| ever_married      | 3.064   | 1  | 0.080044   | .       | Fail to Reject H0. Marital Status is not statistically significant. Note: Significant at α = 0.1 level. |\n| work_type         | 2.479   | 3  | 0.479126   |         | Fail to Reject H0. The factor is not statistically significant in predicting stroke.                    |\n| Residence_type    | 0.135   | 1  | 0.713341   |         | Fail to Reject H0. The residence type is not statistically significant in predicting stroke.            |\n| bmi               | 0.018   | 1  | 0.892611   |         | Fail to Reject H0. The BMI is not statistically significant in predicting stroke.                       |\n| smoking_status    | 1.905   | 2  | 0.385861   |         | Fail to Reject H0. The smoking status is not statistically significant in predicting stroke.            |\n\n\nFrom the ANOVA test we could observe that the variables **age**, **hypertension**, and **avg_glucose_level** are statistically significant in predicting the odds of having a stroke. As well we could observe that the variables **gender**, **heart_disease**, **work_type**, **Residence_type**, **bmi**, and **smoking_status** do not show a statistically significant effect on the odds of stroke at the $\\alpha=0.05$ level. Additionally, there is an interesting observation that the variable **ever_married** is close to significance indicing some curiosity and further exploration.\n\nTherefore, based solely on this ANOVA table the performance evaluation suggests that we consider removing all the statistically not significant variables and keeping the statistically significant varibles: $\\text{age}$, $\\text{hypertension}$, $\\text{avg\\_glucose\\_level}$.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Using the GLM package without K fold cross validation\nmodel2_lr <- glm(\n  stroke ~ age +\n  hypertension +\n  avg_glucose_level , \n  data=train_data , \n  family = \"binomial\" (link=logit)\n  )\n\ns2 <- summary(model2_lr)\nc2 <- coefficients(model2_lr)\nanova2 <- car::Anova(model2_lr, type = 3)\nconfint2 <- confint(model2_lr, level=0.95)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = stroke ~ age + hypertension + avg_glucose_level, \n    family = binomial(link = logit), data = train_data)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -8.232810   0.560826 -14.680  < 2e-16 ***\nage                0.075044   0.007904   9.495  < 2e-16 ***\nhypertension       0.929501   0.210279   4.420 9.85e-06 ***\navg_glucose_level  0.005427   0.001582   3.430 0.000603 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 982.44  on 2349  degrees of freedom\nResidual deviance: 777.43  on 2346  degrees of freedom\nAIC: 785.43\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n\n```{.r .cell-code}\nanova2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: stroke\n                  LR Chisq Df Pr(>Chisq)    \nage                120.407  1  < 2.2e-16 ***\nhypertension        18.205  1  1.983e-05 ***\navg_glucose_level   11.337  1  0.0007598 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n<!-- Add more ANOVA and continued model interpretation here -->\n\n<!-- TODO -->\n\n| Factor            | LR χ²    | Df | p-value     | Signif. | Interpretation (α=0.05)                                                                                 |\n|-------------------|----------|----|-------------|---------|-----------------------------------------------------------------------------------------------------------|\n| age               | 120.407  | 1  | <2.2e-16    | ***     | Reject H0. The factor is statistically significant in predicting stroke.                                 |\n| hypertension      | 18.205   | 1  | 1.983e-05   | ***     | Reject H0. The factor is statistically significant in predicting stroke.                                 |\n| avg_glucose_level | 11.337   | 1  | 0.0007598   | ***     | Reject H0. The factor is statistically significant in predicting stroke.                                 |\n\nWe could see a marginal improvement on the model with and AIC of 785.43 versus the previous AIC of 795.21 and its now even easier to observe that **age** seems to be the most powerful predictor in the model.\n\n\n<!-- Add here the idea of introducing the TWO way testing -->\n\n<!-- Two way interactions\n\nYou should definitely consider two-way interactions, especially between your most significant predictors: $\\text{age}$, $\\text{hypertension}$, and $\\text{avg\\_glucose\\_level}$.\n\nBased on your ANOVA results, prioritize interactions among the strongest drivers:\n\n$\\mathbf{\\text{age} \\times \\text{hypertension}}$\n\n$\\mathbf{\\text{age} \\times \\text{avg\\_glucose\\_level}}$\n\n$\\mathbf{\\text{hypertension} \\times \\text{avg\\_glucose\\_level}}$\n\nYou might also test $\\text{age} \\times \\text{heart\\_disease}$, as $\\text{heart\\_disease}$ is clinically important even if its main effect was marginally insignificant in the multivariate model. -->\n\n#### 3.3.3. Addressing Class Imbalance with SMOTE\n\nThe dataset is highly imbalanced, with only a small number of cases being stroke instances. This can bias machine learning models. We will use SMOTE to create balanced versions of our imputed datasets by generating synthetic minority (stroke) class samples.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Ensure the stroke column is a factor for SMOTE\n# df_mice$stroke <- as.factor(df_mice$stroke)\n# df_mean$stroke <- as.factor(df_mean$stroke)\n# df_age_group$stroke <- as.factor(df_age_group$stroke)\n\n# Create balanced datasets using SMOTE\n# Using the MICE imputed dataset as the primary example for balancing\n\n# Get the number of non-stroke (majority) cases\n# n_majority <- sum(df_mice$stroke == \"0\")\nn_majority <- sum(train_data$stroke == \"No\")\n\n# Calculate the desired total size for a balanced dataset\ndesired_N <- 2 * n_majority\n\n# Create the balanced dataset\ndata_balanced_mice <- ROSE::ovun.sample(\n  stroke ~ ., \n  data = train_data, \n  method = \"over\", \n  N = desired_N, \n  seed = 123\n)$data\n```\n:::\n\n\nWe can observe the class distribution before handling the class imbalance with a small number of cases being stroke instances.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Check the new class distribution\n# cat(\"Original Class Distribution (MICE imputed):\\n\")\nprint(table(train_data$stroke))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  No  Yes \n2224  126 \n```\n\n\n:::\n:::\n\n\nAfter the class distribution balancing the number of cases being stroke instances is much higher.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Check the new class distribution\n# cat(\"\\nBalanced Class Distribution (SMOTE):\\n\")\nprint(table(data_balanced_mice$stroke))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  No  Yes \n2224 2224 \n```\n\n\n:::\n:::\n\n\n#### 3.3.4. Fitting Logistic Regression with Balanced Data\n\nWe will use the balanced dataset for modeling. Same as before the dataset is split into training (70%) and testing (30%).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# data_bal <- ROSE(stroke ~ ., data = train_data, seed = 123)$data\nmodel3_lr <- glm(\n  stroke ~ age +\n  hypertension +\n  avg_glucose_level , \n  data=data_balanced_mice , \n  family = \"binomial\" (link=logit)\n  )\n\ns3 <- summary(model3_lr)\nc3 <- coefficients(model3_lr)\nanova3 <- car::Anova(model3_lr, type = 3)\nconfint3 <- confint(model3_lr, level=0.95)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = stroke ~ age + hypertension + avg_glucose_level, \n    family = binomial(link = logit), data = data_balanced_mice)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -5.6444885  0.1854899 -30.430   <2e-16 ***\nage                0.0782316  0.0027115  28.852   <2e-16 ***\nhypertension       1.1193216  0.0932833  11.999   <2e-16 ***\navg_glucose_level  0.0055918  0.0006791   8.234   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6166.2  on 4447  degrees of freedom\nResidual deviance: 4254.9  on 4444  degrees of freedom\nAIC: 4262.9\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n\n```{.r .cell-code}\nanova3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: stroke\n                  LR Chisq Df Pr(>Chisq)    \nage                1201.85  1  < 2.2e-16 ***\nhypertension        154.34  1  < 2.2e-16 ***\navg_glucose_level    69.73  1  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n| Factor            | LR χ²    | Df | p-value   | Signif. | Interpretation (α=0.05)                          |\n|-------------------|----------|----|-----------|---------|---------------------------------------------------|\n| age               | 1201.85  | 1  | <2.2e-16  | ***     | Reject H0. The factor is highly statistically significant. |\n| hypertension      | 154.34   | 1  | <2.2e-16  | ***     | Reject H0. The factor is highly statistically significant. |\n| avg_glucose_level | 69.73    | 1  | <2.2e-16  | ***     | Reject H0. The factor is highly statistically significant. |\n\nAll the three variables we kept (age, hypertension, and avg_glucose_level) remained statistically significant. what confirms that the relationships between these key clinical factors and stroke outcome are robust.\n\n\n| Factor            | LR χ² (Original, anova2) | LR χ² (Balanced, anova3) | Change in χ²       |\n|-------------------|---------------------------|----------------------------|---------------------|\n| age               | 120.407                   | 1201.85                    | ≈10.0× Increase     |\n| hypertension      | 18.205                    | 154.34                     | ≈8.5× Increase      |\n| avg_glucose_level | 11.337                    | 69.73                      | ≈6.1× Increase      |\n\nWe additionally can observe that there is an massive increase in the $\\chi^2$ values which demonstrates that the oversampling technique has significantly increased the statistical power of the model.\n\nIn summary, the ANOVA test confirmed that balancing the training data has dramatically increased the statistical confidence in the predictive power of the model, which directly led to the massive improvement in the model's ability to identify true stroke cases what will be explored in the next section with a confusion matrix.\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n<!-- Fix confusion matrix to show model improvement -->\n\n#### **3.3.5. Confusion Matrix**\n\nThis section the confusion matrix demonstrates conclusive evidence the undersampling of Stroke cases yields a model with no predictive capability.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# 1) Predicted probabilities from logistic regression\ntest_data$pred_prob <- predict(\n  model_lr,\n  newdata = test_data,\n  type    = \"response\"\n)\n\ntest_data$pred_prob2 <- predict(\n  model2_lr,\n  newdata = test_data,\n  type    = \"response\"\n)\n\ntest_data$pred_prob3 <- predict(\n  model3_lr,\n  newdata = test_data,\n  type    = \"response\"\n)\n\n# 2) Make sure the TRUE outcome is a factor with levels No / Yes\ntest_data$stroke <- factor(test_data$stroke,\n                             levels = c(\"No\", \"Yes\"))\n\n# 3) Class predictions at threshold c = 0.5\ntest_data$pred_class <- ifelse(test_data$pred_prob >= 0.5, \"Yes\", \"No\")\ntest_data$pred_class2 <- ifelse(test_data$pred_prob2 >= 0.5, \"Yes\", \"No\")\ntest_data$pred_class3 <- ifelse(test_data$pred_prob3 >= 0.5, \"Yes\", \"No\")\n\ntest_data$pred_class <- factor(test_data$pred_class, levels = c(\"No\", \"Yes\"))\ntest_data$pred_class2 <- factor(test_data$pred_class2, levels = c(\"No\", \"Yes\"))\ntest_data$pred_class3 <- factor(test_data$pred_class3, levels = c(\"No\", \"Yes\"))\n\n# 4) Confusion matrix: positive = \"Yes\"\ncm <- confusionMatrix(\n  data      = test_data$pred_class,\n  reference = test_data$stroke,\n  positive  = \"Yes\"\n)\ncm2 <- confusionMatrix(\n  data      = test_data$pred_class2,\n  reference = test_data$stroke,\n  positive  = \"Yes\"\n)\ncm3 <- confusionMatrix(\n  data      = test_data$pred_class3,\n  reference = test_data$stroke,\n  positive  = \"Yes\"\n)\n# cm\n# cm2\n# cm3\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# --------------------------------------------------------\n# 1. Define a function to extract key metrics and counts\n# --------------------------------------------------------\nextract_metrics <- function(cm_object, model_name) {\n  # Extract per-class statistics (Sensitivity, Specificity) and Overall Accuracy\n  stats <- cm_object$byClass\n  Accuracy <- cm_object$overall['Accuracy']\n  \n  # Extract Confusion Matrix table for counts\n  cm_table <- cm_object$table\n  \n  # Extract the counts (Assuming 'Yes' is the positive class, top-left is TN)\n  TP <- cm_table['Yes', 'Yes'] # True Positives\n  FN <- cm_table['Yes', 'No']  # False Negatives\n  TN <- cm_table['No', 'No']   # True Negatives\n  FP <- cm_table['No', 'Yes']  # False Positives\n  \n  # Create a data frame with metrics as rows\n  data.frame(\n    Metric = c(\n      \"Accuracy\",\n      \"Sensitivity (Recall)\",\n      \"Specificity\",\n      \"True Positives (TP)\",\n      \"False Negatives (FN)\",\n      \"True Negatives (TN)\",\n      \"False Positives (FP)\"\n    ),\n    Value = c(\n      Accuracy,\n      stats['Sensitivity'],\n      stats['Specificity'],\n      TP,\n      FN,\n      TN,\n      FP\n    ),\n    stringsAsFactors = FALSE\n  ) %>%\n    # Rename the Value column to the model name\n    dplyr::rename(!!model_name := Value)\n}\n\n# --------------------------------------------------------\n# 2. Extract metrics for all three models\n# --------------------------------------------------------\nmetrics_cm <- extract_metrics(cm, \"Model 1 (Full, Imbalanced)\")\nmetrics_cm2 <- extract_metrics(cm2, \"Model 2 (Reduced, Imbalanced)\")\nmetrics_cm3 <- extract_metrics(cm3, \"Model 3 (Reduced, Balanced)\")\n\n# --------------------------------------------------------\n# 3. Merge the three data frames into one comparison table\n# --------------------------------------------------------\ncomparison_table <- metrics_cm %>%\n  dplyr::full_join(metrics_cm2, by = \"Metric\") %>%\n  dplyr::full_join(metrics_cm3, by = \"Metric\")\n\n# --------------------------------------------------------\n# 4. Format the table for clean output\n# --------------------------------------------------------\n\n# Select the rows that are proportions/percentages (1-3) and format to 4 decimal places\ncomparison_table[1:3, 2:4] <- lapply(\n  comparison_table[1:3, 2:4],\n  function(x) { format(as.numeric(x), digits = 4, scientific = FALSE) }\n)\n\n# Select the rows that are counts (4-7) and format as integers\ncomparison_table[4:7, 2:4] <- lapply(\n  comparison_table[4:7, 2:4],\n  function(x) { format(as.integer(x), big.mark = \",\") }\n)\n\n# Print the final formatted table\nprint(comparison_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Metric Model 1 (Full, Imbalanced) Model 2 (Reduced, Imbalanced)\n1             Accuracy                    0.94538                        0.9444\n2 Sensitivity (Recall)                    0.01852                        0.0000\n3          Specificity                    0.99790                        0.9979\n4  True Positives (TP)                          1                             0\n5 False Negatives (FN)                          2                             2\n6  True Negatives (TN)                        951                           951\n7 False Positives (FP)                         53                            54\n  Model 3 (Reduced, Balanced)\n1                      0.7269\n2                      0.6481\n3                      0.7314\n4                          35\n5                         256\n6                         697\n7                          19\n```\n\n\n:::\n:::\n\n\n\n| Metric                | Model 1 (Full, Imbalanced) | Model 2 (Reduced, Imbalanced) | Model 3 (Reduced, Balanced) |\n|-----------------------|-----------------------------|--------------------------------|------------------------------|\n| Accuracy              | 0.94538                     | 0.9444                        | 0.7269                      |\n| Sensitivity (Recall)  | 0.01852                     | 0.0000                        | 0.6481                      |\n| Specificity           | 0.99790                     | 0.9979                        | 0.7314                      |\n| True Positives (TP)   | 1                           | 0                              | 35                          |\n| False Negatives (FN)  | 2                           | 2                              | 256                         |\n| True Negatives (TN)   | 951                         | 951                            | 697                         |\n| False Positives (FP)  | 53                          | 54                             | 19                          |\n\n\nFrom the confusion matrix, the following performance metrics are defined:\n\nThe comparison of the three Logistic Regression models using the confusion matrix reveals that while the imbalanced models (Model 1 and Model 2) achieved high Accuracy ($\\approx 94.5\\%$) and Specificity ($\\approx 0.998$), they were practically useless for stroke prediction with a near-zero Sensitivity and missing almost all actual stroke cases. By contrast, Model 3 which utilized oversampling to address the severe class imbalance in stroke outcome, demonstrated a significant improvement in predictive capability: Sensitivity dramatically improved to $0.6481$ being able to identifying 35 True Positives, confirming that balancing successfully forced the model to learn the patterns of the minority class of stroke outcome. However, this critical gain in recall came with a trade-off, it lowered the Accuracy to $0.7269$ and Specificity to $0.7314$ due to an increase in False Negatives registering 256 cases, but the resulting model is a far more functional screening tool, prioritizing the detection of the outcome  stroke over overall classification correctness.\n\n\n<!-- TODO -->\n<!-- Add Assumption testing here -->\n\n* The observations must be independent.\n* There must be no perfect multicollinearity among independent variables. Use the VIF.\n* Logistic regression assumes linearity of independent variables and log odds.\n* There are no extreme outliers, check using Cooks D\n* The Sample Size is Sufficiently Large. \n\n**Check Multicollinearity**\n\nIn OLS regression, multicollinearity can be calculated either from the correlations among the predictors, or from the correlations among the coefficient estimates, and these result in the same variance inflaction factors (VIFs).\n\nIn GLMs, these two approaches yield similar but different VIFs. John Fox, one of the authors of the car package where the vif() function is found, opts for calculating the VIFs from the coefficient estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(model_lr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                      GVIF Df GVIF^(1/(2*Df))\ngender            1.042583  1        1.021069\nage               1.224353  1        1.106505\nhypertension      1.038949  1        1.019288\nheart_disease     1.072781  1        1.035751\never_married      1.023266  1        1.011566\nwork_type         1.083443  3        1.013447\nResidence_type    1.012883  1        1.006421\navg_glucose_level 1.118062  1        1.057384\nbmi               1.158761  1        1.076457\nsmoking_status    1.086902  2        1.021051\n```\n\n\n:::\n\n```{.r .cell-code}\nvif(model2_lr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              age      hypertension avg_glucose_level \n         1.017823          1.016305          1.017019 \n```\n\n\n:::\n\n```{.r .cell-code}\nvif(model3_lr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              age      hypertension avg_glucose_level \n         1.006566          1.009300          1.015890 \n```\n\n\n:::\n:::\n\n\n\n## 4. Conclusion\n\nThis experiment evaluated the performance of a logistic regression model with common demographic, behavioral, and clinical characteristics using a public stroke dataset. @kaggle01 The findings were not promising because stroke is a rare outcome (about 5% of cases) and even after dealing with the class imbalance there is only marginal improvements, too many false positives. Althought the improvements were marginal the logistic regression model was a great interpretable tool for comprehending the relationship between particular risk variables and the likelihood of stroke and are in line with the clinical literature on cerebrovascular illness. For example we could identify that Age, hypertension, and raised average glucose levels are among the best predictors of stroke outcome.\n\nOur findings of the undesirable performance of Logistic Regression are on pair with other research @hassan2024predictive. More advanced techniques seem to be required for preprocessing the dataset such as **Mean Imputation** for replacing some missing values with the column's mean, **Multivariate Imputation by Chained Equations (MICE)** where we synthetically generate missing values based on other variables, and **Age Group-based Imputation** where we we categorize the age groups and replace missing BMI values with the mean BMI of the corresponding age group.\n\nBut the main solution for the problem might be implementation of the **Dense Stacking Ensemble (DSE) Model**, which uses the best-performing model (Random Forest) as a meta-classifier. This multi-model approach as epxlored in @hassan2024predictive seems to combine the simplicity and interpretability of Logistic Regression models with the superior performance of more sophisticated models. Overall, the findings show that relatively simple models built from routinely collected health indicators can generate meaningfull results when the proper class imbalance is deal and that proves that logistic regression emerges as a strong, interpretable baseline that can be further improved as demonstrated in @hassan2024predictive. In Future work we could explore the use of synthetically generate data and other Imputation techinques and the implementation of **Dense Stacking Ensemble (DSE) Model**. These extensions would help move towards a robust model with low false positive predictions, making the research into a clinically usable tool for stroke risk stratification and targeted prevention.\n\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n### References\n\n::: {#refs}\n:::\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣒⢋⡤⣹⣯⣴⣤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡔⠉⣽⣿⣿⣦⣉⣿⣯⠉⠴⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡸⠀⠆⠿⠻⠻⠟⠿⠿⢟⣶⡄⢹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣣⠏⠀⠀⠀⠀⠀⠀⠀⠀⠘⠇⠘⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡾⠀⠐⢀⠤⠀⠀⠀⠀⠀⠀⠀⡞⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡞⡁⠀⠀⠉⠀⠀⠀⠐⠂⠄⢸⡿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢿⠂⠀⠀⠀⠠⠀⠀⠀⠀⠀⠈⡻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢶⡄⠀⠀⠀⠀⠀⠀⠀⢀⣴⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⡀⠀⠀⠀⠀⠀⠀⢀⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡼⠀⠀⠀⠀⠀⠀⠀⠸⢇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⢀⣀⡀⠴⠙⣆⠀⠀⠀⠀⠀⠀⡠⢢⣿⣷⣤⢀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⣠⡔⠈⣡⣄⡄⠀⠀⠲⠛⠂⠀⠀⠀⠈⠀⢸⣿⣿⣿⣷⣶⣯⣀⣒⡤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⣠⡐⣉⣤⣶⣿⣿⣿⡇⠀⠀⠀⠀⠂⠐⠀⠀⠀⢠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣾⣵⡄⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣛⡀⠀⠀⠠⢄⣀⣐⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⡆⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⡯⠤⠀⢇⣘⡒⠲⢾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣥⠀⠀⠀⠀⠀⠀ -->\n<!-- ⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣟⣓⠀⢰⠤⢭⣽⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡧⠬⠀⢘⣛⣓⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣟⣒⠐⡷⠬⢭⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⠀⠀⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⡁⣛⣓⢲⣿⣿⣿⣿⣿⡏⠉⠛⠛⠛⠻⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡶⢲⠦⢭⣽⣿⣿⣿⣿⣿⣅⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀ -->\n<!-- ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣹⣛⣒⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣧ -->\n<!-- ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠾⠬⣭⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣄⠀⠀⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢼ -->\n<!-- ⠻⣿⣿⣿⣿⠛⠋⢋⠁⠀⠀⠀⣶⢗⣛⣻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣶⣾⣿⣿⡿⣿⣿⣿⣿⣿⣿⣿⣾ -->\n<!-- ⠀⣿⣿⣿⣷⣾⣿⠀⠀⠁⠀⠀⣸⡆⠾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣿⣿⣿⣿⣿⣿⣿⣯ -->\n<!-- ⠀⠈⠻⣿⣿⣿⣿⡆⠀⠤⠄⣰⣿⡇⣭⣿⣿⣿⣿⣿⣿⣿⣿⡿⠻⣿⣿⣿⣿⣿⣿⣿⣿⠿⣿⣿⣿⡿⠟⠋⠁ -->\n<!-- ⠀⠀⠀⠀⠉⠺⢿⣿⣿⣿⣿⣿⣿⣧⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢸⡿⢹⣿⣿⣿⣿⡇⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⡸⢡⣿⣿⣿⣿⣿⠀⢹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⣷⣿⣿⣿⣿⣿⣿⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⢨⣿⣿⣿⣿⣿⣿⣿⣄⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠀ -->\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}