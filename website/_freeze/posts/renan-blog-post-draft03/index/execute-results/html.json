{
  "hash": "f6386358040ddd852206ea867caca148",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Draft for Final Report - v03\"\ndescription: \"Experimenting with draft format+code\"\nauthor:\n  - name: Renan Monteiro Barbosa\n    url: https://github.com/renanmb\n    affiliation: Master of Data Science Program @ The University of West Florida (UWF)\n    # affiliation-url: https://ucsb-meds.github.io/\n# date: 10-24-2022\ncategories: [drafts, renan]\n# citation:\n#   url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/\nimage: images/spongebob-imagination.jpg\ndraft: false\nbibliography: references.bib\nlink-citations: true\n---\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\npackages <- c(\"dplyr\", \"car\", \"ResourceSelection\", \"caret\", \"pROC\",  \"logistf\", \"Hmisc\", \"rcompanion\", \"ggplot2\", \"summarytools\", \"tidyverse\", \"knitr\", \"ggpubr\")\n# install.packages(packages)\n\n# Load Libraries\nlapply(packages, library, character.only = TRUE)\n\n# Set seed for reproducibility\nset.seed(123)\n```\n:::\n\n\nLoading Dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfind_git_root <- function(start = getwd()) {\n  path <- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path <- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root <- find_git_root()\ndatasets_path <- file.path(repo_root, \"datasets\")\n\n# Reading the datafile healthcare-dataset-stroke-data\nsteve_dataset_path <- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\nstroke1 = read_csv(steve_dataset_path, show_col_types = FALSE)\n```\n:::\n\n\nHandling Dataset Features\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1[stroke1 == \"N/A\" | stroke1 == \"Unknown\" | stroke1 == \"children\" | stroke1 == \"other\"] <- NA\nstroke1$bmi <- round(as.numeric(stroke1$bmi), 2)\nstroke1$gender[stroke1$gender == \"Male\"] <- 1\nstroke1$gender[stroke1$gender == \"Female\"] <- 2\nstroke1$gender <- as.numeric(stroke1$gender)\nstroke1$ever_married[stroke1$ever_married == \"Yes\"] <- 1\nstroke1$ever_married[stroke1$ever_married == \"No\"] <- 2\nstroke1$ever_married <- as.numeric(stroke1$ever_married)\nstroke1$work_type[stroke1$work_type == \"Govt_job\"] <- 1\nstroke1$work_type[stroke1$work_type == \"Private\"] <- 2\nstroke1$work_type[stroke1$work_type == \"Self-employed\"] <- 3\nstroke1$work_type[stroke1$work_type == \"Never_worked\"] <- 4\nstroke1$work_type <- as.numeric(stroke1$work_type)\nstroke1$Residence_type[stroke1$Residence_type == \"Urban\"] <- 1\nstroke1$Residence_type[stroke1$Residence_type == \"Rural\"] <- 2\nstroke1$Residence_type <- as.numeric(stroke1$Residence_type)\nstroke1$avg_glucose_level <- as.numeric(stroke1$avg_glucose_level)\nstroke1$heart_disease <- as.numeric(stroke1$heart_disease)\nstroke1$hypertension <- as.numeric(stroke1$hypertension)\nstroke1$age <- round(as.numeric(stroke1$age), 2)\nstroke1$stroke <- as.numeric(stroke1$stroke)\nstroke1$smoking_status[stroke1$smoking_status == \"never smoked\"] <- 1\nstroke1$smoking_status[stroke1$smoking_status == \"formerly smoked\"] <- 2\nstroke1$smoking_status[stroke1$smoking_status == \"smokes\"] <- 3\nstroke1$smoking_status <- as.numeric(stroke1$smoking_status)\nstroke1 <- stroke1[, !(names(stroke1) %in% \"id\")]\n```\n:::\n\n\nRemoving NAs and cleaning Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nstroke1$stroke <- as.factor(stroke1$stroke)\nstroke1_clean <- na.omit(stroke1)\nstrokeclean <- stroke1_clean\nfourassume <- stroke1_clean\n```\n:::\n\n\nShowing Descriptive Statistics for all variables, Mean, Std Deviation, and Interquartile Range\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# dfSummary(strokeclean)\n```\n:::\n\n\n\n## 1. Introduction\n\nThe burden of cerebrovascular accidents, or stroke, remains a major global public health crisis. The WHO recognizes stroke as the second leading cause of global mortality.[@citation1] Given the high morbidity and mortality associated with stroke, the accurate and timely identification of individuals at high risk is a critical priority for healthcare systems globally. Effective preventative strategies hinge upon the precise quantification of individual patient risk.[@citation2]\n\nHistorically, risk stratification has relied on conventional clinical scoring systems, which utilize established clinical characteristics and comorbidities to approximate the future likelihood of cardiovascular disease (CVD) events, including stroke.[@citation2] Because the risk of stroke is intrinsically linked with the risk of other cardiovascular diseases, clinically useful risk scores often encompass multiple related CVD outcomes.[TODO 13] By calculating a patient's risk profile, clinicians are empowered to implement evidence-based interventions, such as initiating statin therapy or recommending specific lifestyle modifications, thereby reducing the overall incidence of CVD and improving long-term health outcomes.[TODO 3 + 13] [@citation2]\n\n### 1.2. The Shift Towards Data-Driven Clinical Prediction\n\nIn recent decades, the increasing availability of granular patient data has accelerated new research trends focused on personalized prediction and disease management. [@citation14] The capacity of modern data systems to handle complex, high-dimensional datasets necessitates the use of computational tools, often in the form of Artificial Intelligence (AI) and Machine Learning (ML) systems.[TODO 15] [@citation14 , @citation15] ML algorithms have demonstrated a superior capacity to predict functional recovery after ischemic stroke compared with preexisting scoring systems based on conventional statistics.[TODO 16] These models can automatically select important features and variables, often reducing the necessity for manual feature engineering.[@citation17]\n\nThe application of ML methods spans a range of tasks from unsupervised learning for pattern discovery to supervised learning for diagnosis and prognosis. [@citation15] While complex models, such as ensemble techniques or deep neural networks, may achieve marginally higher discrimination scores (AUROC), their clinical utility is constrained by their opacity. Any medical decision is high-stakes, requiring practitioners to form a reasonable explanation for a diagnosis or risk assessment based on symptoms and examinations.[@citation18] The \"black box\" nature of complex models, making it difficult to fully understand how a specific output was generated, can lead to mistrust among clinicians and patients and may negatively impact their acceptance and implementation.[TODO 19 + 20]\n\n### 1.3. Justification for Logistic Regression in Medical Informatics\n\nDespite the rise of sophisticated algorithms, Logistic Regression (LR) remains the most widely used modeling approach in stroke research. [@citation4] LR provides a robust, transparent framework for modeling binary outcomes, such as the presence or absence of a stroke event.[TODO 21] [@citation5] The procedure is statistically analogous to multiple linear regression but handles the binomial response variable, yielding quantifiable results in the form of Odds Ratios (ORs). [@citation22] This ability to quantify the independent impact of each variable on the probability of the event—by controlling for confounding effects—is the central advantage of LR. [@citation22]\n\nThe primary justification for employing LR is rooted in the performance-interpretability trade-off. [@citation5] The ability to interpret the model through $\\beta$ coefficients and their corresponding ORs, alongside associated $p$-values and confidence intervals, sets LR apart from more complex ML approaches. [@citation5] This explicit structure allows for direct assessment of the direction and magnitude of risk, a requirement for evidence-based medicine.[@citation5]  While more complex models might achieve greater numerical performance, the lack of transparency can erode provider trust and patient reliance on the technology.[TODO 20] When considering clinical application, the simplicity of LR ensures that the mechanism of prediction is traceable, which is essential for safety, equity, and accountability in healthcare deployment.[TODO 20] [@citation18]\n\n### 1.4. Study Objectives and Reproducibility\n\nThis study aims to rigorously validate a multivariate Logistic Regression model for binary stroke prediction using a standardized set of 11 clinical features. A core objective is to move beyond simple comparison metrics like accuracy [@citation23] and utilize advanced evaluation techniques specifically tailored for imbalanced medical outcomes, such as AUPRC, Sensitivity, and Calibration, to properly contextualize the LR model's clinical utility.[TODO 16] [@citation10] Furthermore, this analysis demonstrates a commitment to transparency and scholarly practice by implementing the entire analytical pipeline within a Quarto workflow. [@citation7] This process ensures the findings are readily reproducible by the academic and clinical community, aligning with modern standards for robust scientific computing and communication.[TODO 24] \n\n## 2. Methodology\n\nWe chose our topic as logistic regression, and subsequently chose the stroke dataset b by Krekorian in Kaggle dataset comparing people with 11 different predictor variables and 1 binary outcome variable, stroke or no stroke.\n\nWe then uploaded the Kaggle dataset into R studio server and analyzed it with R. We first utilized 10 different packages and libraries. They are listed below. These packages and libraries gave us the statistical models we then used to analyze the Kaggle dataset.\n\nWe uploaded the dataset to Rstudioserver and installed the following packages and libraries for our analysis:\n\n1. dplyr\n2. car\n3. ResourceSelection\n4. caret\n5. pROC\n6. Logistf\n7. Hmisc\n8. rcompanion\n9. summary tools\n\nWe first prepared the data, ensuring that all variables in the dataset, both predictor and outcome variables were converted or recoded to numeric as follows:\n\n**(1)** age (continuous), we decided to recode to numeric with 2 places after the decimal.\n\n**(2)** gender (categorical) we coded 1 for male and 2 for female. There was only 1 case where it was coded other. We recoded other as N/A. We also recoded this predictor as numeric.\n\n**(3)** hypertension(categorical) was recoded to numeric\n\n**(4)** heart disease(categorical) was recoded to numeric\n\n**(5)** marital status (categorical) was recoded from yes to 1 and no to 2 and retyped as numeric,\n\n**(6)** Work type(categorical) was recoded as 1 = Government, 2 = private sector, 3 = self-employed, 4 = never worked and then retyped as numeric\n\n**(7)** residence type (categorical) was recoded as 1 = urban and 2 – rural. Then retyped as numeric.\n\n**(8)** bmi (continuous) was recoded as numeric with 2 places after the decimal\n\n**(9)** average glucose level(continuous) was recoded as numeric with 2 places after the decimal\n\n**(10)** smoking status(categorical) was recoded as 1 = never smoked, 2 = formerly smoked and 3 = smokes, and unknown was recoded as N/A. After deletion of N/A the data was retyped as numeric.\n\n**(11)** ID number -was left as is and deleted because it’s not needed\n\n**(12)** Stroke (outcome) is categorical has 2 categories, 1 = stroke, 0 = no stroke\n\nOnce that was done, we got rid of extraneous values such as “N/A”. After deleting rows that were useless or irrelevant values were left with a dataset of 3357 cases, 11 predictor variables and an outcome variable. As the rule of thumb for minimal size to run analyses is 15 cases per number of predictor variables.  Applying this rule of thumb to our project, the dataset’s minimum is 132 cases. Since the cleaned dataset has 3357 cases, we can use logistic regression on the dataset.\n\n## 3. Analysis and Results\n\nExamining the data in the variables: Data Frame & Descriptive Statistics\n\n\n\n### Dataframe\n\nADD the Dataframe here might want to use knitr table\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfSummary(strokeclean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData Frame Summary  \nstrokeclean  \nDimensions: 3357 x 11  \nDuplicates: 0  \n\n----------------------------------------------------------------------------------------------------------------------\nNo   Variable            Stats / Values             Freqs (% of Valid)     Graph                  Valid      Missing  \n---- ------------------- -------------------------- ---------------------- ---------------------- ---------- ---------\n1    gender              Min  : 1                   1 : 1305 (38.9%)       IIIIIII                3357       0        \n     [numeric]           Mean : 1.6                 2 : 2052 (61.1%)       IIIIIIIIIIII           (100.0%)   (0.0%)   \n                         Max  : 2                                                                                     \n\n2    age                 Mean (sd) : 49.4 (18.3)    70 distinct values             . : .          3357       0        \n     [numeric]           min < med < max:                                    . . : : : : .   :    (100.0%)   (0.0%)   \n                         13 < 50 < 82                                        : : : : : : : : :                        \n                         IQR (CV) : 28 (0.4)                               : : : : : : : : : :                        \n                                                                           : : : : : : : : : :                        \n\n3    hypertension        Min  : 0                   0 : 2949 (87.8%)       IIIIIIIIIIIIIIIII      3357       0        \n     [numeric]           Mean : 0.1                 1 :  408 (12.2%)       II                     (100.0%)   (0.0%)   \n                         Max  : 1                                                                                     \n\n4    heart_disease       Min  : 0                   0 : 3151 (93.9%)       IIIIIIIIIIIIIIIIII     3357       0        \n     [numeric]           Mean : 0.1                 1 :  206 ( 6.1%)       I                      (100.0%)   (0.0%)   \n                         Max  : 1                                                                                     \n\n5    ever_married        Min  : 1                   1 : 2599 (77.4%)       IIIIIIIIIIIIIII        3357       0        \n     [numeric]           Mean : 1.2                 2 :  758 (22.6%)       IIII                   (100.0%)   (0.0%)   \n                         Max  : 2                                                                                     \n\n6    work_type           Mean (sd) : 2 (0.6)        1 :  514 (15.3%)       III                    3357       0        \n     [numeric]           min < med < max:           2 : 2200 (65.5%)       IIIIIIIIIIIII          (100.0%)   (0.0%)   \n                         1 < 2 < 4                  3 :  629 (18.7%)       III                                        \n                         IQR (CV) : 0 (0.3)         4 :   14 ( 0.4%)                                                  \n\n7    Residence_type      Min  : 1                   1 : 1709 (50.9%)       IIIIIIIIII             3357       0        \n     [numeric]           Mean : 1.5                 2 : 1648 (49.1%)       IIIIIIIII              (100.0%)   (0.0%)   \n                         Max  : 2                                                                                     \n\n8    avg_glucose_level   Mean (sd) : 108.4 (47.9)   2861 distinct values     :                    3357       0        \n     [numeric]           min < med < max:                                  . :                    (100.0%)   (0.0%)   \n                         55.1 < 92.3 < 271.7                               : : :                                      \n                         IQR (CV) : 39 (0.4)                               : : :                                      \n                                                                           : : : : . . . . .                          \n\n9    bmi                 Mean (sd) : 30.4 (7.2)     364 distinct values      . :                  3357       0        \n     [numeric]           min < med < max:                                    : :                  (100.0%)   (0.0%)   \n                         11.5 < 29.2 < 92                                    : :                                      \n                         IQR (CV) : 8.8 (0.2)                                : : :                                    \n                                                                           . : : : .                                  \n\n10   smoking_status      Mean (sd) : 1.7 (0.8)      1 : 1798 (53.6%)       IIIIIIIIII             3357       0        \n     [numeric]           min < med < max:           2 :  824 (24.5%)       IIII                   (100.0%)   (0.0%)   \n                         1 < 1 < 3                  3 :  735 (21.9%)       IIII                                       \n                         IQR (CV) : 1 (0.5)                                                                           \n\n11   stroke              1. 0                       3177 (94.6%)           IIIIIIIIIIIIIIIIII     3357       0        \n     [factor]            2. 1                        180 ( 5.4%)           I                      (100.0%)   (0.0%)   \n----------------------------------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\nAfter confirming the numerical type of each variable, we ran some descriptive statistics. The mean, standard deviation, and the interquartile range (IQR).\n\n| Variable            | Mean  | Standard Deviation | Min–Max        | IQR |\n|--------------------|-------|---------------------|----------------|-----|\n| gender             | 1.6   | —                   | 1–2            | —   |\n| age                | 49.4  | 18.3                | 13–82          | 28  |\n| hypertension       | 0.1   | —                   | 0–1            | —   |\n| heart_disease      | 0.1   | —                   | 0–1            | —   |\n| ever_married       | 1.2   | —                   | 1–2            | —   |\n| work_type          | 2     | 0.6                 | 1–4            | 0   |\n| Residence_type     | 1.5   | —                   | 1–2            | —   |\n| avg_glucose_level  | 108.4 | 47.9                | 55.1–271.7     | 39  |\n| bmi                | 30.4  | 7.2                 | 11.5–92        | 8.8 |\n| smoking_status     | 1.7   | 0.8                 | 1–3            | 1   |\n| stroke             | —     | —                   | 0–1            | —   |\n\n**Notes:**\n\nFor categorical/binary variables, standard deviation and IQR are not shown\n\nHaving checked the min-max, and std, mean, and IQR for any “anomalies” and finding none, we then created a histogram of each of the variables to view their frequency distribution a shown below.\n\n<!-- ADD histogram plots here -->\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Histogram of gender\np2a <- ggplot(strokeclean, aes(x = gender)) +\n  geom_bar(fill = \"blue\", \n           color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"Histogram of gender\", \n       x = \"gender\", \n       y = \"Frequency\")\n\n# (a) Histogram of gender\np1a <- ggplot(strokeclean, aes(x = gender, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(a) Gender\", x = \"gender\", y = \"Count\")\n\n# Histogram of Age\np2b <- ggplot(strokeclean, aes(x = age)) +\n  geom_histogram(binwidth = 5, \n                 fill = \"green\", \n                 color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"Histogram of Age\", \n       x = \"Age\", \n       y = \"Frequency\")\n\n# (b) Histogram of Age\np1b <- ggplot(strokeclean, aes(x = age, fill = stroke)) +\n  geom_histogram(binwidth = 5, position = \"identity\", alpha = 0.7) +\n  # stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(b) Age\", x = \"Age\", y = \"Frequency\")\n\n# Histogram of hypertension\np2c <- ggplot(strokeclean, aes(x = hypertension)) +\n  geom_bar(fill = \"purple\", \n           color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"Histogram of hypertension\", \n       x = \"hypertension\", \n       y = \"Frequency\")\n\n# (c) Histogram of hypertension\np1c <- ggplot(strokeclean, aes(x = hypertension, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(c) Hypertension\", x = \"hypertension\", y = \"Frequency\")\n\n# Histogram of heart_disease\np2d <- ggplot(strokeclean, aes(x = heart_disease)) +\n  geom_bar( fill = \"orange\",\n            color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"Histogram of heart_disease\", \n       x = \"HeartDisease\", \n       y = \"Frequency\")\n\n# (d) Histogram of heart_disease\np1d <- ggplot(strokeclean, aes(x = heart_disease, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(d) Heart Disease\", x = \"HeartDisease\", y = \"Frequency\")\n\n# Histogram of ever_married\np2e <- ggplot(strokeclean, aes(x = ever_married)) +\n  geom_bar(fill = \"aquamarine\", \n           color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"Histogram of ever_married\", \n       x = \"EverMarried\", \n       y = \"Frequency\")\n\n# (e) Histogram of ever_married\np1e <- ggplot(strokeclean, aes(x = ever_married, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(e) Ever Married\", x = \"EverMarried\", y = \"Frequency\")\n\n\n# Histogram of work_type\np2f <- ggplot(strokeclean, aes(x = work_type)) +\n  geom_bar(fill = \"steelblue\", \n           color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"Histogram of work_type\", \n       x = \"WorkType\", \n       y = \"Frequency\")\n\n# (f) Histogram of work_type\np1f <- ggplot(strokeclean, aes(y = work_type, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(f) Work Type\", y = \"WorkType\", x = \"Frequency\")\n\n# Histogram of Residence_type\np2g <- ggplot(strokeclean, aes(x = Residence_type)) +\n  geom_bar(fill = \"magenta\", \n           color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"Histogram of Residence_type\", \n       x = \"Residence_type\", \n       y = \"Frequency\")\n\n# (g) Histogram of Residence_type\np1g <- ggplot(strokeclean, aes(x = Residence_type, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(g) Residence Type\", x = \"Residence_type\", y = \"Frequency\")\n\n\n# Histogram of avg_gloucose_level\np2h <- ggplot(strokeclean, aes(x = avg_glucose_level)) +\n  geom_histogram(binwidth = 5, \n                 fill = \"chartreuse\", \n                 color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"Histogram of avg_gloucose_level\",\n       x = \"avg-glucose_level\", \n       y = \"Frequency\")\n\n# (h) Histogram of avg_gloucose_level\np1h <- ggplot(strokeclean, aes(x = avg_glucose_level, fill = stroke)) +\n  geom_histogram(binwidth = 10, position = \"identity\", alpha = 0.7) +\n  # stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(h) Avg. Glucose Level\", x = \"Glucose Level\", y = \"Frequency\")\n\n\n# Histogram of bmi\np2i <- ggplot(strokeclean, aes(x = bmi)) +\n  geom_histogram(binwidth = 5, \n                 fill = \"gold\", \n                 color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"Histogram of bmi\", \n       x = \"bmi\", \n       y = \"Frequency\")\n\n# (i) Histogram of bmi\np1i <- ggplot(strokeclean, aes(x = bmi, fill = stroke)) +\n  geom_histogram(binwidth = 2, position = \"identity\", alpha = 0.7) +\n  labs(title = \"(i) BMI\", x = \"BMI\", y = \"Frequency\")\n\n# smoking_status\np2j <- ggplot(strokeclean, aes(x = smoking_status)) +\n  geom_bar(fill = \"deepskyblue\", \n           color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"smoking_status\", \n       x = \"smoking_status\", \n       y = \"Frequency\")\n\n# (j) smoking_status\np1j <- ggplot(strokeclean, aes(y = smoking_status, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(j) Smoking Status\", y = \"smoking_status\", x = \"Frequency\")\n\n\n# Histogram of Age\np2k <- ggplot(strokeclean, aes(x = stroke)) +\n  geom_bar(fill = \"tan\", \n           color = \"white\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"Histogram of Age\", \n       x = \"stroke\", \n       y = \"Frequency\")\n\n# (k) Histogram of Age\np1k <- ggplot(strokeclean, aes(x = age, fill = stroke)) +\n  geom_histogram(binwidth = 5, position = \"identity\", alpha = 0.7) +\n  # stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(b) Age\", x = \"Age\", y = \"Frequency\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine all plots into a single figure\nggarrange(p2a, p2b, p2c, p2d, p2e, p2f, p2g, p2h, p2i, p2j, p2k, \n          ncol = 4, nrow = 3, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine all plots into a single figure\nggarrange(p1a, p1b, p1c, p1d, p1e, p1f, p1g, p1h, p1i, p1j, p1k, \n          ncol = 4, nrow = 3, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n<!-- ADD histogram plots here -->\n\nAs we didn’t see any abnormal data points from the histograms we then proceeded to review and justify our selection of Logistic Regression.\n\n## 4. Mathematical Formulation\n\n**What is Logistic Regression**\n\nLogistic regression is a statistical modeling technique that predicts the probability of a binary outcome (such as 0 or 1) using one or more independent variables.\n\n<!-- separate here -->\n\nThe key idea is to model the log odds (also called the logit) of the probability of the event as a linear function of the predictors:\n\n* The key idea is to model the log odds (also called the logit) of the probability of the event as a linear function of the predictors:\n\n$\\log\\left(\\frac{P(Y=1|X)}{1 - P(Y=1|X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k$\n\n$\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k$\n\nwhere pp is the probability of the outcome (e.g., stroke), the xixi are predictors, and the βiβi are their coefficients.​\n\n* Solving for pp, the equation becomes:\n\n$p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k)}}$\n\nThis is the logistic function, which always outputs values between 0 and 1, making it ideal for probabilities.​\n\n## 5. Core Concepts\n\n<!-- TODO fix the odds -->\n\n**(1)** Odds are defined as $\\frac{p}{1 - p}$ p/(1−p)p/(1-p)p/(1−p), the ratio of the probability of the event to the probability of its complement.\n\n**(2)** The logit transformation (natural log of the odds) turns this nonlinear problem into a linear one, so standard linear modeling techniques can be used for estimation.\n\n**(3)** Coefficients ($\\beta$) are commonly estimated using maximum likelihood methods, not ordinary least squares.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula <- stroke ~ gender + age + hypertension + heart_disease + ever_married +\n  work_type + Residence_type + avg_glucose_level + bmi + smoking_status\n```\n:::\n\n\nA comparison between Logistic Regression and Multiple Regression is shown below\n\n| Feature                 | Multiple Regression                       | Logistic Regression                                      |\n|-------------------------|---------------------------------------------|-----------------------------------------------------------|\n| Outcome variable type   | Continuous (real numbers)                   | Categorical/Binary (e.g., 0 or 1)                         |\n| Example prediction      | Predicting house prices                     | Predicting disease presence/absence                      |\n| Model equation          | Linear combination of predictors            | Log odds/logit (S-shaped curve: logistic function)        |\n| Estimation method       | Least squares                               | Maximum likelihood                                        |\n| Output type             | Actual values (e.g., $125,000)              | Probability of being in a category (e.g., 87%)            |\n| Usage                   | Continuous outcome (income, cost, score)    | Categorical outcome (yes/no, 0/1)                         |\n\n\nBut before we can run the all the models of Logistic Regression, there are 4 assumptions of Logistic Regression that we need to determine if the dataset and models can run without violating any or all the assumptions of Logistic Regression\n\n\n**Assumption 1:** the outcome variable has 2 outcomes, stroke, or no stroke. This is \tconfirmed from the data frame above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assumption 1: The Outcome Variable is 0 or 1\nunique(fourassume$stroke)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 0\nLevels: 0 1\n```\n\n\n:::\n:::\n\n\n**Assumption 2:** There is a linear relationship between each of the predictor variables and the outcome variable. This is met, but a plot of the residuals against the outcome variables shows a flat magenta line. As shown below\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assumption 2: There is linear relationship between the outcome variable and each predictor that is numeric. Categorical predictors are reviewed in the histograms avove\nfourassume$ageadj <- fourassume$age + abs(min(fourassume$age)) + 1\nfourassume$avg_glucose_leveladj <- fourassume$avg_glucose_level + abs(min(fourassume$avg_glucose_level)) + 1\nfourassume$bmiadj <- fourassume$bmi + abs(min(fourassume$bmi)) + 1\nstr(fourassume)\nnumeric_vars <- sapply(fourassume, is.numeric)\nfourassume_numeric <- fourassume[, numeric_vars]\nrcorr(as.matrix(fourassume_numeric))\nfourAdj <- fourassume\nfourAdj <- fourAdj[ , !(names(fourAdj) %in% c(\"age\", \"heart_disease\", \"avg_glucose_level\", \"bmi\")) ]\nmodel4 <- glm(stroke ~ ageadj + avg_glucose_leveladj + bmiadj, data=fourAdj, family=binomial)\n# residualPlots(model4)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresidualPlots(model4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     Test stat Pr(>|Test stat|)\nageadj                  1.9958           0.1577\navg_glucose_leveladj    0.0070           0.9331\nbmiadj                  0.3549           0.5514\n```\n\n\n:::\n:::\n\n\n**Assumption 3:**  There are no substantial outliers. We can demonstratre this by using Cooks D shows a range between 0 and .0122. The rule of thumb is 4/ the nsize. Ie 4/3577 or .0012.  While our value of .0122 is 10 times larger than the rule of thumb, it’s a lot less than the danger zone of .05. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assumption 3: Assess Influentional Outliers that are numeric. Categorical predictors are reviewed n the hhistrams above\nalias(model4)\nrcorr(as.matrix(fourassume_numeric))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninfluencePlot(model4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        StudRes          Hat       CookD\n17    2.3495313 0.0040917969 0.014793349\n83    2.5500288 0.0045425981 0.026906139\n87    3.0778217 0.0004677603 0.012890963\n131   3.2110607 0.0003364260 0.014101935\n186  -0.7488292 0.0184781611 0.001532740\n2583 -0.7113787 0.0167417735 0.001232964\n```\n\n\n:::\n:::\n\n\n**Assumption 4:**  Finally, there is no multicollinearity as shown by using vif from the car package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assumption 4: Assess Multicollinearity for numeric predictors\nvif(model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              ageadj avg_glucose_leveladj               bmiadj \n            1.070909             1.081460             1.101382 \n```\n\n\n:::\n:::\n\n\nThe results show that range is 1.01 to 1.21. Multicollinearity becomes a danger at substantially higher values i.e., 5 or 10.  This means there is no collinearity.\n\n**Conclusion:** \n\nthe 4 assumptions of Logistic Regression are met.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit of the Model with Nagelkerke R\nhoslem.test(model4$y, fitted(model4), g = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tHosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  model4$y, fitted(model4)\nX-squared = 8.8522, df = 8, p-value = 0.3549\n```\n\n\n:::\n\n```{.r .cell-code}\nnagelkerke(model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Models\n                                                                                \nModel: \"glm, stroke ~ ageadj + avg_glucose_leveladj + bmiadj, binomial, fourAdj\"\nNull:  \"glm, stroke ~ 1, binomial, fourAdj\"                                     \n\n$Pseudo.R.squared.for.model.vs.null\n                             Pseudo.R.squared\nMcFadden                            0.1713030\nCox and Snell (ML)                  0.0691131\nNagelkerke (Cragg and Uhler)        0.2022700\n\n$Likelihood.ratio.test\n Df.diff LogLik.diff  Chisq   p.value\n      -3     -120.21 240.42 7.721e-52\n\n$Number.of.observations\n           \nModel: 3357\nNull:  3357\n\n$Messages\n[1] \"Note: For models fit with REML, these statistics are based on refitting with ML\"\n\n$Warnings\n[1] \"None\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predictive Capability\nmodel4_CM <- glm(stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data=fourassume, family = binomial)\n```\n:::\n\n\n## 6. Developing 3 Different Logistic Regression Models\n\nWe decided to develop 3 different logistic regression models. The rationale for this came from the percentage of strokes from the Kaggle dataset compared to the percentage of strokes in the US. The percentage of strokes in the Kagle dataset is 5.6%, compared the CDC’s percentage of strokes at 3.1%.  There are problems with bias, separation and skewed predicted probabilities.\n\n**(1)** **Small Sample Bias** is an issue where the outcome is a stroke, is rare, which could produce biased parameter estimates. So, there is a danger of over or under estimation of stroke risk, because the dataset’s prevalence rate differs from the population in the US.\n\n**(2)** **Separation:** if the dataset is imbalanced there is a danger of categories of predictors predicting the outcome at a perfect percentage of 100% or near perfect. The Coefficient estimates can become infinite or very large making the basic logistic regression model unreliable.\n\n**(3)** **Miscallibrated probabilities:** The predicted probabilities from standard logistic regression can be skewed when the datasets outcome of a stroke, 5.6% doesn’t match the population levels.\n\nBecause of these reasons, 2 alternative models are being used to compare. Firth Regression and Refinement of Firth Regression called FLIC.\n\nIn datasets of rare events, **Firth Regression** introduces bias reduction through Jeffries Prior that reduces the biases in datasets with rare events. This pulls parameter estimates away from infinity and large numbers.\n\n**Firth regression** produced refined finite estimates even if there is perfect prediction between predictors that perfectly separate stroke vs no stroke cases.\n\nFinally, **Firth Regression** produces results similar to large sample sizes.\n\n**Probability calibration:** Firth regression, while correcting bias, tends to bias predicted event probabilities (average predicted toward 0.5).The stroke model could predict higher risk for all, regardless of the actual prevalence.\n\n**FLIC (Firth’s logistic regression with intercept correction)** adjusts the intercept after fitting the model so that the average predicted probability exactly matches the observed rate in your data (5.6% in this case). This is especially useful if your sample prevalence intentionally differs from the “true” population prevalence, as in case-control studies or enriched samples.\n\nHence there 3 models, base, Firth, and Flic Logistic Regression models.\n\n## 7. Analyzing the 3 models\n\nThe Three different Models of Logistic Regression: Baseline Firth and Flic Correction. We are creating 3 different models to really test to see if the dataset had a stroke percentage that is less than the real percentage of stroke to population ratio in the US. Because this is a so called “rare event” Firth regression takes this into account. as does its refinement FLIC.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Baseline Logistic Regression\nmodel_base <- glm(formula, data=strokeclean, family=binomial)\nprob_base <- predict(model_base, type=\"response\")\n\n# Firth Logistic Regression\nmodel_firth <- logistf(formula, data=strokeclean)\nprob_firth <- predict(model_firth, type=\"response\")\n\n# FLIC Correction (this correction changes the intercept)\nmodel_flic <- flic(formula, data=strokeclean)\nprob_flic <- predict(model_flic, type=\"response\")\n\nlabels <- strokeclean$stroke\n```\n:::\n\n\nCreating Youdens J. Youden’s J is a good way to look at how well each model balances sensitivity and selectivity. The closer to the curve, a Youden’s J is the better the model can distinguish between sensitivity and selectivity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyouden_point <- function(roc_obj) {\n  coords <- coords(roc_obj, \"best\", best.method = \"youden\", ret=c(\"threshold\", \"sensitivity\", \"specificity\", \"youden\"))\n  return(coords)\n}\n```\n:::\n\n\nResults:\n\nPlot the ROC curves and Annotate Youden’s J on each of the Curve\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_base <- factor(ifelse(prob_base > 0.5, 1, 0), levels=c(0,1))\npred_firth <- factor(ifelse(prob_firth > 0.5, 1, 0), levels=c(0,1))\npred_flic <- factor(ifelse(prob_flic > 0.5, 1, 0), levels=c(0,1))\n\nmetrics <- function(pred, prob, labels, name) {\n  cm <- confusionMatrix(pred, labels, positive = \"1\")\n  roc_obj <- roc(labels, as.numeric(prob))\n  auc_val <- auc(roc_obj)\n  precision <- cm$byClass[\"Pos Pred Value\"]\n  recall <- cm$byClass[\"Sensitivity\"]\n  f1 <- 2 * ((precision * recall) / (precision + recall))\n  youden <- youden_point(roc_obj)\n  # All list arguments separated by commas only, no '+'\n  list(\n    confusion = cm$table,\n    precision = precision,\n    recall = recall,\n    f1 = f1,\n    auc = auc_val,\n    roc_obj = roc_obj,\n    youden = youden,\n    model = name\n  )\n}\n```\n:::\n\n\nIntialize Results. We have to initialize results before calling the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults_base <- metrics(pred_base, prob_base, labels, \"Baseline LR\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting levels: control = 0, case = 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nresults_firth <- metrics(pred_firth, prob_firth, labels, \"firth LR\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting levels: control = 0, case = 1\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nresults_flic <- metrics(pred_flic, prob_flic, labels, \"flic LR\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting levels: control = 0, case = 1\nSetting direction: controls < cases\n```\n\n\n:::\n:::\n\n\nPrint Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"\\n== Baseline Logistic Regression ==\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Baseline Logistic Regression ==\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(results_base[1:6])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$confusion\n          Reference\nPrediction    0    1\n         0 3177  178\n         1    0    2\n\n$precision\nPos Pred Value \n             1 \n\n$recall\nSensitivity \n 0.01111111 \n\n$f1\nPos Pred Value \n    0.02197802 \n\n$auc\nArea under the curve: 0.8285\n\n$roc_obj\n\nCall:\nroc.default(response = labels, predictor = as.numeric(prob))\n\nData: as.numeric(prob) in 3177 controls (labels 0) < 180 cases (labels 1).\nArea under the curve: 0.8285\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nYouden's J (optimal threshold):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nYouden's J (optimal threshold):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(results_base$youden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   threshold sensitivity specificity   youden\n1 0.06934436   0.7444444   0.7777778 1.522222\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n== Firth Logistic Regression ==\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== Firth Logistic Regression ==\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(results_firth[1:6])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$confusion\n          Reference\nPrediction    0    1\n         0 3176  178\n         1    1    2\n\n$precision\nPos Pred Value \n     0.6666667 \n\n$recall\nSensitivity \n 0.01111111 \n\n$f1\nPos Pred Value \n    0.02185792 \n\n$auc\nArea under the curve: 0.8285\n\n$roc_obj\n\nCall:\nroc.default(response = labels, predictor = as.numeric(prob))\n\nData: as.numeric(prob) in 3177 controls (labels 0) < 180 cases (labels 1).\nArea under the curve: 0.8285\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nYouden's J (optimal threshold):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nYouden's J (optimal threshold):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(results_firth$youden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   threshold sensitivity specificity   youden\n1 0.07100345   0.7444444   0.7777778 1.522222\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\n== FLIC Logistic Regression ==\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n== FLIC Logistic Regression ==\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(results_flic[1:6])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$confusion\n          Reference\nPrediction    0    1\n         0 3177  178\n         1    0    2\n\n$precision\nPos Pred Value \n             1 \n\n$recall\nSensitivity \n 0.01111111 \n\n$f1\nPos Pred Value \n    0.02197802 \n\n$auc\nArea under the curve: 0.8285\n\n$roc_obj\n\nCall:\nroc.default(response = labels, predictor = as.numeric(prob))\n\nData: as.numeric(prob) in 3177 controls (labels 0) < 180 cases (labels 1).\nArea under the curve: 0.8285\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nYouden's J (optimal threshold):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nYouden's J (optimal threshold):\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(results_flic$youden)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   threshold sensitivity specificity   youden\n1 0.06935441   0.7444444   0.7777778 1.522222\n```\n\n\n:::\n:::\n\n\nPlot the ROC curves and Annotate Youden's J on each of the Curves\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(results_base$roc_obj, col=\"cyan\", main=\"ROC Curves: Baseline (blue) vs Firth (red)\")\nplot(results_firth$roc_obj, col=\"magenta\", add=TRUE)\nplot(results_flic$roc_obj, col =\"gold\", add=TRUE)\nauc(results_base$roc_obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea under the curve: 0.8285\n```\n\n\n:::\n\n```{.r .cell-code}\nauc(results_firth$roc_obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea under the curve: 0.8285\n```\n\n\n:::\n\n```{.r .cell-code}\nauc(results_flic$roc_obj)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea under the curve: 0.8285\n```\n\n\n:::\n\n```{.r .cell-code}\npoints(\n  1-results_base$youden[\"specificity\"],\n  results_base$youden[\"sensitivity\"],\n  col=\"cyan\", pch=19, cex=1.5\n)\npoints(\n  1-results_firth$youden[\"specificity\"],\n  results_firth$youden[\"sensitivity\"],\n  col=\"magenta\", pch=19, cex=1.5\n)\npoints(\n  1-results_flic$youden[\"specificity\"],\n  results_flic$youden[\"sensitivity\"],\n  col=\"gold\", pch=19, cex=1.5\n)\n\nlegend(\"bottomright\", legend=c(\"Baseline\", \"Firth\",\"flic\"), col=c(\"cyan\", \"magenta\", \"gold\"), lwd=2)\n\ntext(\n  x=1-results_base$youden[\"specificity\"], y=results_base$youden[\"sensitivity\"],\n  labels=paste0(\"Youden: \", round(results_base$youden[\"youden\"], 3)),\n  pos=4, col=\"cyan\"\n)\ntext(\n  x=1-results_firth$youden[\"specificity\"], y=results_firth$youden[\"sensitivity\"],\n  labels=paste0(\"Youden: \", round(results_firth$youden[\"youden\"], 3)),\n  pos=4, col=\"magenta\"\n)\ntext(\n  x=1-results_flic$youden[\"specificity\"], y=results_flic$youden[\"sensitivity\"],\n  labels=paste0(\"Youden: \", round(results_flic$youden[\"youden\"], 3)),\n  pos=4, col=\"gold\"\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nHere we see the results. Note that overlaying the curves and Youden’s J is EXACTLY the same for all three models. This is a strong indication that the dataset is currently balanced enough to distinguish between stroke and non stroke. **The bias, if any, would have shown up in a different AUC curve, and a different Youden’s J. It does not.**\n\nPlot the Confusion Matrices\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(3, 1), mar = c(6, 5, 6, 2))  # more top margin for all\nfourfoldplot(results_base$confusion, color = c(\"lightskyblue\", \"plum2\"),\n             conf.level = 0, margin = 1, main = \"Baseline Confusion Matrix\")\nfourfoldplot(results_firth$confusion, color = c(\"lightskyblue\", \"plum2\"),\n             conf.level = 0, margin = 1, main = \"Firth Confusion Matrix\")\nfourfoldplot(results_flic$confusion, color = c(\"lightskyblue\", \"plum2\"),\n             conf.level = 0, margin = 1, main = \"Flic Confusion Matrix\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1,1), mar = c(5, 4, 4, 2)) # Reset to default after\n```\n:::\n\n\n## 8. Conclusion\n\nThe results indicate that the confusion matrices are the same. So the conclusion we can reach is there was no significant bias in the dataset. The dataset can distinguish between stroke and non stroke events with sufficient selectivity.\n\n\n\n\n\n### References\n\n::: {#refs}\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}