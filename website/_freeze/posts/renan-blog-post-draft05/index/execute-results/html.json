{
  "hash": "7f95882d3eff5541fc3a8b8de8311196",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Draft Final Report - v05\"\ndescription: \"Change Format to fit with the samples and reviewed equations\"\nauthor:\n  - name: Renan Monteiro Barbosa\n    url: https://github.com/renanmb\n    affiliation: Master of Data Science Program @ The University of West Florida (UWF)\n    # affiliation-url: https://ucsb-meds.github.io/\n# date: 10-24-2022\ncategories: [draft, renan]\n# citation:\n#   url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/\nimage: images/spongebob-imagination.jpg\ndraft: false\nbibliography: references.bib\nlink-citations: true\n---\n\n## 1. Introduction\n\nStroke is one of the leading causes of death and disability worldwide and remains a major public health challenge [@WHO2025]. Because stroke often occurs suddenly and can result in long-term neurological impairment, early identification of individuals at elevated risk is critical for prevention and timely intervention. Data-driven risk prediction models enable clinicians and public health professionals to quantify individual-level risk and to target high-risk groups for lifestyle counselling and clinical management.\n\nLogistic Regression (LR) is one of the most widely used approaches for modelling binary outcomes such as disease presence or absence [@sperandei2014understanding]. It extends linear regression to cases where the outcome is categorical and provides interpretable coefficients and odds ratios that describe how each predictor is associated with the probability of the event. LR has been applied across a wide range of domains, including child undernutrition and anaemia [@asmare2024determinants], road traffic safety [@rahman2021identification; @chen2024binary; @chen2020modeling], health-care utilisation and clinical admission decisions [@hutchinson2023predictors], and fraud detection [@samara2024using]. These applications highlight both the flexibility of LR and its suitability for real-world decision-making problems.\n\nIn this project, we analyse a publicly available stroke dataset that includes key demographic, behavioural, and clinical predictors such as age, gender, hypertension status, heart disease, marital status, work type, residence type, smoking status, body mass index (BMI), and average glucose level. These variables are commonly reported in the stroke and cardiovascular literature as important determinants of risk. Using this dataset, we first clean and recode the variables into appropriate numeric formats and then develop a series of supervised learning models for stroke prediction.\n\nLogistic Regression is used as the primary, interpretable baseline model, but its performance is compared against several more complex machine-learning techniques, including Decision Tree, Random Forest, Gradient Boosted Machine, k-Nearest Neighbours, and Support Vector Machine (radial). Model performance is evaluated using accuracy, sensitivity, specificity, ROC curves, AUC, and confusion matrices. The main objectives are to identify the most influential predictors of stroke and to determine whether advanced machine-learning models offer meaningful improvements over Logistic Regression for classification of stroke risk in this dataset.\n\n<!-- \n⣿⣿⡇⢩⠘⣴⣿⣥⣤⢦⢁⠄⠉⡄⡇⠛⠛⠛⢛⣭⣾⣿⣿⡏\n⣿⣿⣿⡇⠹⢇⡹⣿⣿⣛⣓⣿⡿⠞⠑⣱⠄⢀⣴⣿⣿⣿⣿⡟\n⣿⣿⣿⣧⣸⡄⣿⣪⡻⣿⠿⠋⠄⠄⣀⣀⢡⣿⣿⣿⣿⡿⠋\n⠘⣿⣿⣿⣿⣷⣭⣓⡽⡆⡄⢀⣤⣾⣿⣿⣿⣿⣿⡿⠋\n⠄⢨⡻⡇⣿⢿⣿⣿⣭⡶⣿⣿⣿⣜⢿⡇⡿⠟⠉\n⠄⠸⣷⡅⣫⣾⣿⣿⣿⣷⣙⢿⣿⣿⣷⣦⣚⡀\n⠄⠄⢉⣾⡟⠙⠶⠖⠈⢻⣿⣷⣅⢻⣿⣿⣿⣿⣿⣶⣶⡆⠄⣤⡀\n⠄⢠⣿⣿⣧⣀⣀⣀⣀⣼⣿⣿⣿⡎⢿⣿⣿⣿⣿⣿⣿⣇⠄⠈⠁\n⠄⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢇⣎⢿⣿⣿⣿⣿⣿⣿⣿⣶⣶\n⠄⠄⠻⢿⣿⣿⣿⣿⣿⣿⣿⢟⣫⣾⣿⣷⡹⣿⣿⣿⣿⣿⣿⣿⡟\n⠄⠄⠄⠄⢮⣭⣍⡭⣭⡵⣾⣿⣿⣿⡎⣿⣿⣌⠻⠿⠿⠿⠟⠋\n⠄⠄⠄⠄⠈⠻⣿⣿⣿⣿⣹⣿⣿⣿⡇⣿⣿⡿\n⠄⠄⣀⣴⣾⣶⡞⣿⣿⣿⣿⣿⣿⣿⣾⣿⡿ \n-->\n\n## 2. Methods\n\nThe binary logistic regression model is part of a family of statistical models called generalised linear models. The main characteristic that differentiates binary logistic regression from other generalised linear models is the type of dependent (or outcome) variable. @harris2019statistics A dependent variable in a binary logistic regression has two levels. For example, a variable that records whether or not someone has ever been diagnosed with a health condition like Stroke could be measured in two categories, yes and no. Likewise, someone might have coronary heart disease or not, be physically active or not, be a current smoker or not, or have any one of thousands of diagnoses or personal behaviours and characteristics that are of interest in family medicine.\n\nThe binary logistic regression algorithm below:\n\n$$ln\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_{0} + \\beta_{1}x_{1} + \\cdots + \\beta_{k}x_{k}$$\n\nWhere $\\pi = P[Y =1]$ is the probability of the outcome.\n\n\n- Logistic Regression\n- Decision Tree\n- Random Forest\n- Gradient Boosted Machine\n- k-Nearest Neighbors\n- Support Vector Machine\n\n### Assumptions\n\nBinary logistic regression relies on the following underlying assumptions to be true:\n\n* The observations must be independent.\n* There must be no perfect multicollinearity among independent variables.\n* Logistic regression assumes linearity of independent variables and log odds.\n* There are no extreme outliers\n* The Sample Size is Sufficiently Large. Field recommends a minimum of 50 cases. @field2024discovering Hosmer, Lemeshow, and Sturdivant @hosmer2013applied suggest a minimum sample of 10 observations per independent variable in the model. Leblanc and Fitzgerald (2000) @leblanc2000logistic suggest a minimum of 30 observations per independent variable.\n\n\n\n<!-- \n⣿⣿⡇⢩⠘⣴⣿⣥⣤⢦⢁⠄⠉⡄⡇⠛⠛⠛⢛⣭⣾⣿⣿⡏\n⣿⣿⣿⡇⠹⢇⡹⣿⣿⣛⣓⣿⡿⠞⠑⣱⠄⢀⣴⣿⣿⣿⣿⡟\n⣿⣿⣿⣧⣸⡄⣿⣪⡻⣿⠿⠋⠄⠄⣀⣀⢡⣿⣿⣿⣿⡿⠋\n⠘⣿⣿⣿⣿⣷⣭⣓⡽⡆⡄⢀⣤⣾⣿⣿⣿⣿⣿⡿⠋\n⠄⢨⡻⡇⣿⢿⣿⣿⣭⡶⣿⣿⣿⣜⢿⡇⡿⠟⠉\n⠄⠸⣷⡅⣫⣾⣿⣿⣿⣷⣙⢿⣿⣿⣷⣦⣚⡀\n⠄⠄⢉⣾⡟⠙⠶⠖⠈⢻⣿⣷⣅⢻⣿⣿⣿⣿⣿⣶⣶⡆⠄⣤⡀\n⠄⢠⣿⣿⣧⣀⣀⣀⣀⣼⣿⣿⣿⡎⢿⣿⣿⣿⣿⣿⣿⣇⠄⠈⠁\n⠄⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢇⣎⢿⣿⣿⣿⣿⣿⣿⣿⣶⣶\n⠄⠄⠻⢿⣿⣿⣿⣿⣿⣿⣿⢟⣫⣾⣿⣷⡹⣿⣿⣿⣿⣿⣿⣿⡟\n⠄⠄⠄⠄⢮⣭⣍⡭⣭⡵⣾⣿⣿⣿⡎⣿⣿⣌⠻⠿⠿⠿⠟⠋\n⠄⠄⠄⠄⠈⠻⣿⣿⣿⣿⣹⣿⣿⣿⡇⣿⣿⡿\n⠄⠄⣀⣴⣾⣶⡞⣿⣿⣿⣿⣿⣿⣿⣾⣿⡿ \n-->\n\n## 3. Analysis and Results\n\nImport all the dependencies:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npackages <- c(\"dplyr\", \"car\", \"ResourceSelection\", \"caret\", \"pROC\",  \"logistf\", \"Hmisc\", \"rcompanion\", \"ggplot2\", \"summarytools\", \"tidyverse\", \"knitr\", \"ggpubr\", \"ggcorrplot\", \"randomForest\", \"gbm\", \"kernlab\", \"skimr\")\n# Load Libraries\nlapply(packages, library, character.only = TRUE)\n# Set seed for reproducibility\nset.seed(123)\n```\n:::\n\n\n<!-- Maybe add a list describing the packages used with the citations attached -->\n\n### 3.1. Data Ingestion\n\nData source: [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset) @kaggle01\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfind_git_root <- function(start = getwd()) {\n  path <- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path <- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root <- find_git_root()\ndatasets_path <- file.path(repo_root, \"datasets\")\n\n# Reading the datafile healthcare-dataset-stroke-data\nstroke_path <- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\nstroke1 = read_csv(stroke_path, show_col_types = FALSE)\n```\n:::\n\n\n### 3.2. Exploratory Data Analysis (EDA)\n\n**Dataset Description**\n\nThe **Stroke Prediction Dataset** @kaggle01 is a publically available dataset for educational purposes containing 5,110 observations containing predictors commonly associated with cerebrovascular risk. The dataset is composed of 11 clinical and demographic features and 1 feature which is **id** a unique identifier for the patient. The dataset has features including patient's **age**, **gender**, presence of conditions like **hypertension** and **heart disease**, **work type**, **residence type**, **average glucose level**, and **BMI**. This dataset is primarily intended for educational purposes as it shares a lot of similarities with the Jackson Heart Study (JHS) dataset but it is not as descriptive.\n\n| Feature Name       | Description                                              | Data Type          | Key Values/Range                                           |\n|--------------------|----------------------------------------------------------|--------------------|------------------------------------------------------------|\n| id                 | Unique identifier for the patient                        | Numeric            | Unique numeric ID                                          |\n| gender             | Patient's gender                                         | Character          | Male, Female, Other                                        |\n| age                | Patient's age in years                                   | Numeric            | 0.08 to 82                                                 |\n| hypertension       | Indicates if the patient has hypertension                | Numeric (binary)   | 0 (No), 1 (Yes)                                            |\n| heart_disease      | Indicates if the patient has any heart diseases          | Numeric (binary)   | 0 (No), 1 (Yes)                                            |\n| ever_married       | Whether the patient has ever been married                | Character          | No, Yes                                                    |\n| work_type          | Type of occupation                                       | Character          | Private, Self-employed, Govt_job, children, Never_worked   |\n| Residence_type     | Patient's area of residence                              | Character          | Rural, Urban                                               |\n| avg_glucose_level  | Average glucose level in blood                           | Numeric            | ≈55.12 to 271.74                                           |\n| bmi                | Body Mass Index                                          | Character          | ≈10.3 to 97.6 (has NA values)                              |\n| smoking_status     | Patient's smoking status                                 | Character          | formerly smoked, never smoked, smokes, Unknown             |\n| stroke             | Target Variable: Whether the patient had a stroke        | Numeric (binary)   | 0 (No Stroke), 1 (Stroke)                                  |\n\n<!-- TODO add a footnote for table -->\n\n<!-- \n⠄⠄⣿⣿⣿⣿⠘⡿⢛⣿⣿⣿⣿⣿⣧⢻⣿⣿⠃⠸⣿⣿⣿⠄⠄⠄⠄⠄\n⠄⠄⣿⣿⣿⣿⢀⠼⣛⣛⣭⢭⣟⣛⣛⣛⠿⠿⢆⡠⢿⣿⣿⠄⠄⠄⠄⠄\n⠄⠄⠸⣿⣿⢣⢶⣟⣿⣖⣿⣷⣻⣮⡿⣽⣿⣻⣖⣶⣤⣭⡉⠄⠄⠄⠄⠄\n⠄⠄⠄⢹⠣⣛⣣⣭⣭⣭⣁⡛⠻⢽⣿⣿⣿⣿⢻⣿⣿⣿⣽⡧⡄⠄⠄⠄\n⠄⠄⠄⠄⣼⣿⣿⣿⣿⣿⣿⣿⣿⣶⣌⡛⢿⣽⢘⣿⣷⣿⡻⠏⣛⣀⠄⠄\n⠄⠄⠄⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⠙⡅⣿⠚⣡⣴⣿⣿⣿⡆⠄\n⠄⠄⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠄⣱⣾⣿⣿⣿⣿⣿⣿⠄\n⠄⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿APPROVED⣿⣿⣿⣿⣿⠄\n⠄⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠣⣿⣿⣿⣿⣿⣿⣿⣿⣿⠄\n⠄⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠛⠑⣿⣮⣝⣛⠿⠿⣿⣿⣿⣿⠄\n⢠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⠄⠄⠄⠄⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠄\n-->\n\n#### **3.2.1 Dataset Preprocessing**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Handle dataset features\nstroke1[stroke1 == \"N/A\" | stroke1 == \"Unknown\" | stroke1 == \"children\" | stroke1 == \"other\"] <- NA\nstroke1$bmi <- round(as.numeric(stroke1$bmi), 2)\nstroke1$gender[stroke1$gender == \"Male\"] <- 1\nstroke1$gender[stroke1$gender == \"Female\"] <- 0\nstroke1$gender <- as.numeric(stroke1$gender)\nstroke1$ever_married[stroke1$ever_married == \"Yes\"] <- 1\nstroke1$ever_married[stroke1$ever_married == \"No\"] <- 0\nstroke1$ever_married <- as.numeric(stroke1$ever_married)\nstroke1$work_type[stroke1$work_type == \"Govt_job\"] <- 1\nstroke1$work_type[stroke1$work_type == \"Private\"] <- 2\nstroke1$work_type[stroke1$work_type == \"Self-employed\"] <- 3\nstroke1$work_type[stroke1$work_type == \"Never_worked\"] <- 4\nstroke1$work_type <- as.numeric(stroke1$work_type)\nstroke1$Residence_type[stroke1$Residence_type == \"Urban\"] <- 1\nstroke1$Residence_type[stroke1$Residence_type == \"Rural\"] <- 2\nstroke1$Residence_type <- as.numeric(stroke1$Residence_type)\nstroke1$avg_glucose_level <- as.numeric(stroke1$avg_glucose_level)\nstroke1$heart_disease <- as.numeric(stroke1$heart_disease)\nstroke1$hypertension <- as.numeric(stroke1$hypertension)\nstroke1$age <- round(as.numeric(stroke1$age), 2)\nstroke1$stroke <- as.numeric(stroke1$stroke)\nstroke1$smoking_status[stroke1$smoking_status == \"never smoked\"] <- 1\nstroke1$smoking_status[stroke1$smoking_status == \"formerly smoked\"] <- 2\nstroke1$smoking_status[stroke1$smoking_status == \"smokes\"] <- 3\nstroke1$smoking_status <- as.numeric(stroke1$smoking_status)\nstroke1 <- stroke1[, !(names(stroke1) %in% \"id\")]\n\n# Remove NAs and clean dataset\nstroke1$stroke <- as.factor(stroke1$stroke)\nstroke1_clean <- na.omit(stroke1)\nstrokeclean <- stroke1_clean\nfourassume <- stroke1_clean\n\nstrokeclean$stroke <- factor(\n  strokeclean$stroke,\n  levels = c(\"0\", \"1\"),\n  labels = c(\"No\", \"Yes\")\n)\n\nfourassume$stroke <- factor(\n  fourassume$stroke,\n  levels = c(\"0\", \"1\"),\n  labels = c(\"No\", \"Yes\")\n)\n```\n:::\n\n\nThe initial exploration demonstrated that the **Stroke Prediction Dataset** @kaggle01 has several issues requiring changes for handling missing values, converting character (categorical) features into numerical codes, and removing the identifier column.\n\n<!-- TODO -->\nSo as part of data preprocessing we will be focused on establishing consistency and ensuring all variables are in a format suitable for predictive modeling. This process starts by systematically addressing non-standard representations of missing data. Specifically, all instances of the string values \"N/A\", \"Unknown\", \"children\", and \"other\" found across the dataset were unified and replaced with the standard statistical missing value representation, NA.\n\nThen we proceed with converting several character-based (categorical) features into numerical features, which is necessary for predictive modeling.\n\n<!-- TODO -->\n\nThe feature **bmi**, initially read as a character variable was first converted to a numeric data type and subsequently rounded to two decimal places.\n\nThe binary categorical features were encoded into numerical indicators. The feature **gender** was transformed so that **\"Male\"** was encoded to 1 and **\"Female\"** was encoded to 0, and the **ever_married** was transformed so that **\"Yes\"** encoded to 1 and **\"No\"** encoded to 0.\n\nFeatures with multiple categories were also numerically encoded into numerical indicators. The **work_type** feature had its categories encoded so that **\"Govt_job\"** = 1, **\"Private\"** = 2, **\"Self-employed\"** = 3, and **\"Never_worked\"** = 4. The **Residence_type** was encoded so that **\"Urban\"** = 1 and **\"Rural\"** = 2. Finally, the **smoking_status** feature was encoded into three numerical levels, those being **\"never smoked\"** = 1, **\"formerly smoked\"** = 2, and **\"smokes\"** = 3.\n\nAdditionally, the continuous numerical variables **avg_glucose_level**, **heart_disease**, and **hypertension** were explicitly confirmed as numeric data types, with the **age** feature also being rounded to two decimal places for consistency.\n\n<!-- TODO -->\nThe final stage of preprocessing involved removing the **id** column, which served only as a unique identifier and held no predictive value. This action left the dataset with 11 core predictors. The target variable, **stroke**, was then converted into a factor (a categorical data type in R) named **stroke1**, and its levels were explicitly labeled as $\\text{\"No\"} = 0$ and $\\text{\"Yes\"} = 1$. The entire process concluded with the removal of all remaining observations containing missing or inconsistent entries, resulting in the creation of the final, clean data frames, **strokeclean** and **fourassume**.\n\n\n**Dataset Preprocessing Conclusion**\n\nThe **Stroke Prediction Dataset** @kaggle01 that started containing 5,110 observations and 12 features. After cleaning missing and inconsistent entries among other necessarychanges, ended as a dataset containing 3,357 observations and 11 predictors commonly associated with cerebrovascular risk. Those key predictors are listed below.\n\n<!-- TODO modify this table to add a better description and be more visually engaging -->\n\n| Feature Name      | Description                                      | Data Type | Values                                                               |\n|-------------------|--------------------------------------------------|-----------|----------------------------------------------------------------------|\n| gender            | Patient's gender                                 | Numeric   | 1 (Male), 0 (Female)                                                 |\n| age               | Patient's age in years                           | Numeric   | Range 0.08 to 82; rounded to 2 decimal places                        |\n| hypertension      | Indicates if the patient has hypertension        | Numeric   | 0 (No), 1 (Yes)                                                      |\n| heart_disease     | Indicates if the patient has any heart diseases  | Numeric   | 0 (No), 1 (Yes)                                                      |\n| ever_married      | Whether the patient has ever been married        | Numeric   | 1 (Yes), 0 (No)                                                      |\n| work_type         | Type of occupation                               | Numeric   | 1 (Govt_job), 2 (Private), 3 (Self-employed), 4 (Never_worked)       |\n| Residence_type    | Patient's area of residence                      | Numeric   | 1 (Urban), 2 (Rural)                                                 |\n| avg_glucose_level | Average glucose level in blood                   | Numeric   | Range ≈55.12 to 271.74                                               |\n| bmi               | Body Mass Index                                  | Numeric   | Range ≈10.3 to 97.6; converted from character, rounded to 2 decimals |\n| smoking_status    | Patient's smoking status                         | Numeric   | 1 (never smoked), 2 (formerly smoked), 3 (smokes)                    |\n| stroke            | Target Variable: Whether the patient had stroke  | Numeric   | 0 (No Stroke), 1 (Stroke)                                            |\n\n<!-- TODO add table footnote -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# skim(stroke1)\n# nrow(fourassume)\nclass(strokeclean$stroke)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"factor\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# unique(stroke1$stroke)\n```\n:::\n\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n#### 3.2.2 Dataset Visualization\n\nBefore developing predictive models, an exploratory analysis was conducted to understand the distribution, structure, and relationships within the cleaned dataset (N = 3,357). This step is crucial in rare-event medical modeling because data imbalance, skewed predictors, or correlated variables can directly influence model behavior and classification performance.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# (a) Histogram of gender\np1a <- ggplot(strokeclean, aes(x = gender, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(a) Gender\", x = \"gender\", y = \"Count\")\n\n# (b) Histogram of Age\np1b <- ggplot(strokeclean, aes(x = age, fill = stroke)) +\n  geom_histogram(binwidth = 5, position = \"identity\", alpha = 0.7) +\n  # stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(b) Age\", x = \"Age\", y = \"Frequency\")\n\n# (c) Histogram of hypertension\np1c <- ggplot(strokeclean, aes(x = hypertension, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(c) Hypertension\", x = \"hypertension\", y = \"Frequency\")\n\n# (d) Histogram of heart_disease\np1d <- ggplot(strokeclean, aes(x = heart_disease, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(d) Heart Disease\", x = \"HeartDisease\", y = \"Frequency\")\n\n# (e) Histogram of ever_married\np1e <- ggplot(strokeclean, aes(x = ever_married, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(e) Ever Married\", x = \"EverMarried\", y = \"Frequency\")\n\n# (f) Histogram of work_type\np1f <- ggplot(strokeclean, aes(y = work_type, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(f) Work Type\", y = \"WorkType\", x = \"Frequency\")\n\n# (g) Histogram of Residence_type\np1g <- ggplot(strokeclean, aes(x = Residence_type, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(g) Residence Type\", x = \"Residence_type\", y = \"Frequency\")\n\n# (h) Histogram of avg_gloucose_level\np1h <- ggplot(strokeclean, aes(x = avg_glucose_level, fill = stroke)) +\n  geom_histogram(binwidth = 10, position = \"identity\", alpha = 0.7) +\n  # stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(h) Avg. Glucose Level\", x = \"Glucose Level\", y = \"Frequency\")\n\n# (i) Histogram of bmi\np1i <- ggplot(strokeclean, aes(x = bmi, fill = stroke)) +\n  geom_histogram(binwidth = 2, position = \"identity\", alpha = 0.7) +\n  labs(title = \"(i) BMI\", x = \"BMI\", y = \"Frequency\")\n\n# (j) smoking_status\np1j <- ggplot(strokeclean, aes(y = smoking_status, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(j) Smoking Status\", y = \"smoking_status\", x = \"Frequency\")\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4 plots per Figure\n# p1a, p1b, p1c, p1d\nggarrange(p1a, p1b, p1c, p1d, \n          ncol = 2, nrow = 2, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# p1e p1f p1g p1h\nggarrange(p1e, p1f, p1g, p1h,\n          ncol = 2, nrow = 2, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# p1i p1j\nggarrange(p1i, p1j,\n          ncol = 2, nrow = 2, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n### 3.3. Statistical Modelling\n\nWe are splitting the dataset intro trainning and testing data.\n\n<!-- TODO ----- Data splitting for logistic regression involves partitioning your dataset into training and testing sets to build and validate the model's probability of success predictions -->\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_df <- strokeclean\nmodel_df <- na.omit(model_df)\nmodel_df$stroke <- factor(model_df$stroke)\nlevels(model_df$stroke) <- c(\"No\", \"Yes\")\ntable(model_df$stroke)\n\nindex <- createDataPartition(strokeclean$stroke, p = 0.70, list = FALSE)\ntrain_data <- strokeclean[index, ]\ntest_data  <- strokeclean[-index, ]\n\ntrain_data$stroke <- factor(train_data$stroke, levels = c(\"No\",\"Yes\"))\ntest_data$stroke  <- factor(test_data$stroke,  levels = c(\"No\",\"Yes\"))\n```\n:::\n\n\n- Logistic Regression\n- Decision Tree\n- Random Forest\n- Gradient Boosted Machine\n- k-Nearest Neighbors\n- Support Vector Machine\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# skim(stroke1)\nclass(stroke1$stroke)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"factor\"\n```\n\n\n:::\n\n```{.r .cell-code}\nunique(stroke1$stroke)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 0\nLevels: 0 1\n```\n\n\n:::\n:::\n\n\n#### Repeated K-fold cross-validation.\n\nThe trainControl() function in the R caret package is used to control the computational nuances and resampling methods employed by the train() function. It allows us to implement Repeated K-fold cross-validation (\"repeatedcv\").\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- trainControl(\nmethod = \"repeatedcv\",\nnumber = 5,\nrepeats = 3,\nclassProbs = TRUE,\nsummaryFunction = twoClassSummary,\nverboseIter = FALSE\n)\n```\n:::\n\n\n#### Logistic Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_lr <- glm(\n  stroke ~ . , \n  data=train_data , \n  family = \"binomial\" (link=logit)\n  )\ns1 <- summary(model_lr)\nc1 <- coefficients(model_lr)\nanova1 <- car::Anova(model_lr, type = 3)\nconfint1 <- confint(model_lr, level=0.95)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n:::\n\n\n<!-- Add Assumption testing here -->\n\n* The observations must be independent.\n* There must be no perfect multicollinearity among independent variables. Use the VIF.\n* Logistic regression assumes linearity of independent variables and log odds.\n* There are no extreme outliers, check using Cooks D\n* The Sample Size is Sufficiently Large. \n\n**Check Multicollinearity**\n\nIn OLS regression, multicollinearity can be calculated either from the correlations among the predictors, or from the correlations among the coefficient estimates, and these result in the same variance inflaction factors (VIFs).\n\nIn GLMs, these two approaches yield similar but different VIFs. John Fox, one of the authors of the car package where the vif() function is found, opts for calculating the VIFs from the coefficient estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(model_lr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           gender               age      hypertension     heart_disease \n         1.031955          1.184980          1.036113          1.065005 \n     ever_married         work_type    Residence_type avg_glucose_level \n         1.022551          1.056785          1.013043          1.114666 \n              bmi    smoking_status \n         1.150445          1.047057 \n```\n\n\n:::\n:::\n\n\n**Check Outliers**\n\n\n\n<!-- \n⣿⣿⡇⢩⠘⣴⣿⣥⣤⢦⢁⠄⠉⡄⡇⠛⠛⠛⢛⣭⣾⣿⣿⡏\n⣿⣿⣿⡇⠹⢇⡹⣿⣿⣛⣓⣿⡿⠞⠑⣱⠄⢀⣴⣿⣿⣿⣿⡟\n⣿⣿⣿⣧⣸⡄⣿⣪⡻⣿⠿⠋⠄⠄⣀⣀⢡⣿⣿⣿⣿⡿⠋\n⠘⣿⣿⣿⣿⣷⣭⣓⡽⡆⡄⢀⣤⣾⣿⣿⣿⣿⣿⡿⠋\n⠄⢨⡻⡇⣿⢿⣿⣿⣭⡶⣿⣿⣿⣜⢿⡇⡿⠟⠉\n⠄⠸⣷⡅⣫⣾⣿⣿⣿⣷⣙⢿⣿⣿⣷⣦⣚⡀\n⠄⠄⢉⣾⡟⠙⠶⠖⠈⢻⣿⣷⣅⢻⣿⣿⣿⣿⣿⣶⣶⡆⠄⣤⡀\n⠄⢠⣿⣿⣧⣀⣀⣀⣀⣼⣿⣿⣿⡎⢿⣿⣿⣿⣿⣿⣿⣇⠄⠈⠁\n⠄⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢇⣎⢿⣿⣿⣿⣿⣿⣿⣿⣶⣶\n⠄⠄⠻⢿⣿⣿⣿⣿⣿⣿⣿⢟⣫⣾⣿⣷⡹⣿⣿⣿⣿⣿⣿⣿⡟\n⠄⠄⠄⠄⢮⣭⣍⡭⣭⡵⣾⣿⣿⣿⡎⣿⣿⣌⠻⠿⠿⠿⠟⠋\n⠄⠄⠄⠄⠈⠻⣿⣿⣿⣿⣹⣿⣿⣿⡇⣿⣿⡿\n⠄⠄⣀⣴⣾⣶⡞⣿⣿⣿⣿⣿⣿⣿⣾⣿⡿ \n-->\n\n## 4. Conclusion\n\nThis experiment compared a conventional logistic regression model with several machine-learning algorithms and examined whether common demographic, behavioral, and clinical characteristics may be used to predict stroke risk using a stroke dataset. Stroke was a rare outcome (about 5% of cases) in the final sample of 3,357 people that was analyzed after the data was cleaned and inconsistent or missing values were eliminated. In addition to reflecting actual epidemiology, this significant class disparity complicates classification, particularly when it comes to identifying the minority (stroke) class.\n\nAge, hypertension, cardiac disease, and raised average glucose levels are among the best predictors of stroke, according to the baseline logistic regression model. Smoking status substantially increased risk.  These variables were identified as significant risk factors by odds ratios significantly greater than 1 and confidence intervals that did not cross 1.  These results support the use of logistic regression as an interpretable tool for comprehending the relationship between particular risk variables and the likelihood of stroke and are in line with the clinical literature on cerebrovascular illness.\n\nThe logistic regression model performed reasonably well overall in terms of prediction; however, sensitivity for stroke cases was more constrained at the default 0.5 probability threshold, as would be expected with an imbalanced outcome.  The model clearly outperformed random guessing, according to the ROC curve and AUC values, but there was still space for improvement in terms of differentiating between stroke and non-stroke patients.  Youden's J statistic offers a method for selecting a different categorization threshold that enhances the ratio of sensitivity to specificity, which may be crucial in a screening setting when it is expensive to miss actual stroke cases.\n\nMore sophisticated models, such Random Forest and Gradient Boosted Machine, were able to attain somewhat higher AUC values than logistic regression in the machine-learning comparison, showing superior discrimination across a range of thresholds.  However, these increases in AUC came at the expense of decreased interpretability and were not always accompanied by significant increases in sensitivity at fixed cut-offs.  Logistic regression, on the other hand, offers precise odds ratios and confidence intervals that are simpler for public health professionals and doctors to understand when discussing risk and developing interventions.\n\nBecause of the severe class imbalance, sensitivity for stroke cases was extremely low (around 2%), meaning that the model almost never predicted “stroke = Yes” and therefore missed most true stroke cases.\n\nTo address this, the decision threshold was lowered from 0.5 to 0.2. At this cut-off, sensitivity increased from roughly 2% to about 22%, while specificity remained high at around 95%. Overall accuracy dropped slightly to about 91%, but balanced accuracy improved, indicating a more reasonable trade-off between detecting stroke cases and avoiding false positives. This threshold experiment illustrates a key practical point: for rare but serious outcomes such as stroke, it can be preferable to sacrifice some overall accuracy in order to reduce the number of missed high-risk individuals. In this setting, the logistic model is more appropriately viewed as a screening or risk-flagging tool rather than a definitive diagnostic rule.\n\nOverall, the findings show that relatively simple models built from routinely collected health indicators can meaningfully distinguish between individuals with and without stroke, even in the presence of substantial class imbalance. Logistic regression emerges as a strong, interpretable baseline, while tree-based ensemble methods provide incremental performance improvements at the cost of transparency. Future work could focus on external validation, calibration assessment, more sophisticated imbalance-handling techniques, and the inclusion of additional clinical or longitudinal information. These extensions would help move from proof-of-concept modelling toward robust, clinically usable tools for stroke risk stratification and targeted prevention.\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n### References\n\n::: {#refs}\n:::\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣒⢋⡤⣹⣯⣴⣤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡔⠉⣽⣿⣿⣦⣉⣿⣯⠉⠴⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡸⠀⠆⠿⠻⠻⠟⠿⠿⢟⣶⡄⢹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣣⠏⠀⠀⠀⠀⠀⠀⠀⠀⠘⠇⠘⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡾⠀⠐⢀⠤⠀⠀⠀⠀⠀⠀⠀⡞⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡞⡁⠀⠀⠉⠀⠀⠀⠐⠂⠄⢸⡿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢿⠂⠀⠀⠀⠠⠀⠀⠀⠀⠀⠈⡻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢶⡄⠀⠀⠀⠀⠀⠀⠀⢀⣴⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⡀⠀⠀⠀⠀⠀⠀⢀⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡼⠀⠀⠀⠀⠀⠀⠀⠸⢇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⢀⣀⡀⠴⠙⣆⠀⠀⠀⠀⠀⠀⡠⢢⣿⣷⣤⢀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⣠⡔⠈⣡⣄⡄⠀⠀⠲⠛⠂⠀⠀⠀⠈⠀⢸⣿⣿⣿⣷⣶⣯⣀⣒⡤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⣠⡐⣉⣤⣶⣿⣿⣿⡇⠀⠀⠀⠀⠂⠐⠀⠀⠀⢠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣾⣵⡄⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣛⡀⠀⠀⠠⢄⣀⣐⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⡆⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⡯⠤⠀⢇⣘⡒⠲⢾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣥⠀⠀⠀⠀⠀⠀ -->\n<!-- ⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣟⣓⠀⢰⠤⢭⣽⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡧⠬⠀⢘⣛⣓⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣟⣒⠐⡷⠬⢭⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⠀⠀⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⡁⣛⣓⢲⣿⣿⣿⣿⣿⡏⠉⠛⠛⠛⠻⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡶⢲⠦⢭⣽⣿⣿⣿⣿⣿⣅⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀ -->\n<!-- ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣹⣛⣒⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣧ -->\n<!-- ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠾⠬⣭⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣄⠀⠀⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢼ -->\n<!-- ⠻⣿⣿⣿⣿⠛⠋⢋⠁⠀⠀⠀⣶⢗⣛⣻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣶⣾⣿⣿⡿⣿⣿⣿⣿⣿⣿⣿⣾ -->\n<!-- ⠀⣿⣿⣿⣷⣾⣿⠀⠀⠁⠀⠀⣸⡆⠾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣿⣿⣿⣿⣿⣿⣿⣯ -->\n<!-- ⠀⠈⠻⣿⣿⣿⣿⡆⠀⠤⠄⣰⣿⡇⣭⣿⣿⣿⣿⣿⣿⣿⣿⡿⠻⣿⣿⣿⣿⣿⣿⣿⣿⠿⣿⣿⣿⡿⠟⠋⠁ -->\n<!-- ⠀⠀⠀⠀⠉⠺⢿⣿⣿⣿⣿⣿⣿⣧⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢸⡿⢹⣿⣿⣿⣿⡇⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⡸⢡⣿⣿⣿⣿⣿⠀⢹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⣷⣿⣿⣿⣿⣿⣿⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⢨⣿⣿⣿⣿⣿⣿⣿⣄⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠀ -->\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}