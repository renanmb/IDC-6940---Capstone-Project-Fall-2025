{
  "hash": "36c84c289bd8e51b0229279a5524f7d9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Draft Final Report - v05\"\ndescription: \"Change Format to fit with the samples and reviewed equations\"\nauthor:\n  - name: Renan Monteiro Barbosa\n    url: https://github.com/renanmb\ncategories: [draft, renan]\nimage: images/spongebob-imagination.jpg\ndraft: false\nbibliography: references.bib\nlink-citations: true\n---\n\nThis draft the introduction and methods and part of the analysis was modified from Week 06 with the intent to match other students code. At this point the projects started to diverge drastically.\n\n## 1. Introduction\n\nStroke is one of the leading causes of death and disability worldwide and remains a major public health challenge [@WHO2025]. Because stroke often occurs suddenly and can result in long-term neurological impairment, early identification of individuals at elevated risk is critical for prevention and timely intervention. Data-driven risk prediction models enable clinicians and public health professionals to quantify individual-level risk and to target high-risk groups for lifestyle counselling and clinical management.\n\nLogistic Regression (LR) is one of the most widely used approaches for modelling binary outcomes such as disease presence or absence [@sperandei2014understanding]. It extends linear regression to cases where the outcome is categorical and provides interpretable coefficients and odds ratios that describe how each predictor is associated with the probability of the event. LR has been applied across a wide range of domains, including child undernutrition and anaemia [@asmare2024determinants], road traffic safety [@rahman2021identification; @chen2024binary; @chen2020modeling], health-care utilisation and clinical admission decisions [@hutchinson2023predictors], and fraud detection [@samara2024using]. These applications highlight both the flexibility of LR and its suitability for real-world decision-making problems.\n\nIn this project, we analyse a publicly available stroke dataset that includes key demographic, behavioural, and clinical predictors such as age, gender, hypertension status, heart disease, marital status, work type, residence type, smoking status, body mass index (BMI), and average glucose level. These variables are commonly reported in the stroke and cardiovascular literature as important determinants of risk. Using this dataset, we first clean and recode the variables into appropriate numeric formats and then develop a series of supervised learning models for stroke prediction.\n\nLogistic Regression is used as the primary, interpretable baseline model, but its performance is compared against several more complex machine-learning techniques, including Decision Tree, Random Forest, Gradient Boosted Machine, k-Nearest Neighbours, and Support Vector Machine (radial). Model performance is evaluated using accuracy, sensitivity, specificity, ROC curves, AUC, and confusion matrices. The main objectives are to identify the most influential predictors of stroke and to determine whether advanced machine-learning models offer meaningful improvements over Logistic Regression for classification of stroke risk in this dataset.\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n\n## 2. Methods\n\nThe binary logistic regression model is part of a family of statistical models called generalised linear models. The main characteristic that differentiates binary logistic regression from other generalised linear models is the type of dependent (or outcome) variable. @harris2019statistics A dependent variable in a binary logistic regression has two levels. For example, a variable that records whether or not someone has ever been diagnosed with a health condition like Stroke could be measured in two categories, yes and no. Likewise, someone might have coronary heart disease or not, be physically active or not, be a current smoker or not, or have any one of thousands of diagnoses or personal behaviours and characteristics that are of interest in family medicine.\n\nThe binary logistic regression algorithm below:\n\n$$ln\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_{0} + \\beta_{1}x_{1} + \\cdots + \\beta_{k}x_{k}$$ \n\nWhere $\\pi = P[Y =1]$ is the probability of the outcome.\n\n<!-- TODO add equation numbering {#eq-1} -->\n\n- Logistic Regression\n- Decision Tree\n- Random Forest\n- Gradient Boosted Machine\n- k-Nearest Neighbors\n- Support Vector Machine\n\n### Assumptions\n\nBinary logistic regression relies on the following underlying assumptions to be true:\n\n* The observations must be independent.\n* There must be no perfect multicollinearity among independent variables.\n* Logistic regression assumes linearity of independent variables and log odds.\n* There are no extreme outliers\n* The Sample Size is Sufficiently Large. Field recommends a minimum of 50 cases. @field2024discovering Hosmer, Lemeshow, and Sturdivant @hosmer2013applied suggest a minimum sample of 10 observations per independent variable in the model. Leblanc and Fitzgerald (2000) @leblanc2000logistic suggest a minimum of 30 observations per independent variable.\n\n\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n\n## 3. Analysis and Results\n\nImport all the dependencies:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npackages <- c(\"dplyr\", \"car\", \"ResourceSelection\", \"caret\", \"pROC\",  \"logistf\", \"Hmisc\", \"rcompanion\", \"ggplot2\", \"summarytools\", \"tidyverse\", \"knitr\", \"ggpubr\", \"ggcorrplot\", \"randomForest\", \"gbm\", \"kernlab\", \"skimr\", \"corrplot\", \"scales\", \"tidyr\", \"RColorBrewer\")\n# Load Libraries\nlapply(packages, library, character.only = TRUE)\n# Set seed for reproducibility\nset.seed(123)\n```\n:::\n\n\n<!-- Maybe add a list describing the packages used with the citations attached -->\n\n### 3.1. Data Ingestion\n\nData source: [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset) @kaggle01\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfind_git_root <- function(start = getwd()) {\n  path <- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path <- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root <- find_git_root()\ndatasets_path <- file.path(repo_root, \"datasets\")\n\n# Reading the datafile healthcare-dataset-stroke-data\nstroke_path <- file.path(datasets_path, \"kaggle-healthcare-dataset-stroke-data/healthcare-dataset-stroke-data.csv\")\nstroke1 = read_csv(stroke_path, show_col_types = FALSE)\n```\n:::\n\n\n### 3.2. Exploratory Data Analysis (EDA)\n\n**Dataset Description**\n\nThe **Stroke Prediction Dataset** @kaggle01 is a publically available dataset for educational purposes containing 5,110 observations containing predictors commonly associated with cerebrovascular risk. The dataset is composed of 11 clinical and demographic features and 1 feature which is **id** a unique identifier for the patient. The dataset has features including patient's **age**, **gender**, presence of conditions like **hypertension** and **heart disease**, **work type**, **residence type**, **average glucose level**, and **BMI**. This dataset is primarily intended for educational purposes as it shares a lot of similarities with the Jackson Heart Study (JHS) dataset but it is not as descriptive.\n\n| Feature Name       | Description                                              | Data Type          | Key Values/Range                                           |\n|--------------------|----------------------------------------------------------|--------------------|------------------------------------------------------------|\n| id                 | Unique identifier for the patient                        | Numeric            | Unique numeric ID                                          |\n| gender             | Patient's gender                                         | Character          | Male, Female, Other                                        |\n| age                | Patient's age in years                                   | Numeric            | 0.08 to 82                                                 |\n| hypertension       | Indicates if the patient has hypertension                | Numeric (binary)   | 0 (No), 1 (Yes)                                            |\n| heart_disease      | Indicates if the patient has any heart diseases          | Numeric (binary)   | 0 (No), 1 (Yes)                                            |\n| ever_married       | Whether the patient has ever been married                | Character          | No, Yes                                                    |\n| work_type          | Type of occupation                                       | Character          | Private, Self-employed, Govt_job, children, Never_worked   |\n| Residence_type     | Patient's area of residence                              | Character          | Rural, Urban                                               |\n| avg_glucose_level  | Average glucose level in blood                           | Numeric            | ≈55.12 to 271.74                                           |\n| bmi                | Body Mass Index                                          | Character          | ≈10.3 to 97.6 (has NA values)                              |\n| smoking_status     | Patient's smoking status                                 | Character          | formerly smoked, never smoked, smokes, Unknown             |\n| stroke             | Target Variable: Whether the patient had a stroke        | Numeric (binary)   | 0 (No Stroke), 1 (Stroke)                                  |\n\n<!-- TODO add a footnote for table -->\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n\n#### **3.2.1 Dataset Preprocessing**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Handle dataset features\nstroke1[stroke1 == \"N/A\" | stroke1 == \"Unknown\" | stroke1 == \"children\" | stroke1 == \"other\"] <- NA\nstroke1$bmi <- round(as.numeric(stroke1$bmi), 2)\nstroke1$gender[stroke1$gender == \"Male\"] <- 1\nstroke1$gender[stroke1$gender == \"Female\"] <- 0\nstroke1$gender <- as.numeric(stroke1$gender)\nstroke1$ever_married[stroke1$ever_married == \"Yes\"] <- 1\nstroke1$ever_married[stroke1$ever_married == \"No\"] <- 0\nstroke1$ever_married <- as.numeric(stroke1$ever_married)\nstroke1$work_type[stroke1$work_type == \"Govt_job\"] <- 1\nstroke1$work_type[stroke1$work_type == \"Private\"] <- 2\nstroke1$work_type[stroke1$work_type == \"Self-employed\"] <- 3\nstroke1$work_type[stroke1$work_type == \"Never_worked\"] <- 4\nstroke1$work_type <- as.numeric(stroke1$work_type)\nstroke1$Residence_type[stroke1$Residence_type == \"Urban\"] <- 1\nstroke1$Residence_type[stroke1$Residence_type == \"Rural\"] <- 2\nstroke1$Residence_type <- as.numeric(stroke1$Residence_type)\nstroke1$avg_glucose_level <- as.numeric(stroke1$avg_glucose_level)\nstroke1$heart_disease <- as.numeric(stroke1$heart_disease)\nstroke1$hypertension <- as.numeric(stroke1$hypertension)\nstroke1$age <- round(as.numeric(stroke1$age), 2)\nstroke1$stroke <- as.numeric(stroke1$stroke)\nstroke1$smoking_status[stroke1$smoking_status == \"never smoked\"] <- 1\nstroke1$smoking_status[stroke1$smoking_status == \"formerly smoked\"] <- 2\nstroke1$smoking_status[stroke1$smoking_status == \"smokes\"] <- 3\nstroke1$smoking_status <- as.numeric(stroke1$smoking_status)\nstroke1 <- stroke1[, !(names(stroke1) %in% \"id\")]\n\n# Remove NAs and clean dataset\nstroke1$stroke <- as.factor(stroke1$stroke)\nstroke1_clean <- na.omit(stroke1)\nstrokeclean <- stroke1_clean\nfourassume <- stroke1_clean\n\nstrokeclean$stroke <- factor(\n  strokeclean$stroke,\n  levels = c(\"0\", \"1\"),\n  labels = c(\"No\", \"Yes\")\n)\n\nfourassume$stroke <- factor(\n  fourassume$stroke,\n  levels = c(\"0\", \"1\"),\n  labels = c(\"No\", \"Yes\")\n)\n```\n:::\n\n\nThe initial exploration demonstrated that the **Stroke Prediction Dataset** @kaggle01 has several issues requiring changes for handling missing values, converting character (categorical) features into numerical codes, and removing the identifier column.\n\n<!-- TODO -->\nSo as part of data preprocessing we will be focused on establishing consistency and ensuring all variables are in a format suitable for predictive modeling. This process starts by systematically addressing non-standard representations of missing data. Specifically, all instances of the string values \"N/A\", \"Unknown\", \"children\", and \"other\" found across the dataset were unified and replaced with the standard statistical missing value representation, NA.\n\nThen we proceed with converting several character-based (categorical) features into numerical features, which is necessary for predictive modeling.\n<!-- TODO -->\n\nThe feature **bmi**, initially read as a character variable was first converted to a numeric data type and subsequently rounded to two decimal places.\n\nThe binary categorical features were encoded into numerical indicators. The feature **gender** was transformed so that **\"Male\"** was encoded to 1 and **\"Female\"** was encoded to 0, and the **ever_married** was transformed so that **\"Yes\"** encoded to 1 and **\"No\"** encoded to 0.\n\nFeatures with multiple categories were also numerically encoded into numerical indicators. The **work_type** feature had its categories encoded so that **\"Govt_job\"** = 1, **\"Private\"** = 2, **\"Self-employed\"** = 3, and **\"Never_worked\"** = 4. The **Residence_type** was encoded so that **\"Urban\"** = 1 and **\"Rural\"** = 2. Finally, the **smoking_status** feature was encoded into three numerical levels, those being **\"never smoked\"** = 1, **\"formerly smoked\"** = 2, and **\"smokes\"** = 3.\n\nAdditionally, the continuous numerical variables **avg_glucose_level**, **heart_disease**, and **hypertension** were explicitly confirmed as numeric data types, with the **age** feature also being rounded to two decimal places for consistency.\n\n<!-- TODO -->\nThe final stage of preprocessing involved removing the **id** column, which served only as a unique identifier and held no predictive value. This action left the dataset with 11 core predictors. The target variable, **stroke**, was then converted into a factor (a categorical data type in R) named **stroke1**, and its levels were explicitly labeled as $\\text{\"No\"} = 0$ and $\\text{\"Yes\"} = 1$. The entire process concluded with the removal of all remaining observations containing missing or inconsistent entries, resulting in the creation of the final, clean data frames, **strokeclean** and **fourassume**.\n\n\n**Dataset Preprocessing Conclusion**\n\nThe **Stroke Prediction Dataset** @kaggle01 that started containing 5,110 observations and 12 features. After cleaning missing and inconsistent entries among other necessarychanges, ended as a dataset containing 3,357 observations and 11 predictors commonly associated with cerebrovascular risk. Those key predictors are listed below.\n\n<!-- TODO modify this table to add a better description and be more visually engaging -->\n\n| Feature Name      | Description                                      | Data Type | Values                                                               |\n|-------------------|--------------------------------------------------|-----------|----------------------------------------------------------------------|\n| gender            | Patient's gender                                 | Numeric   | 1 (Male), 0 (Female)                                                 |\n| age               | Patient's age in years                           | Numeric   | Range 0.08 to 82; rounded to 2 decimal places                        |\n| hypertension      | Indicates if the patient has hypertension        | Numeric   | 0 (No), 1 (Yes)                                                      |\n| heart_disease     | Indicates if the patient has any heart diseases  | Numeric   | 0 (No), 1 (Yes)                                                      |\n| ever_married      | Whether the patient has ever been married        | Numeric   | 1 (Yes), 0 (No)                                                      |\n| work_type         | Type of occupation                               | Numeric   | 1 (Govt_job), 2 (Private), 3 (Self-employed), 4 (Never_worked)       |\n| Residence_type    | Patient's area of residence                      | Numeric   | 1 (Urban), 2 (Rural)                                                 |\n| avg_glucose_level | Average glucose level in blood                   | Numeric   | Range ≈55.12 to 271.74                                               |\n| bmi               | Body Mass Index                                  | Numeric   | Range ≈10.3 to 97.6; converted from character, rounded to 2 decimals |\n| smoking_status    | Patient's smoking status                         | Numeric   | 1 (never smoked), 2 (formerly smoked), 3 (smokes)                    |\n| stroke            | Target Variable: Whether the patient had stroke  | Numeric   | 0 (No Stroke), 1 (Stroke)                                            |\n\n<!-- TODO add table footnote -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# skim(stroke1)\n# nrow(fourassume)\n# class(strokeclean$stroke)\n# unique(strokeclean$gender)\n```\n:::\n\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n#### **3.2.2 Dataset Visualization**\n\nBefore developing predictive models, an exploratory analysis was conducted to understand the distribution, structure, and relationships within the cleaned dataset (N = 3,357). This step is crucial in rare-event medical modeling because data imbalance, skewed predictors, or correlated variables can directly influence model behavior and classification performance.\n\n**Histograms**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# 1. Get the total number of rows in your data frame\nTOTAL_ROWS <- nrow(strokeclean)\n\n# 2. Use the modified ggplot code\np1a <- ggplot(strokeclean, aes(x = gender, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    # The calculation is (bar_count / TOTAL_ROWS) * 100, rounded to 1 decimal place.\n    position = position_dodge(width = 0.9),\n    aes(\n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    vjust = -0.5,\n    size = 3\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  scale_x_continuous(\n    breaks = c(0, 1), \n    labels = c(\"Female\", \"Male\")\n  ) +\n  labs(title = \"(a) Gender\", x = \"Gender\", y = \"Count\")\n\n# (b) Histogram of Age\np1b <- ggplot(strokeclean, aes(x = age, fill = stroke)) +\n  geom_histogram(binwidth = 1, position = \"identity\", alpha = 0.7) +\n  # stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(b) Age\", x = \"Age\", y = \"Frequency\")\n\n# (b) Bivariate Density Plot of Age\n# p1b <- ggplot(strokeclean, aes(x = age, fill = stroke)) + # Keep fill=stroke\n#   geom_density(alpha = 0.5) + # Overlap the two density curves\n#   labs(title = \"(b) Age\", x = \"Age\", y = \"Density\")\n\n# (c) Histogram of hypertension\np1c <- ggplot(strokeclean, aes(x = hypertension, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    position = position_dodge(width = 0.9),\n    aes(\n      group = stroke,\n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    vjust = -0.5,\n    size = 3\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  # Map 0/1 to Yes/No\n  scale_x_continuous(\n    breaks = c(0, 1),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(title = \"(c) Hypertension\", x = \"Hypertension\", y = \"Frequency\")\n\n# (d) Histogram of heart_disease\np1d <- ggplot(strokeclean, aes(x = heart_disease, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    position = position_dodge(width = 0.9),\n    aes(\n      group = stroke,\n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    vjust = -0.5,\n    size = 3\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  # Map 0/1 to Yes/No\n  scale_x_continuous(\n    breaks = c(0, 1),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(title = \"(d) Heart Disease\", x = \"Heart Disease\", y = \"Frequency\")\n\n# (e) Histogram of ever_married\np1e <- ggplot(strokeclean, aes(x = ever_married, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    position = position_dodge(width = 0.9),\n    aes(\n      group = stroke,\n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    vjust = -0.5,\n    size = 3\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  scale_x_continuous(\n    breaks = c(0, 1),\n    labels = c(\"No\", \"Yes\")\n  ) +\n  # Assuming 'No'/'Yes' are string/factor values, use scale_x_discrete if needed\n  labs(title = \"(e) Ever Married\", x = \"Ever Married\", y = \"Frequency\")\n\n# (f) Histogram of work_type\np1f <- ggplot(strokeclean, aes(y = work_type, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    position = position_dodge(width = 0.9), \n    aes(\n      group = stroke,\n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    hjust = -0.1, # Shift text right for horizontal bar\n    size = 3,\n    color = \"black\"\n  ) +\n  # Expand X-axis (Frequency) for horizontal bar\n  scale_x_continuous(expand = expansion(mult = c(0, 0.5))) +\n  # Adding Work type labels make it too convoluted\n  # scale_y_continuous(\n  #   breaks = c(1, 2, 3, 4), \n  #   labels = c(\"Govt_job\", \"Private\", \"Self-employed\", \"Never_worked\")\n  # ) + \n  labs(title = \"(f) Work Type\", y = \"Work Type\", x = \"Frequency\")\n\n# (g) Histogram of Residence_type\np1g <- ggplot(strokeclean, aes(x = Residence_type, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    # Crucial for aligning text labels with the dodged bars\n    position = position_dodge(width = 0.9), \n    aes(\n      # Defines the group for position_dodge to work correctly on text\n      group = stroke, \n      \n      # Combined label: Percentage (top line) + Count (bottom line)\n      label = paste0(\n        # Percentage calculation: (count / TOTAL_ROWS) * 100\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    vjust = -0.5, # Moves the two-line label slightly above the bar\n    size = 3,\n    color = \"black\" # Ensures better visibility\n  ) +\n  # Adds 15% extra space to the top of the y-axis to prevent label clipping\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) + \n  scale_x_continuous(\n    breaks = c(1, 2),\n    labels = c(\"Urban\", \"Rural\")\n  ) +\n  labs(title = \"(g) Residence Type\", x = \"Residence Type\", y = \"Frequency (Count)\")\n\n# (h) Histogram of avg_gloucose_level\np1h <- ggplot(strokeclean, aes(x = avg_glucose_level, fill = stroke)) +\n  geom_histogram(binwidth = 5, position = \"identity\", alpha = 0.7) +\n  # stat_count(aes(label = ..count..), geom = \"text\", vjust = -0.5, size = 2) +\n  labs(title = \"(h) Avg. Glucose Level\", x = \"Glucose Level\", y = \"Frequency\")\n\n# (h) Bivariate Density plot of avg_gloucose_level\n# p1h <- ggplot(strokeclean, aes(x = avg_glucose_level, fill = stroke)) +\n#   geom_density(alpha = 0.5) +\n#   labs(title = \"Avg. Glucose Level by Stroke Status\", x = \"Average Glucose Level\", y = \"Density\")\n\n# (i) Histogram of bmi\np1i <- ggplot(strokeclean, aes(x = bmi, fill = stroke)) +\n  geom_histogram(binwidth = 2, position = \"identity\", alpha = 0.7) +\n  labs(title = \"(i) BMI\", x = \"BMI\", y = \"Frequency\")\n\n# (i) Bivariate Density plot of bmi\n# p1i <- ggplot(strokeclean, aes(x = bmi, fill = stroke)) +\n#   geom_density(alpha = 0.5) +\n#   labs(title = \"BMI Distribution by Stroke Status\", x = \"BMI\", y = \"Density\")\n\n# (j) smoking_status\np1j <- ggplot(strokeclean, aes(y = smoking_status, fill = stroke)) +\n  geom_bar(position = \"dodge\") +\n  stat_count(\n    position = position_dodge(width = 0.9), \n    aes(\n      group = stroke, \n      label = paste0(\n        round(after_stat(count) / TOTAL_ROWS * 100, 1), \"% \", \"or \",\n        after_stat(count)\n      )\n    ),\n    geom = \"text\",\n    hjust = -0.1, \n    size = 3,\n    color = \"black\" \n  ) +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.5))) + \n  labs(title = \"(j) Smoking Status\", y = \"Smoking Status\", x = \"Frequency (Count)\")\n```\n:::\n\n\nWe can observe from the histograms (a), (b), (c) and (d) the following: \n\nThe data appears to be slightly imbalanced towards **female** gender and the proportion of stroke cases relative to the total number of individuals in each gender appears similar for both genders, even if it looks slightly higher in the male doesnt seem to be significant difference. \n\nThe number of stroke cases increases dramatically after the **age** of $\\approx 50$ and peaks in the 60 to 80 age range. This strongly suggests **age is a critical risk factor for stroke**.\n\nThe majority of patients do not have **hypertension** and the proportion of stroke cases (blue bar) is visibly much higher in the group with **hypertension**. This indicates that **hypertension is a strong risk factor for stroke**.\n\nSimilar to hypertension, the majority of patients do not have **heart disease** and the proportion of stroke cases (blue bar) is visibly much higher in the group with **heart disease**. This indicates that **heart disease is a very strong risk factor** for stroke, even stronger than hypertension when based alone on the observed proportions.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# p1a, p1b, p1c, p1d\n# (a) Histogram of gender \n# (b) Histogram of Age\n# (c) Histogram of hypertension\n# (d) Histogram of heart_disease\nggarrange(p1a, p1b, p1c, p1d, \n          ncol = 2, nrow = 2, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Histogram of (a)gender, (b)age, (c)hypertension, (d)heart_disease.](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWe can observe from the histograms (e), (f), (g) and (h) the following: \n\nThe stroke rate appears higher for those who have ever been married which is a fascinating plot that catches our attention, this must be correlated with another variable. Our guess is that having been married being associated with a higher stroke risk in this dataset, is possibly due to the married group skewing toward older ages\n\nAcross the four work types encoded, \"Govt_job\" = 1, \"Private\" = 2  \"Self-employed\" = 3, \"Never Worked\" = 4. **Self-employed** individuals appear to have the highest risk proportion among the working groups. Followed by the **Private** which is the largest group (total $\\approx 2200$) and naturally accounts for the highest raw count of stroke cases (109) with a proportion of stoke incidence sligthly higher than **Govt_job**.\n\nThe stroke outcomes based on the patient's **residence type** has a very similar raw count their proportions seems to be similar as well. This suggests that **residence type** does not appear to be a significant factor for stroke risk.\n\nFrom the distribution of **average glucose** (HbA1c) we can visually spot that the stroke cases are more frequent for high-glucose relative to the total population at those high levels. This higher propportion indicates that **high average glucose (HbA1c) level is a significant risk factor** for stroke.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# p1e p1f p1g p1h\n# (e) Histogram of ever_married\n# (f) Histogram of work_type\n# (g) Histogram of Residence_type\n# (h) Histogram of avg_gloucose_level\nggarrange(p1e, p1f, p1g, p1h,\n          ncol = 2, nrow = 2, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Histogram of (e)ever_married, (f)work_type, (g)Residence_type, (h)avg_gloucose_level.](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe can observe from the histograms (i) and (j) the following: \n\nFor the BMI distribution we can observe that the majority of the patient population (pink bars) falls within the overweight to obese range (BMI $\\approx 25$ to $35$). So as a consequence we can expect that the frequency of stroke cases (blue bars) will follow the distribution of the overall population, meaning most strokes occur where the largest number of people are located which are the BMI values between $25$ and $35$.\n\nHowever, we can visually spot that the stroke occurence is drops significantly closer to a healthy BMI of 20. So although the risk of stroke does seem to be generally higher than average once BMI exceeds the ideal range and moves into the overweight and obese categories because there is a larger distribution within the overweight to obese range, we can conclude that because the skewed distributin that **BMI is a significant risk factor predictor for stroke**.\n\nThe stroke outcomes are compared across the three smoking status categories encoded: **smokes = 3**, **formerly smoked = 2**, and **never smoked = 1**.\n\nThis plot is highlights a particularly interesting aspect of this dataset. The highest proportional risk of stroke appears to be in the **formerly smoked** group. This finding is common in medical literature @oshunbade2020cigarette, as individuals who have a history of smoking may have accrued vascular damage that persists, but their stroke risk is still lower than the risk for current smokers if they continue to smoke. \n\nThis information is importante, because the **formerly smoked** group shows the highest rate, suggesting that **a history of smoking is a significant indicator of risk.**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# p1i p1j\n# (i) Histogram of bmi\n# (j) smoking_status\nggarrange(p1i, p1j,\n          ncol = 2, nrow = 1, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Histogram of (i)bmi, (j)smoking_status.](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n#### **3.2.3 Correlation Analysis**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Select numeric predictors\nnumeric_vars = strokeclean[, c(\n  \"age\", \"bmi\", \"avg_glucose_level\", \"hypertension\", \"heart_disease\"\n)]\n\n# Correlation matrix\ncorr_matrix = cor(numeric_vars)\n\n# High-contrast heatmap\np2 <- ggcorrplot(\n  corr_matrix,\n  method = \"square\",\n  type = \"lower\",\n  lab = TRUE,\n  lab_size = 4.5,\n  tl.cex = 12,\n  tl.srt = 45,\n  outline.col = NA,\n  colors = c(\"#B2182B\", \"white\", \"#2166AC\"),   # high contrast red→white→blue\n  ggtheme = theme_minimal(base_size = 14)\n) +\n  ggtitle(\"Correlation Heatmap of Key Numeric Predictors\") +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    axis.text  = element_text(color = \"black\", size = 11)\n  )\n```\n:::\n\n\n**Interpretation**\nCorrelation Heatmap of Key Numeric Predictors\n\nAll correlations are weak to moderate (0.00–0.26) → no multicollinearity concerns.\n\nAge shows small but meaningful positive correlations with:\n\nglucose (0.24)\n\nhypertension (0.26)\n\nheart disease (0.26)\n→ consistent with known aging-related cardiovascular risk patterns.\n\nBMI has very weak correlations with all other predictors (0.04–0.16) → behaves independently in this dataset.\n\nAvg glucose moderately correlates with:\n\nhypertension (0.17)\n\nheart disease (0.14)\n→ aligns with metabolic/vascular relationships.\n\nHypertension and heart disease are weakly correlated (0.11) → related but not redundant.\n\nThese correlations confirm that the predictors provide unique, non-overlapping information, and all can be safely included in the logistic regression model without multicollinearity issues.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np2\n```\n\n::: {.cell-output-display}\n![Correlation Analysis.](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n### 3.3. Statistical Modelling\n\nInitially, we split the dataset into a training set (70%) and a test set (30%) to evaluate out-of-sample performance, then we used this training data for our statistical modelling. It is important to note that during splitting, stratified sampling was used (via caret::createDataPartition) to maintain the stroke/no-stroke ratio. @chen2020modeling\n\n<!-- TODO ----- Data splitting for logistic regression involves partitioning your dataset into training and testing sets to build and validate the model's probability of success predictions -->\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_df <- strokeclean\nmodel_df <- na.omit(model_df)\nmodel_df$stroke <- factor(model_df$stroke)\nlevels(model_df$stroke) <- c(\"No\", \"Yes\")\ntable(model_df$stroke)\n\nindex <- createDataPartition(strokeclean$stroke, p = 0.70, list = FALSE)\ntrain_data <- strokeclean[index, ]\ntest_data  <- strokeclean[-index, ]\n\ntrain_data$stroke <- factor(train_data$stroke, levels = c(\"No\",\"Yes\"))\ntest_data$stroke  <- factor(test_data$stroke,  levels = c(\"No\",\"Yes\"))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert all multi-level categoricals to factors with a clear reference level\ntrain_data$work_type     <- factor(train_data$work_type)\ntrain_data$Residence_type<- factor(train_data$Residence_type)\ntrain_data$smoking_status<- factor(train_data$smoking_status)\n\n# The same should be done for test_data and the binary variables \ntest_data$work_type     <- factor(test_data$work_type)\ntest_data$Residence_type<- factor(test_data$Residence_type)\ntest_data$smoking_status<- factor(test_data$smoking_status)\n\n# if you want the output to label the levels (e.g., \"Male\" vs \"Female\")\n# instead of \"gender\" and \"gender1\" (for Male = 1 vs Female = 0).\n# For 0/1, R's glm is usually fine, but for clean output factors are better.\n# For multi-level, it's essential.\n```\n:::\n\n\n\n<!-- - Logistic Regression\n- Decision Tree\n- Random Forest\n- Gradient Boosted Machine\n- k-Nearest Neighbors\n- Support Vector Machine -->\n\n\n#### **3.3.1. Repeated K-fold cross-validation**\n\nThe trainControl() function in the R caret package is used to control the computational nuances and resampling methods employed by the train() function. It allows us to implement Repeated K-fold cross-validation (\"repeatedcv\").\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nctrl <- trainControl(\nmethod = \"repeatedcv\",\nnumber = 5,\nrepeats = 3,\nclassProbs = TRUE,\nsummaryFunction = twoClassSummary,\nverboseIter = FALSE\n)\n```\n:::\n\n\n#### **3.3.2. Logistic Regression**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_lr <- glm(\n  stroke ~ . , \n  data=train_data , \n  family = \"binomial\" (link=logit)\n  )\ns1 <- summary(model_lr)\nc1 <- coefficients(model_lr)\nanova1 <- car::Anova(model_lr, type = 3)\nconfint1 <- confint(model_lr, level=0.95)\n\nmodel_lr2 <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"glm\",\nfamily = \"binomial\",\nmetric = \"ROC\",\ntrControl = ctrl\n)\n```\n:::\n\n\n**Interpretation — Logistic Regression Coefficients**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = stroke ~ ., family = binomial(link = logit), data = train_data)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        -8.113391   0.854545  -9.494  < 2e-16 ***\ngender             -0.112742   0.204658  -0.551  0.58172    \nage                 0.078380   0.008614   9.099  < 2e-16 ***\nhypertension        0.914733   0.214191   4.271 1.95e-05 ***\nheart_disease       0.339604   0.277662   1.223  0.22130    \never_married       -0.532738   0.293381  -1.816  0.06939 .  \nwork_type2          0.072261   0.288269   0.251  0.80207    \nwork_type3         -0.290608   0.324634  -0.895  0.37069    \nwork_type4         -9.306169 649.652359  -0.014  0.98857    \nResidence_type2    -0.072792   0.198250  -0.367  0.71349    \navg_glucose_level   0.005488   0.001670   3.287  0.00101 ** \nbmi                 0.002103   0.015554   0.135  0.89245    \nsmoking_status2     0.208775   0.226763   0.921  0.35722    \nsmoking_status3     0.345133   0.266423   1.295  0.19517    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 982.44  on 2349  degrees of freedom\nResidual deviance: 767.21  on 2336  degrees of freedom\nAIC: 795.21\n\nNumber of Fisher Scoring iterations: 14\n```\n\n\n:::\n\n```{.r .cell-code}\nanova1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: stroke\n                  LR Chisq Df Pr(>Chisq)    \ngender               0.305  1   0.580683    \nage                107.200  1  < 2.2e-16 ***\nhypertension        17.103  1   3.54e-05 ***\nheart_disease        1.439  1   0.230228    \never_married         3.064  1   0.080044 .  \nwork_type            2.479  3   0.479126    \nResidence_type       0.135  1   0.713341    \navg_glucose_level   10.535  1   0.001171 ** \nbmi                  0.018  1   0.892611    \nsmoking_status       1.905  2   0.385861    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n<!-- ANOVA conclusion -->\n\nConclusion: These three predictors ($\\text{age}$, $\\text{hypertension}$, and $\\text{avg\\_glucose\\_level}$) are the most important and reliable drivers of stroke risk in your final model. They independently contribute meaningful, non-redundant predictive power.\n\n$\\text{ever\\_married}$ has a Marginal Significance. While its $\\text{p-value}$ is close to $0.05$, suggesting a trend, it is not definitively significant in this multivariable model.\n\n$\\text{heart\\_disease}$ Insignificant. Its unique contribution is not statistically distinguishable from zero after accounting for other factors like $\\text{age}$ and $\\text{hypertension}$ (which it is correlated with).\n\nBased solely on this ANOVA table, the performance evaluation suggests:\n\nKeep: $\\text{age}$, $\\text{hypertension}$, $\\text{avg\\_glucose\\_level}$.\n\nConsider Removing: $\\text{gender}$, $\\text{bmi}$, $\\text{Residence\\_type}$, $\\text{work\\_type}$, and $\\text{smoking\\_status}$.\n\nTwo way interactions\n\nYou should definitely consider two-way interactions, especially between your most significant predictors: $\\text{age}$, $\\text{hypertension}$, and $\\text{avg\\_glucose\\_level}$.\n\nBased on your ANOVA results, prioritize interactions among the strongest drivers:\n\n$\\mathbf{\\text{age} \\times \\text{hypertension}}$\n\n$\\mathbf{\\text{age} \\times \\text{avg\\_glucose\\_level}}$\n\n$\\mathbf{\\text{hypertension} \\times \\text{avg\\_glucose\\_level}}$\n\nYou might also test $\\text{age} \\times \\text{heart\\_disease}$, as $\\text{heart\\_disease}$ is clinically important even if its main effect was marginally insignificant in the multivariate model.\n\n\n\n<!-- TODO -->\n\n- Age is a strong and highly significant predictor (p < 0.001).\nHigher age is associated with a substantial increase in the odds of stroke.\n\n- Hypertension has a significant positive effect on stroke risk (p = 0.0468), indicating hypertensive individuals are more likely to experience stroke.\n\n- Average glucose level is also a important predictor (p = 0.0267).\nHigher glucose values modestly increase stroke risk.\n\n- Heart disease shows a positive association but is only borderline significant (p = 0.0718).\nThis suggests a potential effect, but not statistically explainable in this model.\n\n- Smoking has likewise borderline significant (p = 0.0714), indicating a  increased risk among smokers, but the evidence is not too much strong.\n\n- BMI, gender, and marital status show no meaningful statistical association with stroke in this dataset (all p > 0.26). These variables did not contribute substantially to prediction after accounting for other factors.\n\n- Model fit improved substantially from the null model\n(deviance reduced from 953.4 → 776.8; AIC = 794.8), indicating a reasonable fit and useful predictive value.\n\n**Odds ratios and confidence intervals**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Odds ratios and 95% confidence intervals\ncoef_est <- coef(model_lr)\nOR       <- exp(coef_est)\n\nconf_int <- exp(confint(model_lr))  # confidence intervals on OR scale\n# conf_int <- confint(model_lr, level=0.95)\n\nodds_table <- cbind(OR, conf_int)\ncolnames(odds_table) <- c(\"OR\", \"2.5 %\", \"97.5 %\")\nround(odds_table, 3)\n```\n:::\n\n\n**Interpretation**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nodds_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OR        2.5 %       97.5 %\n(Intercept)       2.995016e-04 5.241734e-05 1.501885e-03\ngender            8.933809e-01 5.952209e-01 1.329871e+00\nage               1.081533e+00 1.064009e+00 1.100629e+00\nhypertension      2.496110e+00 1.631252e+00 3.782969e+00\nheart_disease     1.404392e+00 8.003898e-01 2.385372e+00\never_married      5.869958e-01 3.364242e-01 1.068991e+00\nwork_type2        1.074936e+00 6.232860e-01 1.940865e+00\nwork_type3        7.478086e-01 3.995414e-01 1.435847e+00\nwork_type4        9.086193e-05           NA 8.029055e+24\nResidence_type2   9.297939e-01 6.291289e-01 1.370644e+00\navg_glucose_level 1.005503e+00 1.002197e+00 1.008789e+00\nbmi               1.002105e+00 9.713431e-01 1.032426e+00\nsmoking_status2   1.232167e+00 7.869056e-01 1.918172e+00\nsmoking_status3   1.412178e+00 8.278837e-01 2.361771e+00\n```\n\n\n:::\n:::\n\n\nThe logistic regression findings demonstrate how each predictor impacts the likelihood of having a stroke, while keeping other variables constant:\n\n- Age (OR = 1.075, CI: 1.059–1.093)\nAge is the strongest continuous predictor. Each additional year of age increases the odds of stroke by about 7.5%, and the confidence interval does not include 1, indicating strong statistical significance.\n\n- Hypertension (OR = 1.577, CI: 0.996–2.450)\nIndividuals with hypertension have roughly 58% higher odds of stroke compared to those without hypertension, although the lower CI bound is just below 1. This suggests a borderline significant effect, but clinically important.\n\n- Heart disease (OR = 1.628, CI: 0.942–2.733)\nHeart disease increases stroke odds by about 63%, but the CI includes 1, implying the association is positive but not statistically strong in this dataset.\n\n- Average glucose level (OR = 1.004, CI: 1.000–1.007)\nHigher glucose levels are associated with slightly increased stroke risk. Though the effect is small, the CI indicates marginal significance, aligning with known metabolic risk patterns.\n\n- BMI (OR = 1.007, CI: 0.975–1.037)\nBMI shows almost no meaningful effect on stroke risk, and the CI overlaps 1. This predictor does not significantly influence stroke likelihood in this dataset.\n\n- Smoking (Fsmoked OR = 1.263; Smokes OR = 1.598)\n\n- Former smokers have 26% higher odds, but CI crosses 1 → weak evidence.\n\n- Current smokers have ~60% higher odds, but CI still overlaps 1 → suggests increased risk but not statistically conclusive here.\n\n- Gender (Female) (OR = 1.259; CI: 0.842–1.903)\nFemales show slightly higher odds, but this effect is not statistically significant.\n\n- Ever married (OR = 1.126; CI: 0.590–2.013)\nMarital status has no clear effect on stroke odds in this sample.\n\n**Model predictions and performance on the test set**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# 1) Predicted probabilities from logistic regression\ntest_data$pred_prob <- predict(\n  model_lr,\n  newdata = test_data,\n  type    = \"response\"\n)\n\n# 2) Make sure the TRUE outcome is a factor with levels No / Yes\ntest_data$stroke <- factor(test_data$stroke,\n                             levels = c(\"No\", \"Yes\"))\n\n# 3) Class predictions at threshold c = 0.5\ntest_data$pred_class <- ifelse(test_data$pred_prob >= 0.5, \"Yes\", \"No\")\ntest_data$pred_class <- factor(test_data$pred_class,\n                                 levels = c(\"No\", \"Yes\"))\n\n# 4) Confusion matrix: positive = \"Yes\"\ncm <- confusionMatrix(\n  data      = test_data$pred_class,\n  reference = test_data$stroke,\n  positive  = \"Yes\"\n)\n\n# cm\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  951  53\n       Yes   2   1\n                                          \n               Accuracy : 0.9454          \n                 95% CI : (0.9295, 0.9586)\n    No Information Rate : 0.9464          \n    P-Value [Acc > NIR] : 0.5908          \n                                          \n                  Kappa : 0.0296          \n                                          \n Mcnemar's Test P-Value : 1.562e-11       \n                                          \n            Sensitivity : 0.018519        \n            Specificity : 0.997901        \n         Pos Pred Value : 0.333333        \n         Neg Pred Value : 0.947211        \n             Prevalence : 0.053625        \n         Detection Rate : 0.000993        \n   Detection Prevalence : 0.002979        \n      Balanced Accuracy : 0.508210        \n                                          \n       'Positive' Class : Yes             \n                                          \n```\n\n\n:::\n:::\n\n\nFrom the confusion matrix, the following performance metrics are defined:\n\n**Accuracy**\n$$\n\\text{Accuracy} =\n\\frac{TP + TN}{TP + TN + FP + FN}.\n$$\n**Sensitivity (Recall / True Positive Rate)**\n\n$$\n\\text{Sensitivity} =\n\\frac{TP}{TP + FN}.\n$$\n**Specificity (True Negative Rate)**\n\n$$\n\\text{Specificity} =\n\\frac{TN}{TN + FP}.\n$$\n\n**Positive Predictive Value (Precision)**\n$$\n\\text{PPV} =\n\\frac{TP}{TP + FP}.\n$$\n**Negative Predictive Value (NPV)**\n\n$$\n\\text{NPV} =\n\\frac{TN}{TN + FN}.\n$$\n\n**Interpretation of Logistic Regression Performance (Test Set)**\n\n- Accuracy = 94.25%\nThe model correctly classified most cases, mainly because the dataset is highly imbalanced (only ~6% stroke cases). High accuracy here does not mean good stroke detection.\n\n- Sensitivity (True Positive Rate) = 0.017\nThe model correctly identified only 1 out of 59 actual stroke cases (≈1.7%).\n→ This shows the model fails to detect stroke cases, which is common in rare-event medical datasets.\n\n- Specificity (True Negative Rate) = 1.00\nThe model correctly classified all non-stroke cases.\n→ It is extremely good at predicting “No stroke,” which dominates the dataset.\n\n- Positive Predictive Value (Precision) = 1.00\nWhen the model predicts “Yes,” it is always correct — but it predicted “Yes” only once.\nHigh precision is misleading because the model rarely predicts a positive case.\n\n- Negative Predictive Value = 0.942\nMost “No” predictions are correct, matching the overall class imbalance.\n\n- Kappa = 0.031\nKappa measures agreement beyond chance. A value near zero shows the model performs only slightly better than random when considering class imbalance.\n\n- Balanced Accuracy = 0.508\nWhen weighting sensitivity and specificity equally, the model performs at chance level (~50%).\n→ Confirms that stroke detection is weak.\n\n- McNemar’s Test p < 0.0001\nStrong evidence that the model’s errors are systematically skewed—it overwhelmingly predicts “No stroke.”\n\nThe logistic regression model achieves high accuracy only because the negative class dominates.It detects almost no true stroke cases, giving extremely poor sensitivity.\nIt performs well for the majority class (non-stroke), but fails for the minority class (stroke).\n\nThese results highlight the challenge of severe class imbalance, which requires additional techniques (e.g., SMOTE, class weights, resampling) to improve medical-event prediction.\n\n**ROC curve and AUC for the logistic model**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Compute ROC\nroc_glm <- roc(\n  response  = test_data$stroke,\n  predictor = test_data$pred_prob,\n  levels    = c(\"No\",\"Yes\"),\n  direction = \"<\"\n)\n\nauc_val <- auc(roc_glm)\n\n# Extract data for ggplot\nroc_df <- data.frame(\n  fpr = rev(1 - roc_glm$specificities),\n  tpr = rev(roc_glm$sensitivities)\n)\n\n# Plot\nroc_plot <- ggplot(roc_df, aes(x = fpr, y = tpr)) +\n  geom_line(color = \"#0072B2\", size = 1.2) +\n  geom_abline(linetype = \"dashed\", color = \"gray60\", linewidth = 0.9) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    x = \"1 – Specificity (False Positive Rate)\",\n    y = \"Sensitivity (True Positive Rate)\"\n  ) +\n  annotate(\"text\",\n           x = 0.70, y = 0.20,\n           label = paste0(\"AUC = \", round(auc_val, 3)),\n           size = 5) +\n  theme_bw(base_size = 13) +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nA higher AUC (closer to 1) indicates better discrimination between stroke and non-stroke cases. Values substantially above 0.5 indicate that the model performs better than random classification.\n\n**Interpretation of ROC Curve and AUC (Test Set)**\n\nThe ROC curve evaluates the model’s ability to distinguish between stroke and non-stroke cases across all possible classification thresholds, not just the default 0.5 cutoff.\n\nThe AUC = 0.815, which indicates good discriminative performance.\n\nAUC = 0.5 is  no discrimination (random guessing)\n\nAUC = 0.7–0.8 is acceptable\n\nAUC = 0.8–0.9 is good\n\nAUC > 0.9 is excellent\n\n- Even though the confusion matrix showed poor sensitivity at threshold 0.5, the AUC reveals that the model can separate the two classes reasonably well if a better threshold is chosen.\n\n- The strong AUC compared to weak sensitivity highlights the impact of severe class imbalance and the importance of customizing the probability cutoff for medical prediction tasks.\n\nOverall, the ROC analysis suggests that the logistic model contains useful predictive signal, but performance for detecting stroke can be improved with:\n\n- threshold tuning,\n\n- cost-sensitive training,\n\n- resampling techniques (SMOTE / oversampling).\n\n\n<!-- TODO -->\n<!-- Add Assumption testing here -->\n\n* The observations must be independent.\n* There must be no perfect multicollinearity among independent variables. Use the VIF.\n* Logistic regression assumes linearity of independent variables and log odds.\n* There are no extreme outliers, check using Cooks D\n* The Sample Size is Sufficiently Large. \n\n**Check Multicollinearity**\n\nIn OLS regression, multicollinearity can be calculated either from the correlations among the predictors, or from the correlations among the coefficient estimates, and these result in the same variance inflaction factors (VIFs).\n\nIn GLMs, these two approaches yield similar but different VIFs. John Fox, one of the authors of the car package where the vif() function is found, opts for calculating the VIFs from the coefficient estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(model_lr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                      GVIF Df GVIF^(1/(2*Df))\ngender            1.042583  1        1.021069\nage               1.224353  1        1.106505\nhypertension      1.038949  1        1.019288\nheart_disease     1.072781  1        1.035751\never_married      1.023266  1        1.011566\nwork_type         1.083443  3        1.013447\nResidence_type    1.012883  1        1.006421\navg_glucose_level 1.118062  1        1.057384\nbmi               1.158761  1        1.076457\nsmoking_status    1.086902  2        1.021051\n```\n\n\n:::\n:::\n\n\n**Check Outliers**\n\n<!-- TODO -->\n\n\n#### **3.3.3. Decision Tree**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_tree <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"rpart\",\nmetric = \"ROC\",\ntrControl = ctrl,\ntuneLength = 10\n)\n```\n:::\n\n\n#### **3.3.4. Random Forest**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_rf <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"rf\",\nmetric = \"ROC\",\ntrControl = ctrl,\ntuneLength = 5\n)\n```\n:::\n\n\n#### **3.3.5. Gradient Boosted Machine (GBM)**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_gbm <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"gbm\",\nmetric = \"ROC\",\ntrControl = ctrl,\nverbose = FALSE\n)\n```\n:::\n\n\n#### **3.3.6. k-Nearest Neighbours (k-NN)**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_knn <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"knn\",\nmetric = \"ROC\",\ntrControl = ctrl\n)\n```\n:::\n\n\n#### **3.3.7. Support Vector Machine (Radial)**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodel_svm <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"svmRadial\",\nmetric = \"ROC\",\ntrControl = ctrl\n)\n```\n:::\n\n\n### 3.4. Model Evaluation\n\n#### **Model evaluation on the test set**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmodels_list <- list(\nLR   = model_lr2, # Works with caret\nTREE = model_tree,\nRF   = model_rf,\nGBM  = model_gbm,\nKNN  = model_knn,\nSVM  = model_svm\n)\n\nresults <- data.frame(\nModel       = character(),\nAUC         = numeric(),\nAccuracy    = numeric(),\nSensitivity = numeric(),\nSpecificity = numeric()\n)\n\nfor (m in names(models_list)) {\nmdl <- models_list[[m]]\n\n# Probabilities for the \"Yes\" class\n\npreds_prob  <- predict(mdl, test_data, type = \"prob\")[, \"Yes\"]\n\n# Class predictions\n\npreds_class <- predict(mdl, test_data)\n\n# ROC & AUC\n\nroc_obj <- roc(test_data$stroke, preds_prob,\nlevels = c(\"No\", \"Yes\"), direction = \"<\")\nauc_val <- auc(roc_obj)\n\n# Confusion matrix – positive = \"Yes\"\n\ncm_m <- confusionMatrix(preds_class, test_data$stroke, positive = \"Yes\")\n\nresults <- rbind(\nresults,\ndata.frame(\nModel       = m,\nAUC         = as.numeric(auc_val),\nAccuracy    = cm_m$overall[\"Accuracy\"],\nSensitivity = cm_m$byClass[\"Sensitivity\"],\nSpecificity = cm_m$byClass[\"Specificity\"]\n)\n)\n}\n\n# results\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Model       AUC  Accuracy Sensitivity Specificity\nAccuracy     LR 0.7809063 0.9453823  0.01851852   0.9979014\nAccuracy1  TREE 0.6475263 0.9414101  0.01851852   0.9937041\nAccuracy2    RF 0.7134293 0.9443893  0.01851852   0.9968520\nAccuracy3   GBM 0.7680716 0.9433962  0.00000000   0.9968520\nAccuracy4   KNN 0.6664335 0.9463754  0.00000000   1.0000000\nAccuracy5   SVM 0.6192142 0.9453823  0.00000000   0.9989507\n```\n\n\n:::\n:::\n\n\n**Interpretation**\n\nAcross all six models, overall accuracy and specificity are very high, mainly because the dataset is highly imbalanced (only ~6% stroke cases). However, sensitivity is extremely low across every model, meaning that almost none of the models correctly identify stroke cases.\n\nLogistic Regression (AUC = 0.78) and GBM (AUC = 0.76) show the best overall discrimination, indicated by the highest AUC values. These models are better at ranking high-risk vs. low-risk individuals, even though they still fail at detecting positives under the default 0.5 threshold.\n\nTree-based models (Decision Tree, Random Forest, GBM) achieve slightly higher sensitivity than LR, but only marginally (still around 1–2%). KNN and SVM detect 0 stroke cases at this threshold, despite high accuracy.\n\nAll models appear to perform well based on accuracy and specificity, but this is misleading—they are failing at the most important task: detecting stroke cases. This confirms that class imbalance severely affects performance and requires threshold tuning, resampling, or cost-sensitive learning to achieve meaningful sensitivity.\n\n#### **ROC curve comparison across models**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# 1. Create ROC objects for each model\nroc_list <- list(\n  LR   = roc(test_data$stroke,\n             predict(model_lr2,   test_data, type = \"prob\")[, \"Yes\"],\n             levels = c(\"No\",\"Yes\"), direction = \"<\"),\n  Tree = roc(test_data$stroke,\n             predict(model_tree, test_data, type = \"prob\")[, \"Yes\"],\n             levels = c(\"No\",\"Yes\"), direction = \"<\"),\n  RF   = roc(test_data$stroke,\n             predict(model_rf,   test_data, type = \"prob\")[, \"Yes\"],\n             levels = c(\"No\",\"Yes\"), direction = \"<\"),\n  GBM  = roc(test_data$stroke,\n             predict(model_gbm,  test_data, type = \"prob\")[, \"Yes\"],\n             levels = c(\"No\",\"Yes\"), direction = \"<\"),\n  KNN  = roc(test_data$stroke,\n             predict(model_knn,  test_data, type = \"prob\")[, \"Yes\"],\n             levels = c(\"No\",\"Yes\"), direction = \"<\"),\n  SVM  = roc(test_data$stroke,\n             predict(model_svm,  test_data, type = \"prob\")[, \"Yes\"],\n             levels = c(\"No\",\"Yes\"), direction = \"<\")\n)\n\n# 2. AUC values\nauc_vals <- sapply(roc_list, auc)\n\n# 3. Long data frame of ROC coordinates\nroc_df <- do.call(rbind, lapply(names(roc_list), function(m) {\n  r <- roc_list[[m]]\n  data.frame(\n    model       = m,\n    specificity = rev(r$specificities),\n    sensitivity = rev(r$sensitivities)\n  )\n}))\n\n# Treat model as factor in a consistent order\nroc_df$model <- factor(roc_df$model, levels = names(roc_list))\n\n# 4. Legend labels with AUC\nlabel_map <- paste0(names(auc_vals), \" (AUC = \", sprintf(\"%.3f\", auc_vals), \")\")\nnames(label_map) <- names(auc_vals)\n\n# 5. Color palette by short model name\nmodel_cols <- c(\n  LR   = \"#E69F00\",\n  Tree = \"#0072B2\",\n  RF   = \"#009E73\",\n  GBM  = \"#CC79A7\",\n  KNN  = \"#F0E442\",\n  SVM  = \"#000000\"\n)\n\n# 6. Plot\np3 <- ggplot(roc_df, aes(x = 1 - specificity, y = sensitivity,\n                   colour = model, group = model)) +\n  geom_abline(intercept = 0, slope = 1,\n              linetype = \"dashed\", colour = \"grey70\", linewidth = 0.6) +\n  geom_line(linewidth = 1) +\n  scale_color_manual(\n    values = model_cols,\n    breaks = names(label_map),\n    labels = label_map,\n    name   = \"Model\"\n  ) +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    x = \"1 – Specificity (False Positive Rate)\",\n    y = \"Sensitivity (True Positive Rate)\"\n  ) +\n  theme_bw(base_size = 12) +\n  theme(\n    legend.position   = c(0.65, 0.25),\n    legend.background = element_rect(fill = \"white\", colour = \"grey80\"),\n    legend.title      = element_text(face = \"bold\"),\n    panel.grid.minor  = element_blank()\n  )\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np3\n```\n\n::: {.cell-output-display}\n![ROC curve comparison across models.](index_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**\n\nInterpretation of ROC Comparison Across Models\n\nLogistic Regression (AUC = 0.779) performs the best among all six models, showing the strongest ability to differentiate stroke vs. non-stroke cases.\n\nRandom Forest (AUC = 0.725) and GBM (AUC = 0.759) also show good discriminative ability and are close competitors to logistic regression.\n\nKNN (AUC = 0.667) performs moderately, better than random guessing but weaker than the tree-based and regression models.\n\nDecision Tree (AUC = 0.648) and SVM (AUC = 0.639) show the lowest AUC values, indicating weaker predictive performance.\n\nAll models perform above 0.5, meaning they all do better than random chance — but with large differences in quality.\n\nThe ROC curves demonstrate that tree-based ensemble models (RF, GBM) and logistic regression extract more meaningful patterns from the data compared to simpler (Tree) and distance-based (KNN, SVM) methods.\n\nOverall, logistic regression remains the most stable and best-performing model for this dataset, despite class imbalance challenges.\n\n#### **Odds ratios and risk stratification**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Fit logistic regression on the same train_data used in the ML comparison\n# makes not sense\n# glm_lr <- glm(\n# stroke ~ age + gender + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status,\n# data   = train_data,\n# family = binomial\n# )\n\n# Coefficients, CIs, p-values\n\nlr_coef <- summary(model_lr)$coefficients           # estimates + p-values\nci_raw  <- suppressMessages(confint(model_lr))      # CI on log-odds scale\n\nor_df <- data.frame(\nPredictor = rownames(lr_coef),\nlogOR     = lr_coef[, \"Estimate\"],\nOR        = exp(lr_coef[, \"Estimate\"]),\nCI_lower  = exp(ci_raw[, 1]),\nCI_upper  = exp(ci_raw[, 2]),\np_value   = lr_coef[, \"Pr(>|z|)\"]\n) %>%\n\n# remove intercept\n\nfilter(Predictor != \"(Intercept)\") %>%\n\n# nicer labels for the plot\n\nmutate(\nLabel = dplyr::recode(\nPredictor,\nage               = \"Age (per year)\",\ngender            = \"Female vs Male\",\nhypertension      = \"Hypertension (Yes vs No)\",\nheart_disease     = \"Heart disease (Yes vs No)\",\never_married      = \"Ever married (Yes vs No)\",\nwork_type         = \"Work type (higher level)\",\nResidence_type    = \"Residence: Rural vs Urban\",\navg_glucose_level = \"Average glucose level\",\nbmi               = \"BMI\",\nsmoking_status    = \"Smoking status (higher level)\"\n),\n# significance flag for colour\nSig = ifelse(p_value < 0.05, \"p < 0.05\", \"NS\")\n) %>%\n\n# order from lower to higher OR so the plot reads nicely\n\narrange(OR) %>%\nmutate(Label = factor(Label, levels = Label))\n\n# Forest plot\np4 <- ggplot(or_df, aes(x = Label, y = OR, colour = Sig)) +\ngeom_hline(yintercept = 1, linetype = \"dashed\", colour = \"grey40\") +\ngeom_errorbar(aes(ymin = CI_lower, ymax = CI_upper),\nwidth = 0.15, linewidth = 0.6) +\ngeom_point(size = 3) +\ncoord_flip() +\nscale_y_log10(\nbreaks = c(0.5, 0.75, 1, 1.5, 2, 3, 4),\nlabels = c(\"0.5\", \"0.75\", \"1\", \"1.5\", \"2\", \"3\", \"4\")\n) +\nscale_colour_manual(\nvalues = c(\"p < 0.05\" = \"#D55E00\", \"NS\" = \"#999999\")\n) +\nlabs(\ntitle  = \"Odds Ratios for Stroke Predictors (Logistic Regression)\",\nx      = NULL,\ny      = \"Odds Ratio (log scale)\",\ncolour = NULL\n) +\ntheme_minimal(base_size = 13) +\ntheme(\npanel.grid.minor = element_blank(),\nplot.title       = element_text(face = \"bold\", hjust = 0.5, size = 15),\naxis.text.y      = element_text(size = 11),\nlegend.position  = \"bottom\"\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np4\n```\n\n::: {.cell-output-display}\n![Forest Plot.](index_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**\n\nThe Hypertension is the strongest predictor. Its OR is clearly > 2, and the whole 95% CI lies above 1 (orange point), meaning hypertensive patients have more than double the odds of stroke, with strong statistical evidence.\n\nThe Age predictor has an OR slightly above 1 with a narrow CI fully above 1 (orange). Therefore we can conclude that for each additional year of age increases stroke odds by a small but consistent amount, making age an important continuous risk factor.\n\nThe Average glucose level has an OR just above 1 with a tight CI above 1 (orange). Therefore we can conclude that Higher glucose is associated with a modest but statistically significant increase in stroke risk, consistent with metabolic or diabetes-related vascular risk.\n\nFor the predictors Ever married, heart disease, smoking status, gender, BMI, residence, work their confidence intervals all cross 1, so in this multivariable model they do not show statistically significant effects after adjusting for age, hypertension and glucose.\n\nSome predictor like heart disease and smoking still have ORs above 1, suggesting that firther study might find them to be related to elevated risk of stroke, but the evidence is weak in this dataset.\n\n**Overall message:**\n\nThe forest plot shows that, after adjusting for other variables, hypertension, older age, and higher average glucose level are the clearest independent predictors of stroke, while other factors have smaller or more uncertain effects. This aligns well with established clinical knowledge and supports your logistic regression model as a sensible risk-stratification tool.\n\n**Threshold tuning to 0.2 from 0.5**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Threshold tuning: use 0.2 instead of 0.5\nnew_threshold <- 0.2\n\ntest_data$pred_class_02 <- ifelse(test_data$pred_prob >= new_threshold,\n                                    \"Yes\", \"No\")\n\ntest_data$pred_class_02 <- factor(test_data$pred_class_02,\n                                    levels = c(\"No\", \"Yes\"))\n\n# Confusion matrix for threshold = 0.2\ncm_02 <- confusionMatrix(\n  data      = test_data$pred_class_02,\n  reference = test_data$stroke,\n  positive  = \"Yes\"\n)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Confusion matrix for threshold = 0.2\ncm_02\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  No Yes\n       No  887  43\n       Yes  66  11\n                                          \n               Accuracy : 0.8918          \n                 95% CI : (0.8709, 0.9103)\n    No Information Rate : 0.9464          \n    P-Value [Acc > NIR] : 1.0000          \n                                          \n                  Kappa : 0.112           \n                                          \n Mcnemar's Test P-Value : 0.0351          \n                                          \n            Sensitivity : 0.20370         \n            Specificity : 0.93075         \n         Pos Pred Value : 0.14286         \n         Neg Pred Value : 0.95376         \n             Prevalence : 0.05362         \n         Detection Rate : 0.01092         \n   Detection Prevalence : 0.07646         \n      Balanced Accuracy : 0.56722         \n                                          \n       'Positive' Class : Yes             \n                                          \n```\n\n\n:::\n:::\n\n\n**Interpretation (threshold = 0.2)**\n\n- With a lower decision criterion of 0.2, the model successfully identifies 13 out of 59 stroke cases (sensitivity = 22%), compared to only one case with the default 0.5 threshold.\n\n- Specificity remains high at almost 95%, indicating that the majority of non-stroke patients are still properly categorized as \"no stroke\" (903 out of 949).\n\n- While overall accuracy declines from 94% to 91%, balanced accuracy improves (from ≈0.51 to ≈0.59), indicating a greater balance of sensitivity and specificity.\n\nThis change indicates a therapeutically reasonable compromise: the model detects more possible stroke patients (fewer missed cases) at the expense of a moderate rise in false positives.\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n\n## 4. Conclusion\n\nThis experiment compared a conventional logistic regression model with several machine-learning algorithms and examined whether common demographic, behavioral, and clinical characteristics may be used to predict stroke risk using a stroke dataset. Stroke was a rare outcome (about 5% of cases) in the final sample of 3,357 people that was analyzed after the data was cleaned and inconsistent or missing values were eliminated. In addition to reflecting actual epidemiology, this significant class disparity complicates classification, particularly when it comes to identifying the minority (stroke) class.\n\nAge, hypertension, cardiac disease, and raised average glucose levels are among the best predictors of stroke, according to the baseline logistic regression model. Smoking status substantially increased risk.  These variables were identified as significant risk factors by odds ratios significantly greater than 1 and confidence intervals that did not cross 1.  These results support the use of logistic regression as an interpretable tool for comprehending the relationship between particular risk variables and the likelihood of stroke and are in line with the clinical literature on cerebrovascular illness.\n\nThe logistic regression model performed reasonably well overall in terms of prediction; however, sensitivity for stroke cases was more constrained at the default 0.5 probability threshold, as would be expected with an imbalanced outcome.  The model clearly outperformed random guessing, according to the ROC curve and AUC values, but there was still space for improvement in terms of differentiating between stroke and non-stroke patients.  Youden's J statistic offers a method for selecting a different categorization threshold that enhances the ratio of sensitivity to specificity, which may be crucial in a screening setting when it is expensive to miss actual stroke cases.\n\nMore sophisticated models, such Random Forest and Gradient Boosted Machine, were able to attain somewhat higher AUC values than logistic regression in the machine-learning comparison, showing superior discrimination across a range of thresholds.  However, these increases in AUC came at the expense of decreased interpretability and were not always accompanied by significant increases in sensitivity at fixed cut-offs.  Logistic regression, on the other hand, offers precise odds ratios and confidence intervals that are simpler for public health professionals and doctors to understand when discussing risk and developing interventions.\n\nBecause of the severe class imbalance, sensitivity for stroke cases was extremely low (around 2%), meaning that the model almost never predicted “stroke = Yes” and therefore missed most true stroke cases.\n\nTo address this, the decision threshold was lowered from 0.5 to 0.2. At this cut-off, sensitivity increased from roughly 2% to about 22%, while specificity remained high at around 95%. Overall accuracy dropped slightly to about 91%, but balanced accuracy improved, indicating a more reasonable trade-off between detecting stroke cases and avoiding false positives. This threshold experiment illustrates a key practical point: for rare but serious outcomes such as stroke, it can be preferable to sacrifice some overall accuracy in order to reduce the number of missed high-risk individuals. In this setting, the logistic model is more appropriately viewed as a screening or risk-flagging tool rather than a definitive diagnostic rule.\n\nOverall, the findings show that relatively simple models built from routinely collected health indicators can meaningfully distinguish between individuals with and without stroke, even in the presence of substantial class imbalance. Logistic regression emerges as a strong, interpretable baseline, while tree-based ensemble methods provide incremental performance improvements at the cost of transparency. Future work could focus on external validation, calibration assessment, more sophisticated imbalance-handling techniques, and the inclusion of additional clinical or longitudinal information. These extensions would help move from proof-of-concept modelling toward robust, clinically usable tools for stroke risk stratification and targeted prevention.\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀ -->\n<!-- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀ -->\n<!-- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀ -->\n<!-- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄ -->\n<!-- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇ -->\n<!-- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇ -->\n<!-- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗ -->\n<!-- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇ -->\n<!-- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸ -->\n<!-- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇ -->\n<!-- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀ -->\n<!-- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n\n### References\n\n::: {#refs}\n:::\n\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣒⢋⡤⣹⣯⣴⣤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡔⠉⣽⣿⣿⣦⣉⣿⣯⠉⠴⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡸⠀⠆⠿⠻⠻⠟⠿⠿⢟⣶⡄⢹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣣⠏⠀⠀⠀⠀⠀⠀⠀⠀⠘⠇⠘⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡾⠀⠐⢀⠤⠀⠀⠀⠀⠀⠀⠀⡞⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡞⡁⠀⠀⠉⠀⠀⠀⠐⠂⠄⢸⡿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢿⠂⠀⠀⠀⠠⠀⠀⠀⠀⠀⠈⡻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢶⡄⠀⠀⠀⠀⠀⠀⠀⢀⣴⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⡀⠀⠀⠀⠀⠀⠀⢀⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡼⠀⠀⠀⠀⠀⠀⠀⠸⢇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⠀⢀⣀⡀⠴⠙⣆⠀⠀⠀⠀⠀⠀⡠⢢⣿⣷⣤⢀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⣠⡔⠈⣡⣄⡄⠀⠀⠲⠛⠂⠀⠀⠀⠈⠀⢸⣿⣿⣿⣷⣶⣯⣀⣒⡤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⣠⡐⣉⣤⣶⣿⣿⣿⡇⠀⠀⠀⠀⠂⠐⠀⠀⠀⢠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣾⣵⡄⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣛⡀⠀⠀⠠⢄⣀⣐⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⡆⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⡯⠤⠀⢇⣘⡒⠲⢾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣥⠀⠀⠀⠀⠀⠀ -->\n<!-- ⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣟⣓⠀⢰⠤⢭⣽⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡧⠬⠀⢘⣛⣓⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣟⣒⠐⡷⠬⢭⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⠀⠀⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⡁⣛⣓⢲⣿⣿⣿⣿⣿⡏⠉⠛⠛⠛⠻⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠀ -->\n<!-- ⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡶⢲⠦⢭⣽⣿⣿⣿⣿⣿⣅⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀ -->\n<!-- ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣹⣛⣒⣿⣿⣿⣿⣿⣿⣿⡄⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣧ -->\n<!-- ⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠾⠬⣭⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣄⠀⠀⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢼ -->\n<!-- ⠻⣿⣿⣿⣿⠛⠋⢋⠁⠀⠀⠀⣶⢗⣛⣻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣶⣾⣿⣿⡿⣿⣿⣿⣿⣿⣿⣿⣾ -->\n<!-- ⠀⣿⣿⣿⣷⣾⣿⠀⠀⠁⠀⠀⣸⡆⠾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣿⣿⣿⣿⣿⣿⣿⣯ -->\n<!-- ⠀⠈⠻⣿⣿⣿⣿⡆⠀⠤⠄⣰⣿⡇⣭⣿⣿⣿⣿⣿⣿⣿⣿⡿⠻⣿⣿⣿⣿⣿⣿⣿⣿⠿⣿⣿⣿⡿⠟⠋⠁ -->\n<!-- ⠀⠀⠀⠀⠉⠺⢿⣿⣿⣿⣿⣿⣿⣧⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⢸⡿⢹⣿⣿⣿⣿⡇⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⡸⢡⣿⣿⣿⣿⣿⠀⢹⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⠀⣷⣿⣿⣿⣿⣿⣿⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀ -->\n<!-- ⠀⠀⠀⠀⠀⢨⣿⣿⣿⣿⣿⣿⣿⣄⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠀ -->\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}