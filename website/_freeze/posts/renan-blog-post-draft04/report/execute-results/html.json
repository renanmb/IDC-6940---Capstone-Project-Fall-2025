{
  "hash": "0319c9a4d0a7dd0bbc9201e48a11a56e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting stroke risk from common health indicators: a binary logistic regression analysis\"\ndescription: \"Draft 04 for Final report\"\nauthor:\n  - \"Shree Krishna M.S Basnet\"\n  - \"Renan\"\n  - \"Supervisor: Dr. Cohen\"\ndate: \"December 2 2025\"\n# format: \n#   html:\n#     theme: cosmo\n#     toc: true\n#     toc-float: true\n#     code-fold: false\n#     self-contained: true\nnumber-sections: true\nbibliography: references.bib\nlink-citations: true\n# csl: apa-numeric-superscript-brackets.csl\ncategories: [drafts, renan]\nimage: images/spongebob-imagination.jpg\ndraft: false\n---\n\n\n\n\n\n# Introduction\n\nStroke is one of the leading causes of death and disability worldwide and remains a major public health challenge [@WHO2025]. Because stroke often occurs suddenly and can result in long-term neurological impairment, early identification of individuals at elevated risk is critical for prevention and timely intervention. Data-driven risk prediction models enable clinicians and public health professionals to quantify individual-level risk and to target high-risk groups for lifestyle counselling and clinical management.\n\nLogistic Regression (LR) is one of the most widely used approaches for modelling binary outcomes such as disease presence or absence [@sperandei2014understanding]. It extends linear regression to cases where the outcome is categorical and provides interpretable coefficients and odds ratios that describe how each predictor is associated with the probability of the event. LR has been applied across a wide range of domains, including child undernutrition and anaemia [@asmare2024determinants], road traffic safety [@rahman2021identification; @chen2024binary; @chen2020modeling], health-care utilisation and clinical admission decisions [@hutchinson2023predictors], and fraud detection [@samara2024using]. These applications highlight both the flexibility of LR and its suitability for real-world decision-making problems.\n\nIn this project, we analyse a publicly available stroke dataset that includes key demographic, behavioural, and clinical predictors such as age, gender, hypertension status, heart disease, marital status, work type, residence type, smoking status, body mass index (BMI), and average glucose level. These variables are commonly reported in the stroke and cardiovascular literature as important determinants of risk. Using this dataset, we first clean and recode the variables into appropriate numeric formats and then develop a series of supervised learning models for stroke prediction.\n\nLogistic Regression is used as the primary, interpretable baseline model, but its performance is compared against several more complex machine-learning techniques, including Decision Tree, Random Forest, Gradient Boosted Machine, k-Nearest Neighbours, and Support Vector Machine (radial). Model performance is evaluated using accuracy, sensitivity, specificity, ROC curves, AUC, and confusion matrices. The main objectives are to identify the most influential predictors of stroke and to determine whether advanced machine-learning models offer meaningful improvements over Logistic Regression for classification of stroke risk in this dataset.\n\n# Methodology\n\nWe use this study Using clinical and demographic data, this project aims to develop and evaluate a number of supervised machine-learning models for binary stroke prediction.  The dataset, preprocessing procedure, model development approach, and mathematical formulation of logistic regression‚Äîwhich is the main analytical model because of its interpretability and proven application in medical research‚Äîare all covered in this section [@sperandei2014understanding; @hosmer2013applied].\n\n## Dataset and visualization \n\nWe used stroke dataset containing 5,110 observations and 11 predictors commonly associated with cerebrovascular risk. After cleaning missing and inconsistent entries, a final dataset of 3,357 individuals remained for analysis. The dataset includes demographic, behavioral, and clinical indicators widely used in stroke-risk modeling.\n\n###Variables\n\nThe key predictors are listed below.\n\n| Variable              | Type                           | Description                   |\n| --------------------- | ------------------------------ | ----------------------------- |\n| **age**               | Numeric                        | Age of the individual (years) |\n| **gender**            | Categorical (1=Male, 2=Female) | Biological sex                |\n| **hypertension**      | Binary (0/1)                   | Prior hypertension diagnosis  |\n| **heart_disease**     | Binary (0/1)                   | Presence of heart disease     |\n| **ever_married**      | Binary                         | Marital status                |\n| **work_type**         | Categorical (1‚Äì4)              | Employment category           |\n| **Residence_type**    | Binary (1=Urban, 2=Rural)      | Place of residence            |\n| **smoking_status**    | Categorical                    | Never/Former/Smokes           |\n| **bmi**               | Numeric                        | Body Mass Index               |\n| **avg_glucose_level** | Numeric                        | Average glucose level         |\n| **stroke**            | Binary outcome (0=No, 1=Yes)   | Stroke occurrence             |\n\nStroke is a highly unbalanced outcome variable:\n- Yes (stroke): about 5%\n- No (no stroke): around 95%\n\nBias-corrected logistic regression methods and ROC-based model evaluation are justified by this imbalance.\n\n\n# Dataset Prepration\nTo guarantee model validity and stop data leakage, data preprocessing adhered to conventional clinical-analytics protocols [@wang2014].\n\nAmong the steps were:\n\n- Elimination of non-predictive identifiers (patient ID)\n\n- Transforming categorical variables into dummy numerical representations\n\n- Managing uncommon or irregular categories (e.g., \"Other\" gender values handled as absent)\n\n- BMI, glucose, and age conversion to numerical\n\n- Rows with unintelligible labels (\"Unknown,\" \"N/A\") are removed.\n\n- Valid range and consistency verification\n\n- After recoding, missing values might be imputed or removed.\n\n- During splitting, stratified sampling is used to maintain the stroke/no-stroke ratio [@chen2020modeling].\n\nThe objective of preprocessing was to generate a cleansed dataset appropriate for binary classification, even if the Analysis section presents thorough summaries (distributions, histograms, and outlier checks).\n\n# Train‚ÄìTest Split and Cross-Validation\nThe model was trained using 70% of the data.\n\nThirty percent of the data was kept for the last assessment.\n\nIn accordance with best practices for limited or unbalanced clinical datasets, repeated 5-fold cross-validation was utilized to adjust hyperparameters and lower variance [@zhang2020descriptive].\n\n\n\n# Machine-Learning Models Compared\n2.6 Machine-Learning Models Compared\n\nUsing the caret framework, six supervised models were trained for comparison:\n\nLogistic Regression\n\nDecision Tree\n\nRandom Forest\n\nGradient Boosted Machine (GBM)\n\nk-Nearest Neighbors (k-NN)\n\nSupport Vector Machine (Radial Kernel)\n\nEach model used the same cross-validation scheme and the same train/test split to guarantee fair comparison.\n\n# Evaluation Metrics\nModels were evaluated using standard clinical classification metrics:\n\nAccuracy\n\nSensitivity (Recall)\n\nSpecificity\n\nPrecision\n\nF1-Score\n\nReceiver Operating Characteristic (ROC) curve\n\nArea Under the Curve (AUC)\n\nYouden‚Äôs J Statistic\n\nUsed to determine optimal classification threshold:\n\nùêΩ\n=\nSensitivity\n+\nSpecificity\n‚àí\n1\nJ=Sensitivity+Specificity‚àí1\n\nThese metrics are widely used in stroke-risk modeling literature [@chen2020modeling].\n\n\n\n# Analysis\n\nBefore developing predictive models, an exploratory analysis was conducted to understand the distribution, structure, and relationships within the cleaned dataset (N = 3,357). This step is crucial in rare-event medical modeling because data imbalance, skewed predictors, or correlated variables can directly influence model behavior and classification performance.\n\n\n# Distribution of Key Continuous Variables\n\nHistograms were used to assess the spread of the primary numeric predictors (Age, BMI, and Average Glucose Level). These variables demonstrate clinically expected right-skewness, particularly glucose and BMI, consistent with published literature on metabolic and cardiovascular risk distributions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Histograms for key numeric variables\nlibrary(ggplot2)\n\np_age  <- ggplot(strokeclean, aes(age)) + geom_histogram(binwidth=5, fill=\"green\") +\n  labs(title=\"Age Distribution\", x=\"Age\", y=\"Count\")\n\np_bmi  <- ggplot(strokeclean, aes(bmi)) + geom_histogram(binwidth=2, fill=\"pink\") +\n  labs(title=\"BMI Distribution\", x=\"BMI\", y=\"Count\")\n\np_gluc <- ggplot(strokeclean, aes(avg_glucose_level)) + \n  geom_histogram(binwidth=10, fill=\"yellow\") +\n  labs(title=\"Average Glucose Level\", x=\"Glucose Level\", y=\"Count\")\n\nggpubr::ggarrange(p_age, p_bmi, p_gluc, ncol=3)\n```\n\n::: {.cell-output-display}\n![](report_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\nInterpretation\n\nAge shows a broad distribution with a natural right tail, indicating more older adults.\n\nBMI has moderate right-skewness, consistent with obesity patterns in clinical datasets.\n\nGlucose level shows strong right-skewness, highlighting metabolic risk differences across individuals.\n\nThese patterns match clinical expectations for populations at risk of cardiovascular complications.\n\n# Distribution of Key Categorical Variables\n\nBar charts help visualize population composition. The dataset shows more females than males, a balanced rural‚Äìurban distribution, and substantial variation in work type and smoking behavior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bar charts for categorical variables\n\np_gender <- ggplot(strokeclean, aes(gender)) + geom_bar(fill=\"red\") +\nlabs(title=\"Gender Distribution\", x=\"Gender\", y=\"Count\")\n\np_smoke <- ggplot(strokeclean, aes(smoking_status)) + geom_bar(fill=\"blue\") +\nlabs(title=\"Smoking Status\", x=\"Smoking Category\", y=\"Count\")\n\np_res <- ggplot(strokeclean, aes(Residence_type)) + geom_bar(fill=\"green\") +\nlabs(title=\"Residence Type\", x=\"Residence\", y=\"Count\")\n\nggpubr::ggarrange(p_gender, p_smoke, p_res, ncol=3)\n```\n\n::: {.cell-output-display}\n![](report_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nInterpretation\n\nBar plots were created to visualize demographic and behavioral attributes.\nFemales are slightly more represented than males.\n\nSmoking status shows a large ‚Äúnever smoked‚Äù group.\n\nResidence type is roughly balanced between urban and rural households.\n\nThese patterns form the baseline population characteristics.\n\n\n# Correlation Heatmap\n\nThe associations between numerical predictors were assessed using a correlation heatmap. The variables exhibit low to moderate correlation, indicating that the assumptions of logistic regression are not broken. According to earlier stroke-epidemiology studies, age, hyperglycemia, and BMI exhibit the highest associations with stroke risk.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlation heatmap\n\nlibrary(ggcorrplot)\n\nnumeric_vars <- strokeclean[, c(\"age\",\"bmi\",\"avg_glucose_level\",\n\"hypertension\",\"heart_disease\")]\n\ncorr_matrix <- cor(numeric_vars)\n\nggcorrplot::ggcorrplot(corr_matrix,\nlab=TRUE,\ncolors=c(\"purple\",\"gold\",\"grey\"),\ntitle=\"Correlation Heatmap of Key Predictors\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\n‚Ñπ Please use tidy evaluation idioms with `aes()`.\n‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n‚Ñπ The deprecated feature was likely used in the ggcorrplot package.\n  Please report the issue at <https://github.com/kassambara/ggcorrplot/issues>.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](report_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nInterpretation\n\nCorrelations are generally low to moderate, indicating minimal multicollinearity.\n\nAge, glucose, and hypertension show noticeable associations ‚Äî consistent with their clinical relevance.\n\nThis supports the suitability of logistic regression.\n\n# Summary Interpretation\n\n- The distributions of age, BMI, and glucose are skewed to the right.\n\n- Unbalanced category proportions are shown in smoking status and occupational type.\n\n- The correlation levels are sufficiently low to prevent problems with multicollinearity.\n\n- These trends support the application of tree-based ensemble models and logistic regression.\n\nThe modeling approach that follows is guided by this EDA, which offers a basis for comprehending how each predictor might help to stroke categorization.\n\n# Model Development Approach\n\n- Distributions of age, BMI, and glucose are skewed to the right.\n\n- There are unequal category proportions for both work type and smoking status.\n\n- The correlation levels are low enough to prevent problems with multicollinearity.\n\n- The application of logistic regression and tree-based ensemble models is justified by these patterns.\n\nThe modeling approach that follows is guided by this EDA, which offers a basis for comprehending how each predictor may contribute to stroke classification.\n\n# Key Modeling Principles\n\n# Model Development Approach\n\nSix supervised machine-learning algorithms were trained to classify stroke vs. non-stroke cases. All models were implemented using the *caret* package with a consistent workflow to ensure fairness and prevent information leakage.\n\n#Key Modeling Principles\n- Train/Test Split: 70% training, 30% testing  \n- Cross-Validation: Repeated 5-fold CV optimized for ROC  \n- Outcome Variable: Stroke (1 = yes, 0 = no)  \n- Predictors: Age, gender, marital status, hypertension, heart disease, residence type, work type, smoking status, BMI, and average glucose  \n\n# Evaluation Metrics \n  - Accuracy  \n  - Sensitivity (Recall)  \n  - Specificity  \n  - ROC Curve  \n  - AUC (Area Under the Curve)  \n  - Confusion Matrix\n\nThis ensures a standardized and unbiased comparison across all six models.\n\n# Data Spitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_df <- strokeclean\nmodel_df <- na.omit(model_df)\nmodel_df$stroke <- factor(model_df$stroke)\nlevels(model_df$stroke) <- c(\"No\", \"Yes\")\ntable(model_df$stroke)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  No  Yes \n3177  180 \n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(123)\nindex <- createDataPartition(strokeclean$stroke, p = 0.70, list = FALSE)\ntrain_data <- strokeclean[index, ]\ntest_data  <- strokeclean[-index, ]\n\ntrain_data$stroke <- factor(train_data$stroke, levels = c(\"No\",\"Yes\"))\ntest_data$stroke  <- factor(test_data$stroke,  levels = c(\"No\",\"Yes\"))\n```\n:::\n\n# Train Control\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- trainControl(\nmethod = \"repeatedcv\",\nnumber = 5,\nrepeats = 3,\nclassProbs = TRUE,\nsummaryFunction = twoClassSummary,\nverboseIter = FALSE\n)\n```\n:::\n\n\n# Logestic Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_lr <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"glm\",\nfamily = \"binomial\",\nmetric = \"ROC\",\ntrControl = ctrl\n)\n```\n:::\n\n\n# Decision Tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_tree <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"rpart\",\nmetric = \"ROC\",\ntrControl = ctrl,\ntuneLength = 10\n)\n```\n:::\n\n\n# Random Forest\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_rf <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"rf\",\nmetric = \"ROC\",\ntrControl = ctrl,\ntuneLength = 5\n)\n```\n:::\n\n\n# Gradient Boosted Machine\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_gbm <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"gbm\",\nmetric = \"ROC\",\ntrControl = ctrl,\nverbose = FALSE\n)\n```\n:::\n\n\n# k-Nearest Neighbors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_knn <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"knn\",\nmetric = \"ROC\",\ntrControl = ctrl\n)\n```\n:::\n\n\n# Support Vector Machine\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_svm <- train(\nstroke ~ .,\ndata = train_data,\nmethod = \"svmRadial\",\nmetric = \"ROC\",\ntrControl = ctrl\n)\n```\n:::\n\n\n# Evaluation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels_list <- list(\n  LR   = model_lr,\n  TREE = model_tree,\n  RF   = model_rf,\n  GBM  = model_gbm,\n  KNN  = model_knn,\n  SVM  = model_svm\n)\n\nlibrary(pROC)\n\nresults <- data.frame(\n  Model       = character(),\n  AUC         = numeric(),\n  Accuracy    = numeric(),\n  Sensitivity = numeric(),\n  Specificity = numeric()\n)\n\nfor (m in names(models_list)) {\n  mdl <- models_list[[m]]\n\n  # Probabilities for the \"Yes\" class\n  preds_prob  <- predict(mdl, test_data, type = \"prob\")[, \"Yes\"]\n  # Class predictions\n  preds_class <- predict(mdl, test_data)\n\n  # ROC & AUC (explicitly say which level is \"event\")\n  roc_obj <- roc(test_data$stroke, preds_prob,\n                 levels = c(\"No\", \"Yes\"), direction = \"<\")\n  auc_val <- auc(roc_obj)\n\n  # Confusion matrix ‚Äì positive = \"Yes\"\n  cm <- confusionMatrix(preds_class, test_data$stroke, positive = \"Yes\")\n\n  results <- rbind(\n    results,\n    data.frame(\n      Model       = m,\n      AUC         = as.numeric(auc_val),\n      Accuracy    = cm$overall[\"Accuracy\"],\n      Sensitivity = cm$byClass[\"Sensitivity\"],\n      Specificity = cm$byClass[\"Specificity\"]\n    )\n  )\n}\n\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Model       AUC  Accuracy Sensitivity Specificity\nAccuracy     LR 0.7793712 0.9433962  0.00000000   0.9968520\nAccuracy1  TREE 0.6475263 0.9414101  0.01851852   0.9937041\nAccuracy2    RF 0.7250496 0.9463754  0.00000000   1.0000000\nAccuracy3   GBM 0.7592884 0.9433962  0.00000000   0.9968520\nAccuracy4   KNN 0.6668998 0.9463754  0.00000000   1.0000000\nAccuracy5   SVM 0.6390929 0.9414101  0.00000000   0.9947534\n```\n\n\n:::\n:::\n\n\n# ROC Curve Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ROC objects for each model\nroc_lr   <- roc(test_data$stroke,\n                predict(model_lr,   test_data, type = \"prob\")[, \"Yes\"],\n                levels = c(\"No\", \"Yes\"), direction = \"<\")\n\nroc_tree <- roc(test_data$stroke,\n                predict(model_tree, test_data, type = \"prob\")[, \"Yes\"],\n                levels = c(\"No\", \"Yes\"), direction = \"<\")\n\nroc_rf   <- roc(test_data$stroke,\n                predict(model_rf,   test_data, type = \"prob\")[, \"Yes\"],\n                levels = c(\"No\", \"Yes\"), direction = \"<\")\n\nroc_gbm  <- roc(test_data$stroke,\n                predict(model_gbm,  test_data, type = \"prob\")[, \"Yes\"],\n                levels = c(\"No\", \"Yes\"), direction = \"<\")\n\nroc_knn  <- roc(test_data$stroke,\n                predict(model_knn,  test_data, type = \"prob\")[, \"Yes\"],\n                levels = c(\"No\", \"Yes\"), direction = \"<\")\n\nroc_svm  <- roc(test_data$stroke,\n                predict(model_svm,  test_data, type = \"prob\")[, \"Yes\"],\n                levels = c(\"No\", \"Yes\"), direction = \"<\")\n\n# Plot all ROC curves\nplot(roc_lr,   col = \"red\",       main = \"ROC Comparison for Six Models\")\nplot(roc_tree, col = \"blue\",      add = TRUE)\nplot(roc_rf,   col = \"darkgreen\", add = TRUE)\nplot(roc_gbm,  col = \"purple\",    add = TRUE)\nplot(roc_knn,  col = \"orange\",    add = TRUE)\nplot(roc_svm,  col = \"black\",     add = TRUE)\n\nlegend(\n  \"bottomright\",\n  legend = c(\"LR\", \"Tree\", \"RF\", \"GBM\", \"KNN\", \"SVM\"),\n  col    = c(\"red\", \"blue\", \"darkgreen\", \"purple\", \"orange\", \"black\"),\n  lwd    = 2\n)\n```\n\n::: {.cell-output-display}\n![](report_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n# Odds ratio Forest plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_lr <- glm(\nstroke ~ age + gender + hypertension + heart_disease + ever_married +\nwork_type + Residence_type + avg_glucose_level + bmi + smoking_status,\ndata = train_data,\nfamily = binomial\n)\n```\n:::\n\n\n# Coefficient table\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlr_coef <- summary(glm_lr)$coefficients \n```\n:::\n\n\n\n# Odds ratios = exp(beta)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nor_vals <- exp(lr_coef[, \"Estimate\"]) \n\nci_raw <- suppressMessages(confint(glm_lr)) # CI on log-odds scale\nci_or <- exp(ci_raw) # convert to OR scale\n\nplot_df <- data.frame(\nPredictor = rownames(lr_coef),\nOR = or_vals,\nCI_lower = ci_or[, 1],\nCI_upper = ci_or[, 2]\n)\n\n\n\nplot_df <- subset(plot_df, Predictor != \"(Intercept)\")\n\nlibrary(ggplot2)\n\nggplot(plot_df, aes(x = reorder(Predictor, OR), y = OR)) +\ngeom_point(size = 3, color = \"red\") +\ngeom_errorbar(aes(ymin = CI_lower, ymax = CI_upper), width = 0.2) +\ncoord_flip() +\nlabs(\ntitle = \"Odds Ratios for Stroke Predictors (Logistic Regression)\",\ny = \"Odds Ratio (log scale)\",\nx = \"\"\n) +\nscale_y_log10() +\ngeom_hline(yintercept = 1, linetype = \"dashed\") +\ntheme_minimal()\n```\n\n::: {.cell-output-display}\n![](report_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot_df[order(-plot_df$OR), ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                          Predictor        OR  CI_lower CI_upper\nhypertension           hypertension 2.5131891 1.6440071 3.805210\never_married           ever_married 1.7004830 0.9349337 2.962840\nheart_disease         heart_disease 1.4377058 0.8211286 2.436252\nsmoking_status       smoking_status 1.1992887 0.9295752 1.541541\ngender                       gender 1.1098927 0.7473016 1.661886\nage                             age 1.0806359 1.0634477 1.099421\navg_glucose_level avg_glucose_level 1.0054667 1.0021688 1.008742\nbmi                             bmi 1.0027081 0.9721369 1.032857\nResidence_type       Residence_type 0.9328980 0.6313830 1.374861\nwork_type                 work_type 0.8378524 0.6216945 1.131027\n```\n\n\n:::\n:::\n\n\n# stroke rateof top preadators \n\n# hypertension and Stoke\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(strokeclean, aes(x = hypertension, fill = stroke)) +\ngeom_bar(position = \"fill\") +\nscale_y_continuous(labels = scales::percent) +\nlabs(\ntitle = \"Stroke Rate by Hypertension Status\",\nx = \"Hypertension (0 = No, 1 = Yes)\",\ny = \"Percentage\"\n) +\nscale_fill_manual(values = c(\"No\" = \"yellow\", \"Yes\" = \"red\")) +\ntheme_minimal()\n```\n\n::: {.cell-output-display}\n![](report_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nInterpretation:\n\nHypertensive individuals have strongly elevated stroke risk.\n\n# heart deasese and stroke\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(strokeclean, aes(x = heart_disease, fill = stroke)) +\ngeom_bar(position = \"fill\") +\nscale_y_continuous(labels = scales::percent) +\nlabs(\ntitle = \"Stroke Rate by Heart Disease\",\nx = \"Heart Disease (0/1)\",\ny = \"Percentage\"\n) +\nscale_fill_manual(values = c(\"No\" = \"pink\", \"Yes\" = \"green\")) +\ntheme_minimal()\n```\n\n::: {.cell-output-display}\n![](report_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nInterpretation:\n\nIndividuals with heart disease show 4‚Äì5√ó higher stroke rate.\n\n# Smoking and Stroke\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(strokeclean, aes(x = smoking_status, fill = stroke)) +\ngeom_bar(position = \"fill\") +\nscale_y_continuous(labels = scales::percent) +\nlabs(\ntitle = \"Stroke Rate by Smoking Behavior\",\nx = \"1=Never, 2=Former, 3=Smokes\",\ny = \"Percentage\"\n) +\nscale_fill_manual(values = c(\"No\" = \"grey70\", \"Yes\" = \"red\")) +\ntheme_minimal()\n```\n\n::: {.cell-output-display}\n![](report_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nInterpretation:\n\nSmokers and former smokers show higher stroke percentages than never-smokers.\n\n# mini heat map for top preadators\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_numeric <- strokeclean[, c(\"age\",\"avg_glucose_level\",\"bmi\",\n\"hypertension\",\"heart_disease\")]\n\ncorr_matrix <- cor(top_numeric)\n\nggcorrplot::ggcorrplot(\ncorr_matrix,\nlab = TRUE,\ncolors = c(\"blue\", \"white\", \"red\"),\ntitle = \"Correlation of Key Stroke Predictors\"\n)\n```\n\n::: {.cell-output-display}\n![](report_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n### References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "report_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}